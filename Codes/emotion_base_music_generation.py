# -*- coding: utf-8 -*-
"""Emotion_Base_Music_Generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ysP1P341ol9Zb-n8i4mKj-b2a7rI6jeT

# ------------- Mohammad Amin Kiani 4003613052 -------------
## Final Project _ Emotion-Based Music Generation _ ui.ac.ir 400-404

# ØªØ­Ù„ÛŒÙ„ Ù…ØªÙ†

##  Ù†ÛŒØ§Ø²Ù…Ù†Ø¯ÛŒ Ù‡Ø§
"""

!pip install -q transformers datasets sentencepiece scikit-learn langdetect accelerate

from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from transformers import DataCollatorWithPadding
from datasets import load_dataset, Dataset
from sklearn.metrics import accuracy_score, f1_score
from langdetect import detect
import torch
import numpy as np

"""## Ø§Ù…Ø§Ø¯Ù‡"""

def detect_language(text):
    try:
        lang = detect(text)
        return 'fa' if lang == 'fa' else 'en'
    except:
        return 'unknown'

# Ù…Ø¯Ù„ Ø§Ø­Ø³Ø§Ø³Ø§Øª ÙØ§Ø±Ø³ÛŒ (ParsBERT) == > Ø¶Ø¹ÛŒÙ Ù†ÛŒØ§Ø² Ø¨Ù‡ ÙØ§ÛŒÙ† ØªÛŒÙˆÙ† Ú©Ø±Ø¯Ù† Ø¯Ø§Ø±Ø¯
fa_model_name = "HooshvareLab/bert-fa-base-uncased"
fa_tokenizer = AutoTokenizer.from_pretrained(fa_model_name)
fa_model = AutoModelForSequenceClassification.from_pretrained(fa_model_name, num_labels=7)  # ÙØ¹Ù„Ø§Ù‹ 7 Ø¨Ø±Ú†Ø³Ø¨ Ø§Ø­Ø³Ø§Ø³

# Ù…Ø¯Ù„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ (BERT Ø±ÙˆÛŒ GoEmotions ÛŒØ§ Ù…Ø´Ø§Ø¨Ù‡)
# Ø¨Ø±Ø®Ù„Ø§Ù Ø±ÙˆØ¨ÙˆØ¨Ø±Øª Ø§Ø² Ù‚Ø¨Ù„ Ø±ÙˆÛŒ Ø¯ÛŒØªØ§Ø³Øª Ù…Ø±Ø­Ù„Ù‡ Ø¨Ø¹Ø¯ÛŒ ÙØ§ÛŒÙ† ØªÛŒÙˆÙ† Ø´Ø¯Ù‡ Ùˆ Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ Ø§Ù…ÙˆØ²Ø´ Ù†Ø¯Ø§Ø±Ø¯
en_model_name = "nateraw/bert-base-uncased-emotion"
en_tokenizer = AutoTokenizer.from_pretrained(en_model_name)
en_model = AutoModelForSequenceClassification.from_pretrained(en_model_name)

def predict_emotion(text):
    lang = detect_language(text)

    if lang == 'fa':
        tokenizer = fa_tokenizer
        model = fa_model
    elif lang == 'en':
        tokenizer = en_tokenizer
        model = en_model
    else:
        return "Unknown language"

    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        padding="max_length",
        max_length=128              # Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù‡Ø´Ø¯Ø§Ø±
    )
    with torch.no_grad():
        logits = model(**inputs).logits
    probs = torch.nn.functional.softmax(logits, dim=1)
    pred_class = torch.argmax(probs, dim=1).item()

    return f"Ø²Ø¨Ø§Ù† ØªØ´Ø®ÛŒØµâ€ŒØ¯Ø§Ø¯Ù‡â€ŒØ´Ø¯Ù‡: {lang} | Ú©Ù„Ø§Ø³ Ø§Ø­Ø³Ø§Ø³: {pred_class}"

# ØªØ³Øª: Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ
print(predict_emotion("!Ø®ÛŒÙ„ÛŒ Ù†Ø§Ø±Ø§Ø­ØªÙ…"))

# ØªØ³Øª: Ù…ØªÙ† Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ
print(predict_emotion("I'm feeling excited and happy today!"))

"""## --------------------------------------

## ÙØ§ÛŒÙ† ØªÛŒÙˆÙ† Ú©Ø±Ø¯Ù†

### Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ
"""

!git clone https://github.com/Arman-Rayan-Sharif/arman-text-emotion.git
!ls arman-text-emotion/dataset

# from datasets import load_dataset

# # Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ø¯ÛŒØªØ§Ø³Øª
# dataset_fa = load_dataset("sobhanmoosavi/arman-emo")

# #  Ú†Ù†Ø¯ Ù†Ù…ÙˆÙ†Ù‡ Ø§ÙˆÙ„
# dataset_fa['train'][0]

import pandas as pd

# ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ù‡ÛŒÚ† Ù‡Ø¯Ø±ÛŒ Ù†Ø¯Ø§Ø±Ù†Ø¯
train_df = pd.read_csv(
    "arman-text-emotion/dataset/train.tsv",
    sep="\t",
    header=None,
    names=["text","label"]
)
test_df = pd.read_csv(
    "arman-text-emotion/dataset/test.tsv",
    sep="\t",
    header=None,
    names=["text","label"]
)

# Ø­Ø°Ù Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒ Ù†Ø§Ù‚Øµ (Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯)
train_df.dropna(subset=["text","label"], inplace=True)
test_df.dropna(subset=["text","label"], inplace=True)

print("Train sample:")
print(train_df.head(), "\n")
print("Test sample:")
print(test_df.head())

import os
os.environ["WANDB_DISABLED"] = "true"

# Ø¯ÛŒÚ¯Ø± Ø§Ø² Ù…Ø§ Ù†Ø®ÙˆØ§Ù‡Ø¯ : API Key
# Ø§Ø² Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ ÙˆØ²Ù† Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ú©Ù†Ø¯ : Weights & Biases (wandb)

!pip install -q --upgrade transformers

import pandas as pd
import glob
from datasets import Dataset, DatasetDict
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import DataCollatorWithPadding, TrainingArguments, Trainer
import numpy as np
import torch
from sklearn.metrics import accuracy_score, f1_score

# 2. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ TSV  +  Ø¨Ø§ Ù†Ø§Ù…â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§
train_df = pd.read_csv(
    "arman-text-emotion/dataset/train.tsv",
    sep="\t", header=None, names=["text","label"]
)
test_df = pd.read_csv(
    "arman-text-emotion/dataset/test.tsv",
    sep="\t", header=None, names=["text","label"]
)
train_df.dropna(subset=["text","label"], inplace=True)
test_df.dropna(subset=["text","label"], inplace=True)

# 3. Ø³Ø§Ø®Øª DatasetDict
dataset_fa = DatasetDict({
    "train": Dataset.from_pandas(train_df),
    "test":  Dataset.from_pandas(test_df)
})

# 4. Ù†Ú¯Ø§Ø´Øª Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ Ø¨Ù‡ Ø¹Ø¯Ø¯ Ùˆ Ø²ÛŒØ± Ú©Ù„ÛŒØ¯ 'labels'
label_list = sorted(train_df["label"].unique().tolist())
label2id   = {l:i for i,l in enumerate(label_list)}
id2label   = {i:l for l,i in label2id.items()}

def encode_label(ex):
    return {"labels": label2id[ex["label"]]}

#  ÙÙ‚Ø· Ø³ØªÙˆÙ† Ù„ÛŒØ¨Ù„ Ø­Ø°Ù Ø´ÙˆØ¯ 'label'
dataset_fa = dataset_fa.map(encode_label, remove_columns=["label"])

# 5. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ùˆ Ù…Ø¯Ù„ (ParsBERT)
fa_model_name = "HooshvareLab/bert-fa-base-uncased"
fa_tokenizer  = AutoTokenizer.from_pretrained(fa_model_name)
fa_model      = AutoModelForSequenceClassification.from_pretrained(
    fa_model_name,
    num_labels=len(label_list)
)

# 6. ØªÙˆÚ©Ù†â€ŒØ³Ø§Ø²ÛŒ
def tokenize_fn(ex):
    return fa_tokenizer(
        ex["text"],
        truncation=True,
        padding="max_length",
        max_length=128
    )

dataset_fa = dataset_fa.map(tokenize_fn, batched=True, remove_columns=["text"])

# 7. ØªÙ†Ø¸ÛŒÙ… ÙØ±Ù…Øª PyTorch
dataset_fa.set_format(
    type="torch",
    columns=["input_ids","attention_mask","labels"]
)

# 8. DataCollator
data_collator = DataCollatorWithPadding(tokenizer=fa_tokenizer)

# 9. Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return {
        "accuracy": accuracy_score(labels, preds),
        "f1":       f1_score(labels, preds, average="macro")
    }

from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="./parsbert-emotion-fa",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    learning_rate=2e-5,
    weight_decay=0.01,

    # Ù„Ø§Ú¯â€ŒÚ¯ÛŒØ±ÛŒ
    logging_dir="./logs",
    logging_steps=100,    # Ù‡Ø± Û±Û°Û° step ÛŒÚ© Ù„Ø§Ú¯

    # ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ùˆ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø± Ø·ÙˆÙ„ Ø¢Ù…ÙˆØ²Ø´
    do_train=True,
    do_eval=True,
    eval_steps=500,       # Ù‡Ø± ÛµÛ°Û° step Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
    save_steps=500,       # step Ø¨Ø±Ø§ÛŒ Ù‡Ø± 500 ØªØ§ checkpoint Ø°Ø®ÛŒØ±Ù‡
    save_total_limit=2,   # Ø­Ø¯Ø§Ú©Ø«Ø± Û² checkpoint Ù†Ú¯Ù‡ Ø¯Ø§Ø´ØªÙ†

    # (Ø­Ø°Ù load_best_model_at_end Ú†ÙˆÙ† Ù†ÛŒØ§Ø²Ù…Ù†Ø¯ evaluation_strategy )
)

trainer_fa = Trainer(
    model=fa_model,
    args=training_args,
    train_dataset=dataset_fa["train"],
    eval_dataset=dataset_fa["test"],
    tokenizer=fa_tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer_fa.train()


# 13. Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„
fa_model.save_pretrained("./final_parsbert_emotion")
fa_tokenizer.save_pretrained("./final_parsbert_emotion")
print(" Ù…Ø¯Ù„ ÙØ§Ø±Ø³ÛŒ ÙØ§ÛŒÙ†â€ŒØªÛŒÙˆÙ† Ø´Ø¯ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")

"""Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„ ÙØ§ÛŒÙ† ØªÛŒÙˆÙ† Ø´Ø¯Ù‡"""

!zip -r final_parsbert_emotion.zip final_parsbert_emotion

from google.colab import files
files.download("final_parsbert_emotion.zip")

print("All labels:", label_list)
print("Mapping id2label:", id2label)

from transformers import AutoConfig

# Û±. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ø§Ù†ÙÛŒÚ¯ Ùˆ Ø§ÙØ²ÙˆØ¯Ù† Ù†Ú¯Ø§Ø´Øª
config = AutoConfig.from_pretrained(model_dir)
config.id2label   = id2label
config.label2id   = label2id

# Û². Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø¨Ø§ Ú©Ø§Ù†ÙÛŒÚ¯ Ø³ÙØ§Ø±Ø´ÛŒ
model = AutoModelForSequenceClassification.from_pretrained(model_dir, config=config)
tokenizer = AutoTokenizer.from_pretrained(model_dir)

# Û³. Ø³Ø§Ø®Øª pipeline
clf = pipeline("text-classification", model=model, tokenizer=tokenizer)

# Û´. ØªØ³Øª
print(clf("Ø®ÛŒÙ„ÛŒ Ù†Ø§Ø±Ø§Ø­ØªÙ… Ùˆ ØºÙ…Ú¯ÛŒÙ†Ù…"))

from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ùˆ pipeline
model_dir = "/content/final_parsbert_emotion"
tokenizer = AutoTokenizer.from_pretrained(model_dir)
model     = AutoModelForSequenceClassification.from_pretrained(model_dir)
clf = pipeline("text-classification", model=model, tokenizer=tokenizer)


def predict_with_threshold(text, threshold=0.7):
    out = clf(text, top_k=None)[0]
    prob = out['score']
    idx  = int(out['label'].split("_")[-1])
    label = id2label[idx]
    if prob < threshold:
        label = 'OTHER'
    return {"label": label, "score": prob}

# ØªØ³Øª
print(predict_with_threshold("Ø±ÙØªÙ… Ø¨Ø§Ø²Ø§Ø± Ø®Ø±ÛŒØ¯ Ú©Ù†Ù…", threshold=0.7))

"""### Ù…ØªÙ† Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ"""

import numpy as np
import torch
from datasets import load_dataset, DatasetDict
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
    pipeline
)
from sklearn.metrics import f1_score, precision_score, recall_score

# GoEmotions Ø¯ÛŒØªØ§Ø³ØªÙ ØªØ´Ø®ÛŒØµ Ø§Ø­Ø³Ø§Ø³ Ú†Ù†Ø¯Ø¬Ø³Ù…Ø§Ù†ÛŒ Û²Û· Ø¨Ø±Ú†Ø³Ø¨Ù‡

raw_en = load_dataset("go_emotions")

# ØªÙ‚Ø³ÛŒÙ… train/validation/test
print(dataset_en)

# Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ (Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ Û²Û·Ú©Ù„Ø§Ø³Ù‡)
label_names = dataset_en["train"].features["labels"].feature.names
print("Labels:", label_names)
num_labels = len(label_names)

model_name = "roberta-base"  #Ù…Ø¯Ù„ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¯Ù„Ø®ÙˆØ§Ù‡
tokenizer_en = AutoTokenizer.from_pretrained(model_name)

model_en = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=num_labels,
    problem_type="multi_label_classification"
)

from datasets import load_dataset, DatasetDict
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from transformers import DataCollatorWithPadding, TrainingArguments, Trainer
from sklearn.metrics import f1_score, precision_score, recall_score

# 1) Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯ÛŒØªØ§Ø³Øª Ø®Ø§Ù…
raw = load_dataset("go_emotions")
dataset_en = DatasetDict({
    "train":      raw["train"],
    "validation": raw["validation"],
    "test":       raw["test"]
})

# 2) Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ùˆ Ù…Ø¯Ù„
tokenizer_en = AutoTokenizer.from_pretrained("roberta-base")
model_en  = AutoModelForSequenceClassification.from_pretrained(
    "roberta-base",
    num_labels=len(raw["train"].features["labels"].feature.names),
    problem_type="multi_label_classification"
)
num_labels = model_en.config.num_labels

# 3) Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ù„ÛŒØ¨Ù„â€ŒÙ‡Ø§ÛŒ Ø§Ø¹Ø´Ø§Ø±ÛŒ ÙÙ„ÙˆØª Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯
def preprocess(ex):
    enc = tokenizer_en(
        ex["text"],
        truncation=True,
        padding="max_length",
        max_length=128
    )
    label_vec = [0.0] * num_labels
    for idx in ex["labels"]:
        label_vec[idx] = 1.0
    enc["labels"] = label_vec
    return enc

# 4) ØªÙˆÚ©Ù†Ø§ÛŒØ² + Ø­Ø°Ù Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø²Ø§Ø¦Ø¯
dataset_en = dataset_en.map(
    preprocess,
    batched=False,
    remove_columns=["text", "id"]
)

# 5) Ù‚Ø§Ù„Ø¨â€ŒØ¯Ù‡ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ø±Ø§ÛŒ PyTorch
dataset_en.set_format(
    type="torch",
    columns=["input_ids", "attention_mask", "labels"]
)

# Trainer Ùˆ training_args Ù…Ø«Ù„ Ù‚Ø¨Ù„:
data_collator_en = DataCollatorWithPadding(tokenizer=tokenizer_en)

def compute_metrics_en(eval_pred):
    logits, labels = eval_pred
    probs = torch.sigmoid(torch.tensor(logits))
    preds = (probs.numpy() >= 0.5).astype(int)
    labels = labels.numpy()
    return {
        "f1":        f1_score(labels, preds, average="macro", zero_division=0),
        "precision": precision_score(labels, preds, average="macro", zero_division=0),
        "recall":    recall_score(labels, preds, average="macro", zero_division=0),
    }

training_args_en = TrainingArguments(
    output_dir="./roberta-go_emotions",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    learning_rate=2e-5,
    weight_decay=0.01,
    logging_dir="./logs_go",
    logging_steps=500,
    do_train=True,
    do_eval=True,
    eval_steps=1000,
    save_steps=1000,
    save_total_limit=2,
    report_to="none",
    run_name=None,
)

import torch
import torch.nn as nn
from transformers import Trainer

loss_fn = nn.BCEWithLogitsLoss()

class MultiLabelTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        labels = inputs["labels"].float()
        outputs = model(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
        )
        logits = outputs.logits
        loss = loss_fn(logits, labels)
        return (loss, outputs) if return_outputs else loss

trainer_en = MultiLabelTrainer(
    model=model_en,
    args=training_args_en,
    train_dataset=dataset_en["train"],
    eval_dataset=dataset_en["validation"],
    tokenizer=tokenizer_en,
    data_collator=data_collator_en,
    compute_metrics=compute_metrics_en,
)


trainer_en.train()
trainer_en.save_model("./final_go_emotions")
tokenizer_en.save_pretrained("./final_go_emotions")

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ ÙØ§ÛŒÙ†â€ŒØªÛŒÙˆÙ†â€ŒØ´Ø¯Ù‡
tokenizer_en = AutoTokenizer.from_pretrained("./final_go_emotions")
model_en     = AutoModelForSequenceClassification.from_pretrained("./final_go_emotions")

# Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ú†Ù†Ø¯ØªØ§ÛŒÛŒ
clf_en = pipeline(
    "text-classification",
    model=model_en,
    tokenizer=tokenizer_en,
    function_to_apply="sigmoid",
    top_k=None  # Ù‡Ù…Ù‡ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯
)

# ØªØ³Øª
print(clf_en("I am feeling very excited and joyful today!"))
print(clf_en("I am sad and depressed."))

from transformers import pipeline

clf_en = pipeline(
    "text-classification",
    model="nateraw/bert-base-uncased-emotion",
    return_all_scores=False
)

print(clf_en("I feel really happy today!"))
# [{'label': 'joy', 'score': 0.99}]

"""##Ù…ØªÙ† Ù†Ù‡Ø§ÛŒÛŒ"""

#  Ù†ØµØ¨ Gradio (Ø§Ú¯Ø± Ù‡Ù†ÙˆØ² Ù†ØµØ¨ Ù†Ú©Ø±Ø¯Ù‡â€ŒØ§ÛŒØ¯)
!pip install -q gradio

import gradio as gr
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
from langdetect import detect

# Û±) Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ùˆ pipelineÙ‡Ø§ÛŒ Ø¯Ùˆâ€Œâ€ŒØ²Ø¨Ø§Ù†Ù‡
# ÙØ§Ø±Ø³ÛŒ
fa_tokenizer = AutoTokenizer.from_pretrained("/content/final_parsbert_emotion")
fa_model     = AutoModelForSequenceClassification.from_pretrained("/content/final_parsbert_emotion")
pipe_fa      = pipeline("text-classification", model=fa_model, tokenizer=fa_tokenizer)

FA_LABELS = ['ANGRY','FEAR','HAPPY','HATE','OTHER','SAD','SURPRISE']
fa_map = { idx: ("ANGRY" if lab=="HATE" else lab)
           for idx,lab in enumerate(FA_LABELS) }

# Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ single-label
pipe_en = pipeline(
    "text-classification",
    model="nateraw/bert-base-uncased-emotion",
    top_k=1
)

def map_en(label: str):
    if label in ["joy","love"]: return "HAPPY"
    if label=="sadness":         return "SAD"
    if label=="anger":           return "ANGRY"
    if label=="fear":            return "FEAR"
    if label=="surprise":        return "SURPRISE"
    return "OTHER"

# Û²) Ù†Ú¯Ø§Ø´Øª Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´
EMOJI_MAP = {
    "ANGRY":    "ğŸ˜ ",
    "FEAR":     "ğŸ˜¨",
    "HAPPY":    "ğŸ˜ƒ",
    "SAD":      "ğŸ˜¢",
    "SURPRISE": "ğŸ˜²",
    "OTHER":    "ğŸ˜"
}

# Û³) ØªØ§Ø¨Ø¹ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³ Ø¨Ø§ Ø¢Ø³ØªØ§Ù†Ù‡
def analyze_interface(text, threshold=0.7):
    lang = detect(text)
    if lang=="fa":
        out = pipe_fa(text)[0]
        idx = int(out['label'].split("_")[-1])
        emotion = fa_map[idx]
        score   = out['score']
    else:
        out = pipe_en(text)[0]
        emotion = map_en(out['label'])
        score   = out['score']
    if score < threshold:
        emotion = "OTHER"
    emoji = EMOJI_MAP.get(emotion, "")
    return emotion, round(score,3), emoji

# Û´) Ø³Ø§Ø®Øª Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø¨Ø§ Gradio
with gr.Blocks(css="""
    #root {max-width:600px;margin:auto;padding:20px;}
    .output-box {font-size:1.2rem;text-align:center;margin-top:10px;}
""") as demo:
    gr.Markdown("## ğŸµ AI Emotion Detector")
    gr.Markdown("Ù…ØªÙ† Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯ ØªØ§ Â«Ø­Ø³Â» Ø´Ù…Ø§ Ø±Ø§ ØªØ´Ø®ÛŒØµ Ø¯Ù‡Ø¯ØŒ Ù‡Ù…Ø±Ø§Ù‡ Ø¨Ø§ Ø§Ù…ØªÛŒØ§Ø² Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ùˆ Ø§ÛŒÙ…ÙˆØ¬ÛŒ.")
    with gr.Row():
        txt     = gr.Textbox(label="Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ", placeholder="Ø§ÛŒÙ†Ø¬Ø§ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯...", lines=3)
        thresh  = gr.Slider(0,1,value=0.7,step=0.01,label="Ø¢Ø³ØªØ§Ù†Ù‡ Ø¨Ø±Ø§ÛŒ OTHER")
    btn = gr.Button("ØªØ­Ù„ÛŒÙ„ Ú©Ù†", variant="primary")
    with gr.Row():
        label_out = gr.Textbox(label="Ø§Ø­Ø³Ø§Ø³ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒâ€ŒØ´Ø¯Ù‡", interactive=False, elem_classes="output-box")
        score_out = gr.Textbox(label="Ø¯Ø±ØµØ¯ Ø§Ø·Ù…ÛŒÙ†Ø§Ù†",       interactive=False, elem_classes="output-box")
        emoji_out = gr.Textbox(label="Ø§ÛŒÙ…ÙˆØ¬ÛŒ",              interactive=False, elem_classes="output-box")
    btn.click(fn=analyze_interface, inputs=[txt, thresh], outputs=[label_out, score_out, emoji_out])

demo.launch(share=False)

"""#ØªØ­Ù„ÛŒÙ„ ØªØµÙˆÛŒØ±

## Ø¢Ù…Ø§Ø¯Ù‡
"""

!pip install opencv-python keras numpy tensorflow

!pip install --upgrade tensorflow

import os
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"  # Ø§Ø¬Ø±Ø§ÛŒ Ø±ÙˆÛŒ CPU

import cv2
import numpy as np
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import img_to_array

# Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„
!wget https://github.com/oarriaga/face_classification/raw/master/trained_models/emotion_models/fer2013_mini_XCEPTION.102-0.66.hdf5 -O emotion_model.h5

# Ù„ÙˆØ¯ Ù…Ø¯Ù„ Ø¨Ø¯ÙˆÙ† Ú©Ø§Ù…Ù¾Ø§ÛŒÙ„
model = load_model('emotion_model.h5', compile=False)

emotion_labels = ['ANGRY', 'FEAR', 'HAPPY', 'SAD', 'SURPRISE', 'OTHER']

img_path = "/content/face4.jpg"
frame = cv2.imread(img_path)
gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
face_classifier = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
faces = face_classifier.detectMultiScale(gray, 1.3, 5)

for (x, y, w, h) in faces:
    roi_gray = gray[y:y+h, x:x+w]
    roi_gray = cv2.resize(roi_gray, (64, 64), interpolation=cv2.INTER_AREA)

    if np.sum([roi_gray]) != 0:
        roi = roi_gray.astype('float32') / 255.0
        roi = img_to_array(roi)
        roi = np.expand_dims(roi, axis=0)
        roi = roi.reshape((1, 64, 64, 1))

        preds = model.predict(roi)[0]
        label = emotion_labels[np.argmax(preds)]
        confidence = np.max(preds)
        print(f"Detected Emotion: {label} ({confidence:.2f})")
    else:
        print("No face detected.")

import numpy as np
import cv2
from tensorflow.keras.models import load_model
import os
# Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ ÙØ§ÛŒÙ„
print("Exists emotion_model.h5:", os.path.exists("emotion_model.h5"))
emotion_cnn = load_model("emotion_model.h5", compile=False)
print("Loaded model, summary:")
emotion_cnn.summary()  # ØŸ Ù…Ø¯Ù„ Ù„ÙˆØ¯ Ø´Ø¯

"""## ÙØ§ÛŒÙ† ØªÛŒÙˆÙ† Ø¯Ø³ØªÛŒ"""

# â”€â”€â”€  Û±: Ú©Ù„ÙˆÙ† Ø¯ÛŒØªØ§Ø³Øª FER-2013 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
!git clone https://huggingface.co/datasets/Jeneral/fer-2013

# â”€â”€â”€  Û²: ØªØ¨Ø¯ÛŒÙ„ .pt â†’ DatasetDict + Ù†Ú¯Ø§Ø´Øª Ù„ÛŒØ¨Ù„â€ŒÙ‡Ø§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

import torch, pickle
from datasets import Dataset, DatasetDict

# Û±) Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù„ÛŒØ³Øª Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ pickle
with open("fer-2013/train.pt","rb") as f:
    train_list = pickle.load(f)
with open("fer-2013/test.pt","rb") as f:
    test_list  = pickle.load(f)

# Û²) ØªØ¨Ø¯ÛŒÙ„ Ù„ÛŒØ³Øª Ø¨Ù‡ Dataset
train_ds = Dataset.from_list(train_list)
test_ds  = Dataset.from_list(test_list)

# Û³)  DatasetDict (Ù‚Ø¨Ù„ Ø§Ø² Ù‡ÛŒÚ† Ù…Ù¾ Ø¯ÛŒÚ¯Ø±ÛŒ)
ds = DatasetDict({
    "train": train_ds,
    "test":  test_ds
})

# Û´) Ù†Ú¯Ø§Ø´Øª Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ Ø±Ø´ØªÙ‡â€ŒØ§ÛŒ Ø¨Ù‡ Ø¹Ø¯Ø¯
label_list = ["angry","disgust","fear","happy","neutral","sad","surprise"]
label2id   = {l:i for i,l in enumerate(label_list)}
id2label   = {i:l for i,l in enumerate(label_list)}

def label_str_to_int(example):
    return {"labels": label2id[example["labels"]]}

ds = ds.map(label_str_to_int, batched=False)

# Ûµ) Ø¯ÛŒÚ©Ø¯ Ú©Ø±Ø¯Ù† Ø¨Ø§ÛŒØªâ€ŒÙ‡Ø§ Ø¨Ù‡ ØªØµÙˆÛŒØ± PIL
from PIL import Image
import io

def decode_bytes(example):
    example["image"] = Image.open(io.BytesIO(example["img_bytes"])).convert("RGB")
    return example

ds = ds.map(decode_bytes, remove_columns=["img_bytes"])

print(ds)


# â”€â”€â”€  Û³: Ø¯ÛŒÚ©Ø¯ Ú©Ø±Ø¯Ù† Ø¨Ø§ÛŒØªâ€ŒÙ‡Ø§ Ø¨Ù‡ ØªØµÙˆÛŒØ± PIL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print(ds["train"][0])
# {'labels': 0, 'image': <PIL.Image.Image image mode=RGB size=48x48>}

# â”€â”€â”€  Û´: FeatureExtractor Ùˆ Ù…Ø¯Ù„ ViT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# â”€â”€â”€ Config Ùˆ FeatureExtractor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import torch
from transformers import AutoConfig, AutoFeatureExtractor, AutoModelForImageClassification

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# Û· Ú©Ù„Ø§Ø³
label_list = ["angry","disgust","fear","happy","neutral","sad","surprise"]
label2id   = {l:i for i,l in enumerate(label_list)}
id2label   = {i:l for i,l in enumerate(label_list)}

# Û´.Û±) Config
config = AutoConfig.from_pretrained(
    "WinKawaks/vit-tiny-patch16-224",
    num_labels=len(label_list),
    id2label=id2label,
    label2id=label2id
)

# Û´.Û²) FeatureExtractor
feature_extractor = AutoFeatureExtractor.from_pretrained(
    "WinKawaks/vit-tiny-patch16-224",
    size=224
)

# Û´.Û³) Ù„ÙˆØ¯ Ù…Ø¯Ù„
model = AutoModelForImageClassification.from_pretrained(
    "WinKawaks/vit-tiny-patch16-224",
    config=config,
    ignore_mismatched_sizes=True
).to(device)

# Û´.Û´) ÙØ±ÛŒØ² Ú©Ø±Ø¯Ù† Ù‡Ù…Ù‡ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¬Ø² head
for name, param in model.named_parameters():
    if not name.startswith("classifier"):
        param.requires_grad = False

print("Trainable params:", sum(p.numel() for p in model.parameters() if p.requires_grad))


# â”€â”€â”€ Ø³Ù„ÙˆÙ„ Ûµ: Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ùˆ Ù‚Ø§Ù„Ø¨â€ŒØ¯Ù‡ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def preprocess_batch(batch):
    #  Ù‡Ù…Ø²Ù…Ø§Ù† Ù„ÛŒØ³Øª ØªØµØ§ÙˆÛŒØ± Ø±Ø§ resize+normalize
    enc = feature_extractor(images=batch["image"], return_tensors="pt")
    enc["labels"] = torch.tensor(batch["labels"], dtype=torch.long)
    return enc

# Ø§Ø¹Ù…Ø§Ù„ Ø¨Ù‡ Ú©Ù„ Ø¯ÛŒØªØ§Ø³Øª
ds = ds.map(preprocess_batch, batched=True, remove_columns=["image"])
ds.set_format(type="torch", columns=["pixel_values","labels"])


# â”€â”€â”€  Û¶:  Trainer  Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ù…Ø¹ÛŒØ§Ø± Ø¯Ù‚Øª
from transformers import TrainingArguments, Trainer
import numpy as np
import evaluate

metric = evaluate.load("accuracy")
def compute_metrics(p):
    preds = np.argmax(p.predictions, axis=-1)
    return metric.compute(predictions=preds, references=p.label_ids)

training_args = TrainingArguments(
    output_dir="./face_emotion_vit",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    num_train_epochs=5,

    per_device_train_batch_size=4,       # â† Ø®ÛŒÙ„ÛŒ Ú©Ù…
    per_device_eval_batch_size=4,        # â† Ø®ÛŒÙ„ÛŒ Ú©Ù…

    gradient_accumulation_steps=4,       # â† effective batch = 4Ã—4 =16
    gradient_checkpointing=True,         # â† reduce memory for activations

    learning_rate=2e-4,                  # â† Ø§Ù†Ø¯Ú©ÛŒ Ø¨ÛŒØ´ØªØ± Ø¨Ø±Ø§ÛŒ head
    weight_decay=0.0,

    load_best_model_at_end=True,
    metric_for_best_model="accuracy",

    fp16=torch.cuda.is_available(),      # â† mixed precision
    dataloader_num_workers=2,
    logging_steps=100
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=ds["train"],
    eval_dataset=ds["test"],
    tokenizer=feature_extractor,
    compute_metrics=compute_metrics
)

trainer.train()

trainer.save_model("./face_emotion_vit")
feature_extractor.save_pretrained("./face_emotion_vit")


# â”€â”€â”€  Û·: ØªØ³Øª Ù†Ù‡Ø§ÛŒÛŒ (inference) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from transformers import pipeline
pipe = pipeline(
    "image-classification",
    model="./face_emotion_vit",
    feature_extractor="./face_emotion_vit",
    device=0
)
from PIL import Image
print(pipe(Image.open("face4.jpg"), top_k=3))

"""Ø¨Ù‡ Ø¹Ù„Øª Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ù‡Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ ØªØµÙˆÛŒØ± Ùˆ ØµÙˆØª Ù†Ù…ÛŒØªÙˆØ§Ù† ÙØ§ÛŒÙ† ØªÛŒÙˆÙ† Ø§Ù†Ø¬Ø§Ù… Ø¯Ø§Ø¯ Ùˆ ØµØ±ÙØ§ Ù¾Ø§ÛŒÙ¾ Ù„Ø§ÛŒÙ† Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒ Ø´ÙˆØ¯...

# ØªØ­Ù„ÛŒÙ„ ØµÙˆØª
"""

from transformers import pipeline
from pydub import AudioSegment

# Ù„ÙˆØ¯ Ù…Ø¯Ù„ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³ Ø§Ø² ØµÙˆØª
model_id = "superb/wav2vec2-base-superb-er"
emotion_pipeline = pipeline("audio-classification", model=model_id)

# ØªØ¨Ø¯ÛŒÙ„ mp3 ==> wav
audio = AudioSegment.from_file("/content/input2.wav") #shad
audio = audio.set_frame_rate(16000).set_channels(1)
audio.export("/content/sample.wav", format="wav")

# ØªØ³Øª Ù…Ø¯Ù„ Ø±ÙˆÛŒ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ wav
audio_path = "/content/sample.wav"
results = emotion_pipeline(audio_path)
print(results)

"""# ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ù…ÙˆÙ„ØªÛŒ Ù…ÙˆØ¯Ø§Ù„"""

# â”€â”€â”€  Û±: Ù†ØµØ¨ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€----------------------------------------------------
!pip install -q gradio transformers[sentencepiece] soundfile pillow

# â”€â”€â”€  Û²: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-------------------------------------------------------
import gradio as gr
from transformers import pipeline
from langdetect import detect
import soundfile as sf
import tempfile

# ØªØ¹Ø±ÛŒÙ Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ùˆ ÙØ¶Ø§ÛŒ Û¶ Ú©Ù„Ø§Ø³Ù‡
EMOJI = {
    "ANGRY":"ğŸ˜ ", "FEAR":"ğŸ˜¨", "HAPPY":"ğŸ˜ƒ",
    "SAD":"ğŸ˜¢",   "SURPRISE":"ğŸ˜²","OTHER":"ğŸ˜"
}

# â”€â”€ Pipeline Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ â”€â”€
pipe_fa = pipeline(
    "text-classification",
    model="/content/final_parsbert_emotion",
    tokenizer="/content/final_parsbert_emotion",
    device=0
)

# â”€â”€ Pipeline Ù…ØªÙ† Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ single-label â”€â”€
pipe_en = pipeline(
    "text-classification",
    model="nateraw/bert-base-uncased-emotion",
    tokenizer="nateraw/bert-base-uncased-emotion",
    device=0,
    return_all_scores=False
)

def analyze_text(text, threshold):
    # ØªØ´Ø®ÛŒØµ Ø²Ø¨Ø§Ù†
    lang = detect(text)
    if lang == "fa":
        out = pipe_fa(text)[0]
        idx = int(out["label"].split("_")[-1])
        # Ù†Ú¯Ø§Ø´Øª HATEâ†’ANGRY
        FA = ['ANGRY','FEAR','HAPPY','HATE','OTHER','SAD','SURPRISE']
        label = FA[idx] if FA[idx]!="HATE" else "ANGRY"
        score = out["score"]
    else:
        out = pipe_en(text)[0]
        raw = out["label"].lower()
        # Ù†Ú¯Ø§Ø´Øª Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¨Ù‡ ÙØ¶Ø§ÛŒ Ù…Ø´ØªØ±Ú©
        MAP = {
            "joy":"HAPPY","love":"HAPPY",
            "sadness":"SAD","anger":"ANGRY",
            "fear":"FEAR","surprise":"SURPRISE"
        }
        label = MAP.get(raw, "OTHER")
        score = out["score"]

    if score < threshold:
        label = "OTHER"
    return label, round(score,3), EMOJI[label]



# â€”â€” Pipeline Ø§Ø¨Ø²Ø§Ø± FER Ø¨Ø±Ø§ÛŒ ØªØµÙˆÛŒØ± â€”â€”
!wget https://github.com/oarriaga/face_classification/raw/master/trained_models/emotion_models/fer2013_mini_XCEPTION.102-0.66.hdf5 -O emotion_model.h5

#  Û°: Ù…Ø·Ù…Ø¦Ù† Ø´Ø¯Ù† Ø§Ø² Ù†ØµØ¨ tensorflow Ùˆ opencv
!pip install -q tensorflow opencv-python-headless

import numpy as np
import cv2
from tensorflow.keras.models import load_model

emotion_cnn = load_model("emotion_model.h5", compile=False)
FER_LABELS = ['angry','disgust','fear','happy','sad','surprise','neutral']
IMG_MAP = {
  "angry":"ANGRY","disgust":"ANGRY","fear":"FEAR",
  "happy":"HAPPY","sad":"SAD","surprise":"SURPRISE",
  "neutral":"OTHER"
}

face_cascade = cv2.CascadeClassifier(
    cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
)

def analyze_image(img, threshold):
    try:
        gray = cv2.cvtColor(np.array(img.convert("RGB")), cv2.COLOR_RGB2GRAY)
        faces = face_cascade.detectMultiScale(gray, 1.3, 5)

        if len(faces)==0:
            # Ú©Ø±Ø§Ù¾ Ø§Ø² ÙˆØ³Ø· ÙˆÙ‚ØªÛŒ Ú†Ù‡Ø±Ù‡ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯
            h,w = gray.shape
            sz = min(h,w)//2
            x = (w-sz)//2; y = (h-sz)//2
            roi = gray[y:y+sz, x:x+sz]
        else:
            x,y,w,h = faces[0]
            roi = gray[y:y+h, x:x+w]

        face = cv2.resize(roi, (64,64), interpolation=cv2.INTER_AREA)
        face = face.astype("float32")/255.0
        face = face.reshape(1,64,64,1)

        preds = emotion_cnn.predict(face, verbose=0)[0]
        idx   = int(np.argmax(preds))
        score = float(preds[idx])
        raw   = FER_LABELS[idx]
        label = IMG_MAP.get(raw, "OTHER")

        # Ø¨Ø¯ÙˆÙ† ØµÙØ± Ú©Ø±Ø¯Ù† Ø¨Ø§ threshold
        return label, round(score,3), EMOJI[label]
    except Exception:
        return "OTHER", 0.0, EMOJI["OTHER"]


# â”€â”€ Pipeline ØµÙˆØª (Superb) â”€â”€
audio_pipe = pipeline(
    "audio-classification",
    model="superb/wav2vec2-base-superb-er",
    device=0,
    top_k=1
)
def analyze_audio(filepath, threshold):
    # Ù…Ù…Ú©Ù† Ø§Ø³Øª ÙØ±Ù…Øª Ù…ØªÙØ§ÙˆØª Ø¨Ø§Ø´Ø¯
    arr, sr = sf.read(filepath)
    tmp = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
    sf.write(tmp.name, arr, sr)
    out = audio_pipe(tmp.name)[0]
    raw = out["label"].lower()
    score = out["score"]
    AUD_MAP = {
      "hap":"HAPPY","sad":"SAD","ang":"ANGRY",
      "fea":"FEAR","sur":"SURPRISE","neu":"OTHER"
    }
    label = AUD_MAP.get(raw[:3], "OTHER")
    if score < threshold:
        label = "OTHER"
    return label, round(float(score),3), EMOJI[label]


# â”€â”€â”€  Û³: ØªØ§Ø¨Ø¹ Ú†Ù†Ø¯Ù…ÙˆØ¯Ø§Ù„ Ø¨Ø±Ø§ÛŒ Gradio â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€--------------------------------------------------------------
def analyze_all(text, image, audio_path, threshold):
    if text and text.strip() != "":
        return analyze_text(text, threshold)
    if image is not None:
        return analyze_image(image, threshold)
    if audio_path:
        return analyze_audio(audio_path, threshold)
    return "OTHER", 0.0, EMOJI["OTHER"]


# â”€â”€â”€  Û´: Ø³Ø§Ø®Øª Ø±Ø§Ø¨Ø· Gradio â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€----------------------------------------------------------------
with gr.Blocks(css="""
    #root {max-width:600px;margin:auto;padding:20px;}
    .out {font-size:1.3rem;text-align:center;}
""") as demo:
    gr.Markdown("## Ø¯Ø³ØªÛŒØ§Ø± Ú†Ù†Ø¯â€ŒÙ…ÙˆØ¯Ø§Ù„ ØªØ´Ø®ÛŒØµ Ø§Ø­Ø³Ø§Ø³")
    gr.Markdown("Ù…ØªÙ†ØŒ ØªØµÙˆÛŒØ± ÛŒØ§ ØµÙˆØª Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯ ØªØ§ ÛŒÚ©ÛŒ Ø§Ø² Û¶ Ú©Ù„Ø§Ø³ Ø§ØµÙ„ÛŒ Ø±Ø§ Ù‡Ù…Ø±Ø§Ù‡ Ø¯Ø±ØµØ¯ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ùˆ Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ø¨Ø¨ÛŒÙ†ÛŒØ¯.")
    with gr.Row():
        txt = gr.Textbox(label="Ù…ØªÙ† (ÙØ§Ø±Ø³ÛŒ/Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ)", lines=2, placeholder="Ù…Ø«Ø§Ù„: Ù…Ù† Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ´Ø­Ø§Ù„Ù…!")
        img = gr.Image(label="ØªØµÙˆÛŒØ±", type="pil")
        aud = gr.Audio(label="ØµÙˆØª (wav/mp3)", type="filepath")
    thr = gr.Slider(0,1,0.7,step=0.01, label="Ø¢Ø³ØªØ§Ù†Ù‡ OTHER")
    btn = gr.Button("ØªØ­Ù„ÛŒÙ„ Ú©Ù†", variant="primary")
    with gr.Row():
        o1 = gr.Textbox(label="Ø§Ø­Ø³Ø§Ø³",    interactive=False, elem_classes="out")
        o2 = gr.Textbox(label="Ø§Ø¹ØªÙ…Ø§Ø¯",    interactive=False, elem_classes="out")
        o3 = gr.Textbox(label="Ø§ÛŒÙ…ÙˆØ¬ÛŒ",    interactive=False, elem_classes="out")
    btn.click(analyze_all, [txt, img, aud, thr], [o1, o2, o3])

demo.launch(share=False)

"""# ØªÙˆÙ„ÛŒØ¯ Ù…ÙˆØ³ÛŒÙ‚ÛŒ Ø§Ø² Ø§Ø­Ø³Ø§Ø³ ØªØ­Ù„ÛŒÙ„ Ø´Ø¯Ù‡

## Ø¢Ù…Ø§Ø¯Ù‡
"""

!pip install -q "numpy<2"

# â”€â”€â”€  A: Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ù‡Ù…Ù‡ Ù†Ø³Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
!pip uninstall -y torch torchvision torchaudio audiocraft

# â”€â”€â”€  B: Ù†ØµØ¨ Ù†Ø³Ø®Ù‡â€ŒÛŒ CPU-only PyTorch Ùˆ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ù†ØµØ¨ Ù†Ø³Ø®Ù‡â€ŒÛŒ CPU-only PyTorch Ú©Ù‡ Ø¨Ø§ NumPy<2 Ù‡Ù…Ø®ÙˆØ§Ù†ÛŒ Ø¯Ø§Ø±Ù‡
!pip install -q \
    torch==2.1.0+cpu torchvision==0.16.0+cpu torchaudio==2.1.0+cpu \
    --extra-index-url https://download.pytorch.org/whl/cpu

# Ù†ØµØ¨ Ù…Ø¬Ø¯Ø¯ Audiocraft
!pip install -q audiocraft soundfile

import numpy as np, torch, torchaudio
from audiocraft.models import MusicGen

print("numpy:", np.__version__)
print("torch:", torch.__version__)
print("torchaudio:", torchaudio.__version__)

# ØªØ³Øª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ MusicGen Ø±ÙˆÛŒ CPU
device = "cpu"
model = MusicGen.get_pretrained("facebook/musicgen-small").to(device)
print(" MusicGen loaded successfully")

# â”€â”€â”€  Ûµ: Ù†ØµØ¨ Audiocraft (MusicGen) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
!pip install -q audiocraft soundfile

# â”€â”€â”€  Û¶: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ MusicGen â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import torch
from audiocraft.models import MusicGen
import soundfile as sf

# Û±) Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² GPU
device = "cuda" if torch.cuda.is_available() else "cpu"
print("Using device:", device)

# Û²) Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ù¾ÛŒØ´â€ŒØ¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡ (small ÛŒØ§ medium ÛŒØ§ large)
model = MusicGen.get_pretrained("facebook/musicgen-small").to(device)

# Û³) ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ú©Ù„ÛŒ ØªÙˆÙ„ÛŒØ¯: Ø·ÙˆÙ„ Ù†Ù…ÙˆÙ†Ù‡ Ùˆ ØªÙ†ÙˆØ¹
model.set_generation_params(
    duration=20.0,   # Ø·ÙˆÙ„ ØµØ¯Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ù‡ Ø«Ø§Ù†ÛŒÙ‡
    top_k=250        # ØªÙ†ÙˆØ¹ Ù†ØªØ§ÛŒØ¬
)

# â”€â”€â”€  Û·: Ù†Ú¯Ø§Ø´Øª Ø§Ø­Ø³Ø§Ø³ â†’ Ù¾Ø±Ø§Ù…Ù¾Øª Ø¨Ø±Ø§ÛŒ Ù…ÙˆØ³ÛŒÙ‚ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
EMOTION_TO_PROMPT = {
    "HAPPY":    "An upbeat, joyful pop instrumental, lively tempo, major key",
    "SAD":      "A slow, melancholic piano melody with soft strings, minor key",
    "ANGRY":    "A heavy rock track with distorted electric guitar and strong drums",
    "FEAR":     "An eerie ambient soundscape with dissonant synth textures and low drones",
    "SURPRISE": "A bright cinematic fanfare with brass and uplifting percussion",
    "OTHER":    "A calm ambient background piece with gentle pads and soft textures"
}

def generate_music_from_emotion(emotion_label: str, out_path: str="generated.wav"):
    # Ø§Ù†ØªØ®Ø§Ø¨ Ù¾Ø±Ø§Ù…Ù¾Øª
    prompt = EMOTION_TO_PROMPT.get(emotion_label, EMOTION_TO_PROMPT["OTHER"])
    # ØªÙˆÙ„ÛŒØ¯ Ù…ÙˆØ³ÛŒÙ‚ÛŒ
    wav = model.generate([prompt])[0]  # Ø®Ø±ÙˆØ¬ÛŒ: Tensor Ø´Ú©Ù„ (time,)
    # Ø°Ø®ÛŒØ±Ù‡ Ø±ÙˆÛŒ Ø¯ÛŒØ³Ú©
    sf.write(out_path, wav.cpu().numpy(), samplerate=model.sample_rate)
    return out_path

# ØªØ³Øª Ø³Ø±ÛŒØ¹
print("Producing a 10s HAPPY track â€¦")
test_path = generate_music_from_emotion("HAPPY")
print("Saved to", test_path)

# â”€â”€â”€  Û¸: Ø§ØªØµØ§Ù„ Ø¨Ù‡ Gradio (ØªÚ© Ø³Ù„ÙˆÙ„) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import gradio as gr

def gradio_generate(label, threshold_dummy=0.0):
    # Ø§ÛŒÙ†Ø¬Ø§ label Ù‡Ù…Ø§Ù† Ø®Ø±ÙˆØ¬ÛŒ analyze_all Ø§Ø³Øª
    path = generate_music_from_emotion(label)
    return path

with gr.Blocks() as music_demo:
    gr.Markdown("## ğŸ¶ ØªÙˆÙ„ÛŒØ¯ Ù…ÙˆØ³ÛŒÙ‚ÛŒ Ø§Ø² Ø±ÙˆÛŒ Ø§Ø­Ø³Ø§Ø³")
    lbl = gr.Textbox(label="Ø§Ø­Ø³Ø§Ø³ Ø¯Ø±ÛŒØ§ÙØªÛŒ (ÛŒÚ©ÛŒ Ø§Ø²: HAPPY,SAD,ANGRY,FEAR,SURPRISE,OTHER)")
    btn = gr.Button("ØªÙˆÙ„ÛŒØ¯ Ù…ÙˆØ³ÛŒÙ‚ÛŒ")
    out_audio = gr.Audio(label="Ù…ÙˆØ³ÛŒÙ‚ÛŒ ØªÙˆÙ„ÛŒØ¯Ø´Ø¯Ù‡")
    btn.click(fn=gradio_generate, inputs=[lbl], outputs=[out_audio])

music_demo.launch(share=False)

"""## Ø¯Ø³ØªÛŒ"""

# FluidSynth (Ø±Ù†Ø¯Ø±Ú©Ù†Ù†Ø¯Ù‡â€ŒÛŒ MIDI â†’ WAV)
!apt-get -y install fluidsynth

# Ù…Ø§Ú˜ÙˆÙ„ Ù¾Ø§ÛŒØªÙˆÙ†ÛŒ pyFluidSynth + Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§
!pip install -q pyFluidSynth==1.3.3 pretty_midi music21 librosa soundfile numpy midi2audio

# ÛŒÚ© SoundFont Ø¬Ù†Ø±Ø§Ù„ Ø®ÙˆØ¨ (FluidR3)
!wget -q https://github.com/pepaslabs/fluidsynth-soundfonts/raw/master/FluidR3_GM/FluidR3_GM.sf2 -O /content/FluidR3_GM.sf2 \
  || wget -q https://github.com/urish/cyberbotics/raw/master/projects/robotbenchmark/resources/fluidr3/FluidR3_GM.sf2 -O /content/FluidR3_GM.sf2

import time, os, numpy as np, random, pretty_midi
import soundfile as sf
from midi2audio import FluidSynth

SF2_PATH = "/content/FluidR3_GM.sf2"   #  SoundFont Ø¯Ù„Ø®ÙˆØ§Ù‡

# Ù…Ø­Ø¯ÙˆØ¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ³ÛŒÙ‚Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø§Ø­Ø³Ø§Ø³ (Ø¨Ø±Ø§ÛŒ ØªÙ†ÙˆØ¹ Ù‡Ø± Ø¨Ø§Ø± ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ ØªØµØ§Ø¯ÙÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…)
EMO_PROFILE = {
    "HAPPY": {
        "mode": "major",
        "tempo_range": (115, 140),
        "base_keys":  [60, 62, 65, 67],                 # C, D, F, G
        "progressions": [
            ["I","V","vi","IV"], ["I","IV","V","I"], ["I","vi","IV","V"]
        ],
        "melody_density_range": (0.55, 0.85),
        "drums": True,
        "comment": "Ø´Ø§Ø¯ Ùˆ Ù¾Ø±Ø§Ù†Ø±Ú˜ÛŒ"
    },
    "SAD": {
        "mode": "minor",
        "tempo_range": (60, 85),
        "base_keys":  [57, 55, 52],                     # A, G, E
        "progressions": [
            ["i","VI","III","VII"], ["i","iv","i","VII"]
        ],
        "melody_density_range": (0.25, 0.45),
        "drums": False,
        "comment": "Ú©Ù†Ø¯ Ùˆ Ø§Ø­Ø³Ø§Ø³ÛŒ"
    },
    "ANGRY": {
        "mode": "minor",
        "tempo_range": (130, 170),
        "base_keys":  [50, 48, 53],                     # D, C, F
        "progressions": [
            ["i","bVI","bVII","i"], ["i","bIIÂ°","bVII","i"]
        ],
        "melody_density_range": (0.7, 0.95),
        "drums": True,
        "comment": "ØªÙ‡Ø§Ø¬Ù…ÛŒ Ùˆ Ú©ÙˆØ¨Ù†Ø¯Ù‡"
    },
    "FEAR": {
        "mode": "minor",
        "tempo_range": (50, 75),
        "base_keys":  [57, 58, 55],                     # A, Bb, G
        "progressions": [
            ["i","bIIÂ°","i","bVI"], ["i","iv","bIIÂ°","i"]
        ],
        "melody_density_range": (0.2, 0.4),
        "drums": False,
        "comment": "Ù…Ø±Ù…ÙˆØ² Ùˆ ØªÛŒØ±Ù‡"
    },
    "SURPRISE": {
        "mode": "major",
        "tempo_range": (120, 145),
        "base_keys":  [65, 67, 69],                     # F, G, A
        "progressions": [
            ["I","V","IV","â™­VII"], ["I","IV","V","â™­VII"]
        ],
        "melody_density_range": (0.6, 0.9),
        "drums": True,
        "comment": "Ø¬Ù‡Ø´ÛŒ/Ø³ÛŒÙ†Ú©ÙˆÙ¾"
    },
    "OTHER": {
        "mode": "major",
        "tempo_range": (85, 110),
        "base_keys":  [60, 62],
        "progressions": [
            ["I","ii","IV","V"], ["I","IV","ii","V"]
        ],
        "melody_density_range": (0.4, 0.65),
        "drums": False,
        "comment": "Ø®Ù†Ø«ÛŒ/Ù¾Ø³â€ŒØ²Ù…ÛŒÙ†Ù‡"
    },
}

MAJOR_ROMAN = {"I":(0,"maj"),"ii":(2,"min"),"iii":(4,"min"),"IV":(5,"maj"),
               "V":(7,"maj"),"vi":(9,"min"),"â™­VII":(10,"maj")}
MINOR_ROMAN = {"i":(0,"min"),"iv":(5,"min"),"v":(7,"min"),"VI":(8,"maj"),
               "VII":(10,"maj"),"bVI":(8,"maj"),"bVII":(10,"maj"),"bIIÂ°":(1,"dim")}
QUAL2INTS = {"maj":[0,4,7], "min":[0,3,7], "dim":[0,3,6]}

def _choose(profile, rng):
    bpm = rng.randint(*profile["tempo_range"])
    key = rng.choice(profile["base_keys"])
    prog = rng.choice(profile["progressions"])
    density = rng.uniform(*profile["melody_density_range"])
    return bpm, key, prog, density

def _scale(mode):
    return [0,2,4,5,7,9,11] if mode=="major" else [0,2,3,5,7,8,10]

def _add_chord(inst, start, end, root, qual, human_vel=70, rng=None):
    ints = QUAL2INTS[qual]
    for i in ints:
        vel = int(np.clip(human_vel + (rng.randint(-6,6) if rng else 0), 40, 100))
        inst.notes.append(pretty_midi.Note(velocity=vel, pitch=int(root+i), start=start, end=end))

def _drum_bar(inst_drm, bar_start, beat_dur, rng):
    for b in range(4):
        t = bar_start + b*beat_dur
        # Kick/Snare
        inst_drm.notes.append(pretty_midi.Note(100 if b%2==0 else 90,
                                               pitch=36 if b%2==0 else 38,
                                               start=t, end=t+0.05))
        # Hats (Ø¨Ø§ Ú©Ù…ÛŒ Ø¬ÛŒØªØªØ±)
        for off in [0, beat_dur/2]:
            jit = rng.uniform(-0.01, 0.01)
            inst_drm.notes.append(pretty_midi.Note(60, pitch=42, start=t+off+jit, end=t+off+jit+0.03))

def _drum_fill(inst_drm, bar_start, beat_dur, rng):
    # ÛŒÚ© ÙÛŒÙ„ Ø³Ø§Ø¯Ù‡ ØªØ§Ù…â€ŒÙ‡Ø§ Ø¯Ø± Ø§Ù†ØªÙ‡Ø§ÛŒ Ø³Ú©Ø´Ù†
    for i, pitch in enumerate([47,45,43,41]):  # toms
        t = bar_start + i*(beat_dur/2)
        inst_drm.notes.append(pretty_midi.Note(85+rng.randint(-5,5), pitch=pitch, start=t, end=t+0.12))

def _melody_motif(scale, rng, length=4):
    return [rng.choice(scale) for _ in range(length)]

def _add_melody(inst, start, bars, bpm, mode, key, density, rng):
    scale = _scale(mode)
    step = 0.5*(60.0/bpm)       # 8th notes
    total_sub = int(bars*4*2)
    t = start
    last_pitch = key + 7
    motif = _melody_motif(scale, rng, length=rng.choice([3,4,5]))

    lo, hi = key-5, key+(14 if mode=="major" else 12)
    for s in range(total_sub):
        if rng.random() < density:
            if rng.random() < 0.25:
                deg = rng.choice(scale)            # Ú¯Ø§Ù‡ÛŒ Ø®Ø§Ø±Ø¬ Ø§Ø² Ù…ÙˆØªÛŒÙ
            else:
                deg = motif[s % len(motif)]
            base = key + deg + rng.choice([-12,0,12])
            pitch = int(np.clip(int(0.7*last_pitch + 0.3*base), lo, hi))
            jitter = rng.uniform(-0.02, 0.02)
            dur = step if rng.random()<0.7 else step*2
            vel = int(np.clip(75 + rng.randint(-12,12), 40, 110))
            inst.notes.append(pretty_midi.Note(velocity=vel, pitch=pitch, start=t+jitter, end=t+jitter+dur))
            last_pitch = pitch
        t += step

def _roman_dict(mode): return MAJOR_ROMAN if mode=="major" else MINOR_ROMAN

def generate_full_track(emotion: str, minutes: float = 2.5, seed: int | None = None,
                        out_mid: str | None = None, out_wav: str | None = None):
    assert emotion in EMO_PROFILE, f"unknown emotion {emotion}"
    rng = random.Random(seed if seed is not None else int(time.time()*1000) % 2_147_483_647)
    prof = EMO_PROFILE[emotion]
    bpm, key, progression, density = _choose(prof, rng)

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ù…ÛŒØ²Ø§Ù†â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø²Ù…Ø§Ù† Ù‡Ø¯Ù
    sec_per_bar = 4*(60.0/bpm)
    total_bars = max(16, int((minutes*60)/sec_per_bar))   # Ø­Ø¯Ø§Ù‚Ù„ 16 Ù…ÛŒØ²Ø§Ù† Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®ØªØ§Ø±
    # Ø³Ø§Ø®ØªØ§Ø± Ø³Ø§Ø¯Ù‡: A (40%) â€“ B (40%) â€“ A' (20%)
    bars_A = int(total_bars*0.4)
    bars_B = int(total_bars*0.4)
    bars_A2 = total_bars - (bars_A+bars_B)

    pm = pretty_midi.PrettyMIDI()
    inst_har = pretty_midi.Instrument(program=48)    # Strings pad
    inst_mel = pretty_midi.Instrument(program=0)     # Piano
    pm.instruments += [inst_har, inst_mel]
    inst_drm = pretty_midi.Instrument(program=0, is_drum=True) if prof["drums"] else None
    if inst_drm: pm.instruments.append(inst_drm)

    ROM = _roman_dict(prof["mode"])
    beat_dur = (60.0/bpm)

    def add_section(start_bar, num_bars, key_this, progression_this, density_mul=1.0, final_fill=False):
        for i in range(num_bars):
            roman = progression_this[i % len(progression_this)]
            off, qual = ROM[roman]
            root = (key_this + off) % 128
            start = (start_bar+i)*4*beat_dur
            end   = (start_bar+i+1)*4*beat_dur
            _add_chord(inst_har, start, end, root, qual, rng=rng)
            if inst_drm: _drum_bar(inst_drm, start, beat_dur, rng)
        _add_melody(inst_mel, start_bar*4*beat_dur, num_bars, bpm, prof["mode"], key_this, density*density_mul, rng)
        if final_fill and inst_drm:
            _drum_fill(inst_drm, (start_bar+num_bars-1)*4*beat_dur, beat_dur, rng)

    # Ø³Ú©Ø´Ù† A
    add_section(0, bars_A, key, progression, density_mul=1.0, final_fill=True)

    # Ø³Ú©Ø´Ù† B  : Ø§Ø­ØªÙ…Ø§Ù„ Ù…Ø¯ÙˆÙ„Ø§Ø³ÛŒÙˆÙ† + ØªØºÛŒÛŒØ± Ù¾Ø±ÙˆÚ¯Ø±Ø´Ù† Ùˆ Ø¯Ø§Ù†Ø³ÛŒØªÙ‡
    mod_key = key + rng.choice([-2, 2, 0])   # Ù…Ù‚Ø¯Ø§Ø±ÛŒ ØºØ§ÙÙ„Ú¯ÛŒØ±ÛŒ
    new_prog = rng.choice(prof["progressions"])
    add_section(bars_A, bars_B, mod_key, new_prog, density_mul=rng.uniform(0.9, 1.2), final_fill=True)

    # Ø³Ú©Ø´Ù† Aâ€™ (Ø¨Ø§Ø²Ú¯Ø´Øª Ø¨Ø§ Ú©Ù…ÛŒ ØªØºÛŒÛŒØ± Ù…Ù„ÙˆØ¯ÛŒ)
    add_section(bars_A+bars_B, bars_A2, key, progression, density_mul=rng.uniform(0.8, 1.1), final_fill=False)

    # Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§
    stamp = int(time.time())
    out_mid = out_mid or f"{emotion.lower()}_{minutes:.1f}min_{stamp}.mid"
    out_wav = out_wav or f"{emotion.lower()}_{minutes:.1f}min_{stamp}.wav"
    pm.write(out_mid)

    # Ø±Ù†Ø¯Ø± Ø¨Ù‡ WAV (Ø§ÙˆÙ„ ØªÙ„Ø§Ø´ Ø¨Ø§ pyfluidsynthØ› Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§ â†’ midi2audio)
    try:
        audio = pm.fluidsynth(fs=22050, sf2_path=SF2_PATH)
        sf.write(out_wav, audio, 22050)
    except Exception as e:
        print("[render] fallback to midi2audio:", e)
        FluidSynth(sound_font=SF2_PATH, sample_rate=22050).midi_to_audio(out_mid, out_wav)

    meta = {
        "emotion": emotion,
        "minutes": minutes,
        "bpm": bpm,
        "mode": prof["mode"],
        "key_midi": key,
        "progression_A": progression,
        "progression_B": new_prog,
        "drums": bool(inst_drm),
        "comment": prof["comment"],
        "bars_total": total_bars
    }
    return out_mid, out_wav, meta

import soundfile as sf
from midi2audio import FluidSynth

def midi_to_wav(mid_path, wav_path="gen.wav", sr=22050, sf2_path=SF2_PATH):
    #  A:  pretty_midi.fluidsynth (Ù†ÛŒØ§Ø² Ø¨Ù‡ pyFluidSynth)
    try:
        audio = pretty_midi.PrettyMIDI(mid_path).fluidsynth(fs=sr, sf2_path=sf2_path)
        sf.write(wav_path, audio, sr)
        return wav_path
    except Exception as e:
        print("[midi_to_wav] fallback to midi2audio:", e)
        #  B: midi2audio (Ø§Ø² Ø¨Ø§ÛŒÙ†Ø±ÛŒ fluidsynth Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯)
        FluidSynth(sound_font=sf2_path, sample_rate=sr).midi_to_audio(mid_path, wav_path)
        return wav_path

import librosa

TARGETS = {
    "HAPPY":    dict(mode="major", tempo=(115,140), density=(6,10), want_dissonance=False, want_syncopation=True),
    "SAD":      dict(mode="minor", tempo=(60,85),   density=(2,5),  want_dissonance=False, want_syncopation=False),
    "ANGRY":    dict(mode="minor", tempo=(130,170), density=(8,14), want_dissonance=True,  want_syncopation=True),
    "FEAR":     dict(mode="minor", tempo=(50,75),   density=(1,4),  want_dissonance=True,  want_syncopation=False),
    "SURPRISE": dict(mode="major", tempo=(115,145), density=(6,12), want_dissonance=False, want_syncopation=True),
    "OTHER":    dict(mode="major", tempo=(85,110),  density=(4,8),  want_dissonance=False, want_syncopation=False),
}

def analyze_midi(mid_path, emotion):
    pm = pretty_midi.PrettyMIDI(mid_path)
    mel = next((ins for ins in pm.instruments if not ins.is_drum), None)
    note_count = len(mel.notes) if mel else 0
    bars = EMO_TO_PARAMS[emotion]["bars"]
    density = note_count / max(1,bars)

    bpm_guess = EMO_TO_PARAMS[emotion]["bpm"]

    has_dim_like = any((n.end-n.start)<0.15 for ins in pm.instruments for n in ins.notes if not ins.is_drum)
    step = 60.0/bpm_guess/2
    sync_onsets = 0
    if mel:
        for n in mel.notes:
            frac = (n.start/step) % 2
            if 0.4 < frac < 1.6: sync_onsets += 1
    sync_ratio = (sync_onsets / max(1, len(mel.notes))) if mel else 0.0

    target = TARGETS[emotion]
    mode_match = 1.0 if (EMO_TO_PARAMS[emotion]["mode"] == target["mode"]) else 0.0
    tempo_fit  = 1.0 if (target["tempo"][0] <= bpm_guess <= target["tempo"][1]) else 0.0
    d_lo, d_hi = target["density"]
    if density<d_lo: density_fit = max(0.0, 1 - (d_lo-density)/d_lo)
    elif density>d_hi: density_fit = max(0.0, 1 - (density-d_hi)/d_hi)
    else: density_fit = 1.0
    disson_fit = 1.0 if (target["want_dissonance"] and has_dim_like) or (not target["want_dissonance"] and not has_dim_like) else 0.5
    sync_fit   = min(1.0, sync_ratio*1.5) if target["want_syncopation"] else (1.0 - min(1.0, sync_ratio*1.5))

    score = 0.25*mode_match + 0.25*tempo_fit + 0.25*density_fit + 0.15*disson_fit + 0.10*sync_fit
    return {
        "bpm": bpm_guess, "bars": bars,
        "note_density_per_bar": round(density,2),
        "mode_match": round(mode_match,2),
        "tempo_fit": round(tempo_fit,2),
        "density_fit": round(density_fit,2),
        "dissonance_fit": round(disson_fit,2),
        "syncopation_fit": round(sync_fit,2),
        "overall_structural_score": round(float(score),3),
    }

from transformers import pipeline

# Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ ØµÙˆØªÛŒ Ø¨Ø§ superb
audio_clf = pipeline("audio-classification", model="superb/wav2vec2-base-superb-er", top_k=None)
AUD_MAP = { "hap":"HAPPY","sad":"SAD","ang":"ANGRY","fea":"FEAR","sur":"SURPRISE","neu":"OTHER" }

def analyze_midi_struct(mid_path, meta):
    pm = pretty_midi.PrettyMIDI(mid_path)
    mel = next((ins for ins in pm.instruments if not ins.is_drum), None)
    note_count = len(mel.notes) if mel else 0
    density = note_count / max(1, meta["bars_total"])
    bpm = meta["bpm"]
    # Ø³ÛŒÙ…Ù¾Ù„ ÙØ±Ù…ÙˆÙ„
    has_dim_like = any((n.end-n.start)<0.15 for ins in pm.instruments for n in ins.notes if not ins.is_drum)
    step = 60.0/bpm/2
    sync_onsets = 0
    if mel:
        for n in mel.notes:
            frac = (n.start/step) % 2
            if 0.4 < frac < 1.6: sync_onsets += 1
    sync_ratio = (sync_onsets / max(1, len(mel.notes))) if mel else 0.0

    # Ø­Ø¯ÙˆØ¯ Ù‡Ø¯Ù
    TARGETS = {
        "HAPPY":    dict(mode="major", tempo=(115,140), density=(6,10), want_dissonance=False, want_syncopation=True),
        "SAD":      dict(mode="minor", tempo=(60,85),   density=(2,5),  want_dissonance=False, want_syncopation=False),
        "ANGRY":    dict(mode="minor", tempo=(130,170), density=(8,14), want_dissonance=True,  want_syncopation=True),
        "FEAR":     dict(mode="minor", tempo=(50,75),   density=(1,4),  want_dissonance=True,  want_syncopation=False),
        "SURPRISE": dict(mode="major", tempo=(115,145), density=(6,12), want_dissonance=False, want_syncopation=True),
        "OTHER":    dict(mode="major", tempo=(85,110),  density=(4,8),  want_dissonance=False, want_syncopation=False),
    }
    target = TARGETS[meta["emotion"]]
    mode_match = 1.0 if (meta["mode"] == target["mode"]) else 0.0
    tempo_fit  = 1.0 if (target["tempo"][0] <= bpm <= target["tempo"][1]) else 0.0
    d_lo, d_hi = target["density"]
    if density<d_lo: density_fit = max(0.0, 1 - (d_lo-density)/max(d_lo,1))
    elif density>d_hi: density_fit = max(0.0, 1 - (density-d_hi)/max(d_hi,1))
    else: density_fit = 1.0
    disson_fit = 1.0 if (target["want_dissonance"] and has_dim_like) or (not target["want_dissonance"] and not has_dim_like) else 0.5
    sync_fit   = min(1.0, sync_ratio*1.5) if target["want_syncopation"] else (1.0 - min(1.0, sync_ratio*1.5))
    score = 0.25*mode_match + 0.25*tempo_fit + 0.25*density_fit + 0.15*disson_fit + 0.10*sync_fit

    return {
        "bpm": bpm,
        "note_density_per_bar": round(density,2),
        "mode_match": round(mode_match,2),
        "tempo_fit": round(tempo_fit,2),
        "density_fit": round(density_fit,2),
        "dissonance_fit": round(disson_fit,2),
        "syncopation_fit": round(sync_fit,2),
        "overall_structural_score": round(float(score),3),
    }

def evaluate_audio_emotion(wav_path, target_emotion):
    out = audio_clf(wav_path)
    top = max(out, key=lambda x:x["score"])
    mapped = AUD_MAP.get(top["label"].lower()[:3], "OTHER")
    return {
        "audio_pred": mapped,
        "audio_conf": round(float(top["score"]),3),
        "audio_match": 1.0 if mapped==target_emotion else 0.0
    }

def generate_and_evaluate_full(emotion:str, minutes:float=3.0, seed:int|None=None):
    mid, wav, meta = generate_full_track(emotion, minutes=minutes, seed=seed)
    struct = analyze_midi_struct(mid, meta)
    audio  = evaluate_audio_emotion(wav, emotion)
    final_score = round(0.7*struct["overall_structural_score"] + 0.3*audio["audio_match"], 3)
    report = dict(meta); report.update(struct); report.update(audio); report["final_score"] = final_score
    return wav, report

def generate_and_evaluate(emotion:str, seed:int=42, out_mid="gen.mid", out_wav="gen.wav"):
    mid, params = generate_midi_for_emotion(emotion, seed=seed, out_mid=out_mid)
    wav = midi_to_wav(mid, wav_path=out_wav, sr=22050, sf2_path=SF2_PATH)
    report = analyze_midi(mid, emotion)
    return wav, report

# Ù…Ø«Ø§Ù„ Ø§Ø¬Ø±Ø§: Ø§Ø² Ù„ÛŒØ¨Ù„ Ø®Ø±ÙˆØ¬ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒâ€ŒØ§Øª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
wav_path, report = generate_and_evaluate("HAPPY", seed=123)
print("WAV:", wav_path)
print(report)

"""# Ú©Ù„ Ù¾Ø±ÙˆÚ˜Ù‡ Ø¨Ø§Ù‡Ù… ==> Ø§Ø² ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³ ØªØ§ ØªÙˆÙ„ÛŒØ¯ Ù…ÙˆØ³ÛŒÙ‚ÛŒ

## ÙÙ‚Ø· Ø¯Ø±Ø§Ù…Ø² Ùˆ Ù¾ÛŒØ§Ù†Ùˆ
"""

# =======================   Ù‡Ù…Ú¯ÛŒ  ==========================
# Ù†ØµØ¨ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§
!apt-get -qq -y install fluidsynth
!pip install -q gradio transformers[sentencepiece] soundfile pillow pyFluidSynth==1.3.3 \
                pretty_midi music21 librosa midi2audio langdetect tensorflow opencv-python-headless

# SoundFont Ø¨Ø§Ú©ÛŒÙÛŒØª
!wget -q https://github.com/pepaslabs/fluidsynth-soundfonts/raw/master/FluidR3_GM/FluidR3_GM.sf2 -O /content/FluidR3_GM.sf2 \
  || wget -q https://github.com/urish/cyberbotics/raw/master/projects/robotbenchmark/resources/fluidr3/FluidR3_GM.sf2 -O /content/FluidR3_GM.sf2

# Ù…Ø¯Ù„ FER (mini_XCEPTION)
!wget -q https://raw.githubusercontent.com/oarriaga/face_classification/master/trained_models/emotion_models/fer2013_mini_XCEPTION.102-0.66.hdf5 -O /content/emotion_model.h5

# ------------------ Import & Config (Ø¨Ø³ØªÙ† GPU Ø¨Ø±Ø§ÛŒ TF) ----------------
import os, io, time, math, random, json
os.environ["CUDA_VISIBLE_DEVICES"] = os.environ.get("CUDA_VISIBLE_DEVICES", "")
# ÙˆÙ„ÛŒ ØªÙ†Ø³ÙˆØ± Ø±Ø§ Ù…Ø¬Ø¨ÙˆØ± Ø¨Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø³ÛŒ Ù¾ÛŒ ÛŒÙˆ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ Ø§Ø±ÙˆØ± Ù†Ø¯Ù‡Ø¯
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES_TF"] = "-1"   # Ù…Ø§Ø±Ú©Ø± Ù…Ø­Ù„ÛŒ (ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ ÙˆØ¶ÙˆØ­)

import numpy as np
import gradio as gr
import soundfile as sf
from PIL import Image

import torch
from transformers import pipeline

import tensorflow as tf
try:
    tf.config.set_visible_devices([], 'GPU')
except Exception:
    pass

import cv2
from tensorflow.keras.models import load_model

import pretty_midi
from midi2audio import FluidSynth
from langdetect import detect

# ----------------------- Device Ø¨Ø±Ø§ÛŒ Transformer ----------------------
DEVICE = 0 if torch.cuda.is_available() else -1
print("Device for Transformers:", "cuda:0" if DEVICE==0 else "cpu")
SF2_PATH = "/content/FluidR3_GM.sf2"

# ----------------------------- Emoji & Maps ---------------------------
EMOJI = {"ANGRY":"ğŸ˜ ","FEAR":"ğŸ˜¨","HAPPY":"ğŸ˜ƒ","SAD":"ğŸ˜¢","SURPRISE":"ğŸ˜²","OTHER":"ğŸ˜"}

# ------------------------- Pipelines Ù…ØªÙ†/ØµÙˆØª --------------------------
pipe_fa = pipeline(
    "text-classification",
    model="/content/final_parsbert_emotion",
    tokenizer="/content/final_parsbert_emotion",
    device=DEVICE
)
pipe_en = pipeline(
    "text-classification",
    model="nateraw/bert-base-uncased-emotion",
    tokenizer="nateraw/bert-base-uncased-emotion",
    device=DEVICE,
    return_all_scores=False
)
def map_en(label):
    l = label.lower()
    if l in ["joy","love"]: return "HAPPY"
    if l == "sadness":      return "SAD"
    if l == "anger":        return "ANGRY"
    if l == "fear":         return "FEAR"
    if l == "surprise":     return "SURPRISE"
    return "OTHER"

audio_clf = pipeline("audio-classification", model="superb/wav2vec2-base-superb-er", device=DEVICE, top_k=None)
AUD_MAP = {"hap":"HAPPY","sad":"SAD","ang":"ANGRY","fea":"FEAR","sur":"SURPRISE","neu":"OTHER"}

# ------------------------- Ù…Ø¯Ù„ ØªØµÙˆÛŒØ± (TF Ø±ÙˆÛŒ CPU) ---------------------
emotion_cnn = load_model("/content/emotion_model.h5", compile=False)
FER_LABELS = ['angry','disgust','fear','happy','sad','surprise','neutral']
IMG_MAP = {
    "angry":"ANGRY","disgust":"ANGRY","fear":"FEAR",
    "happy":"HAPPY","sad":"SAD","surprise":"SURPRISE","neutral":"OTHER"
}
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

# -------------------------- ØªØ´Ø®ÛŒØµ Ø§Ø­Ø³Ø§Ø³â€ŒÙ‡Ø§ ----------------------------
def analyze_text(text, threshold):
    lang = "fa"
    try:
        lang = detect(text)
    except:
        pass
    if lang == "fa":
        out = pipe_fa(text)[0]
        idx = int(out["label"].split("_")[-1])
        FA = ['ANGRY','FEAR','HAPPY','HATE','OTHER','SAD','SURPRISE']
        label = FA[idx] if FA[idx]!="HATE" else "ANGRY"
        score = float(out["score"])
    else:
        out = pipe_en(text)[0]
        label = map_en(out["label"])
        score = float(out["score"])
    if score < threshold:
        label = "OTHER"
    return label, round(score,3), EMOJI[label]

def analyze_image(img, threshold):
    try:
        gray = cv2.cvtColor(np.array(img.convert("RGB")), cv2.COLOR_RGB2GRAY)
        faces = face_cascade.detectMultiScale(gray, 1.3, 5)
        if len(faces)==0:
            h,w = gray.shape
            sz = min(h,w)//2
            x = (w-sz)//2; y = (h-sz)//2
            roi = gray[y:y+sz, x:x+sz]
        else:
            x,y,w,h = faces[0]
            roi = gray[y:y+h, x:x+w]
        face = cv2.resize(roi, (64,64), interpolation=cv2.INTER_AREA)
        face = face.astype("float32")/255.0
        face = face.reshape(1,64,64,1)
        preds = emotion_cnn.predict(face, verbose=0)[0]
        idx   = int(np.argmax(preds))
        score = float(preds[idx])
        raw   = FER_LABELS[idx]
        label = IMG_MAP.get(raw, "OTHER")
        return label, round(score,3), EMOJI[label]
    except Exception as e:
        # Ù„Ø§Ú¯ Ú©ÙˆÚ†Ú© Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØ¨Ø§Ú¯
        print("[analyze_image error]", repr(e))
        return "OTHER", 0.0, EMOJI["OTHER"]

def analyze_audio(filepath, threshold):
    try:
        out = audio_clf(filepath)
        if not out:
            return "OTHER", 0.0, EMOJI["OTHER"]
        top = max(out, key=lambda x: x["score"])
        raw = top["label"].lower()
        score = float(top["score"])
        label = AUD_MAP.get(raw[:3], "OTHER")
        if score < threshold:
            label = "OTHER"
        return label, round(score,3), EMOJI[label]
    except Exception as e:
        print("[analyze_audio error]", repr(e))
        return "OTHER", 0.0, EMOJI["OTHER"]

def analyze_all(text, image, audio_path, threshold):
    if text and str(text).strip():
        return analyze_text(text, threshold)
    if image is not None:
        return analyze_image(image, threshold)
    if audio_path:
        return analyze_audio(audio_path, threshold)
    return "OTHER", 0.0, EMOJI["OTHER"]

# --------------------- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø³Ø§Ø®Øª Ù…ÙˆØ³ÛŒÙ‚ÛŒ (Rule-based) ----------------
EMO_PROFILE = {
    "HAPPY":    {"mode":"major", "tempo_range":(115,140), "base_keys":[60,62,65,67],
                 "progressions":[["I","V","vi","IV"],["I","IV","V","I"],["I","vi","IV","V"]],
                 "melody_density_range":(0.55,0.85), "drums":True,  "comment":"Ø´Ø§Ø¯ Ùˆ Ù¾Ø±Ø§Ù†Ø±Ú˜ÛŒ"},
    "SAD":      {"mode":"minor", "tempo_range":(60,85),   "base_keys":[57,55,52],
                 "progressions":[["i","VI","III","VII"],["i","iv","i","VII"]],
                 "melody_density_range":(0.25,0.45), "drums":False, "comment":"Ú©Ù†Ø¯ Ùˆ Ø§Ø­Ø³Ø§Ø³ÛŒ"},
    "ANGRY":    {"mode":"minor", "tempo_range":(130,170), "base_keys":[50,48,53],
                 "progressions":[["i","bVI","bVII","i"],["i","bIIÂ°","bVII","i"]],
                 "melody_density_range":(0.7,0.95),  "drums":True,  "comment":"ØªÙ‡Ø§Ø¬Ù…ÛŒ Ùˆ Ú©ÙˆØ¨Ù†Ø¯Ù‡"},
    "FEAR":     {"mode":"minor", "tempo_range":(50,75),   "base_keys":[57,58,55],
                 "progressions":[["i","bIIÂ°","i","bVI"],["i","iv","bIIÂ°","i"]],
                 "melody_density_range":(0.2,0.4),   "drums":False, "comment":"Ù…Ø±Ù…ÙˆØ² Ùˆ ØªÛŒØ±Ù‡"},
    "SURPRISE": {"mode":"major", "tempo_range":(120,145), "base_keys":[65,67,69],
                 "progressions":[["I","V","IV","â™­VII"],["I","IV","V","â™­VII"]],
                 "melody_density_range":(0.6,0.9),   "drums":True,  "comment":"Ø¬Ù‡Ø´ÛŒ/Ø³ÛŒÙ†Ú©ÙˆÙ¾"},
    "OTHER":    {"mode":"major", "tempo_range":(85,110),  "base_keys":[60,62],
                 "progressions":[["I","ii","IV","V"],["I","IV","ii","V"]],
                 "melody_density_range":(0.4,0.65),  "drums":False, "comment":"Ø®Ù†Ø«ÛŒ/Ù¾Ø³â€ŒØ²Ù…ÛŒÙ†Ù‡"},
}
MAJOR_ROMAN = {"I":(0,"maj"),"ii":(2,"min"),"iii":(4,"min"),"IV":(5,"maj"),"V":(7,"maj"),"vi":(9,"min"),"â™­VII":(10,"maj")}
MINOR_ROMAN = {"i":(0,"min"),"iv":(5,"min"),"v":(7,"min"),"VI":(8,"maj"),"VII":(10,"maj"),"bVI":(8,"maj"),"bVII":(10,"maj"),"bIIÂ°":(1,"dim")}
QUAL2INTS = {"maj":[0,4,7], "min":[0,3,7], "dim":[0,3,6]}

def _scale(mode): return [0,2,4,5,7,9,11] if mode=="major" else [0,2,3,5,7,8,10]
def _choose(prof, rng):
    bpm = rng.randint(*prof["tempo_range"])
    key = rng.choice(prof["base_keys"])
    prog = rng.choice(prof["progressions"])
    density = rng.uniform(*prof["melody_density_range"])
    return bpm, key, prog, density

def _add_chord(inst, start, end, root, qual, rng=None):
    for i in QUAL2INTS[qual]:
        vel = int(np.clip(72 + (rng.randint(-6,6) if rng else 0), 40, 100))
        inst.notes.append(pretty_midi.Note(velocity=vel, pitch=int(root+i), start=start, end=end))

def _drum_bar(inst_drm, bar_start, beat_dur, rng):
    for b in range(4):
        t = bar_start + b*beat_dur
        inst_drm.notes.append(pretty_midi.Note(100 if b%2==0 else 90, pitch=36 if b%2==0 else 38, start=t, end=t+0.06))
        for off in [0, beat_dur/2]:
            jit = rng.uniform(-0.01, 0.01)
            inst_drm.notes.append(pretty_midi.Note(62, pitch=42, start=t+off+jit, end=t+off+jit+0.03))

def _drum_fill(inst_drm, bar_start, beat_dur, rng):
    for i, pitch in enumerate([47,45,43,41]):  # toms
        t = bar_start + i*(beat_dur/2)
        inst_drm.notes.append(pretty_midi.Note(86+rng.randint(-5,5), pitch=pitch, start=t, end=t+0.12))

def _add_melody(inst, start, bars, bpm, mode, key, density, rng):
    scale = _scale(mode); step = 0.5*(60.0/bpm)
    total_sub = int(bars*4*2)
    t = start; last_pitch = key+7
    lo, hi = key-5, key+(14 if mode=="major" else 12)
    motif = [rng.choice(scale) for _ in range(rng.choice([3,4,5]))]
    for s in range(total_sub):
        if rng.random() < density:
            deg = (motif[s % len(motif)]) if rng.random()<0.75 else rng.choice(scale)
            base = key + deg + rng.choice([-12,0,12])
            pitch = int(np.clip(int(0.7*last_pitch + 0.3*base), lo, hi))
            jitter = rng.uniform(-0.02, 0.02)
            dur = step if rng.random()<0.7 else step*2
            vel = int(np.clip(78 + rng.randint(-12,12), 40, 110))
            inst.notes.append(pretty_midi.Note(velocity=vel, pitch=pitch, start=t+jitter, end=t+jitter+dur))
            last_pitch = pitch
        t += step

def generate_full_track(emotion: str, minutes: float = 3.0, seed: int | None = None):
    assert emotion in EMO_PROFILE, f"unknown emotion {emotion}"
    rng = random.Random(seed if seed is not None else int(time.time()*1000) % 2_147_483_647)
    prof = EMO_PROFILE[emotion]
    bpm, key, progression, density = _choose(prof, rng)
    sec_per_bar = 4*(60.0/bpm)
    total_bars = max(16, int((minutes*60)/sec_per_bar))
    bars_A = int(total_bars*0.4); bars_B = int(total_bars*0.4); bars_A2 = total_bars - (bars_A+bars_B)

    pm = pretty_midi.PrettyMIDI()
    inst_har = pretty_midi.Instrument(program=rng.choice([48,49,50,51,52]))
    inst_mel = pretty_midi.Instrument(program=rng.choice([0,1,4,5,6,16,24]))
    pm.instruments += [inst_har, inst_mel]
    inst_drm = pretty_midi.Instrument(program=0, is_drum=True) if prof["drums"] else None
    if inst_drm: pm.instruments.append(inst_drm)
    beat_dur = (60.0/bpm)
    ROM = MAJOR_ROMAN if prof["mode"]=="major" else MINOR_ROMAN

    def add_section(start_bar, num_bars, key_this, progression_this, density_mul=1.0, final_fill=False):
        for i in range(num_bars):
            roman = progression_this[i % len(progression_this)]
            off, qual = ROM[roman]
            root = (key_this + off) % 128
            start = (start_bar+i)*4*beat_dur
            end   = (start_bar+i+1)*4*beat_dur
            _add_chord(inst_har, start, end, root, qual, rng)
            if inst_drm: _drum_bar(inst_drm, start, beat_dur, rng)
        _add_melody(inst_mel, start_bar*4*beat_dur, num_bars, bpm, prof["mode"], key_this, density*density_mul, rng)
        if final_fill and inst_drm:
            _drum_fill(inst_drm, (start_bar+num_bars-1)*4*beat_dur, beat_dur, rng)

    add_section(0, bars_A, key, progression, density_mul=1.0, final_fill=True)
    mod_key = key + rng.choice([-2, 2, 0])
    new_prog = rng.choice(prof["progressions"])
    add_section(bars_A, bars_B, mod_key, new_prog, density_mul=random.uniform(0.9, 1.2), final_fill=True)
    add_section(bars_A+bars_B, bars_A2, key, progression, density_mul=random.uniform(0.8, 1.1), final_fill=False)

    stamp = int(time.time())
    out_mid = f"{emotion.lower()}_{minutes:.1f}min_{stamp}.mid"
    out_wav = f"{emotion.lower()}_{minutes:.1f}min_{stamp}.wav"
    pm.write(out_mid)

    # Ø±Ù†Ø¯Ø± WAV
    try:
        audio = pm.fluidsynth(fs=22050, sf2_path=SF2_PATH)
        sf.write(out_wav, audio, 22050)
    except Exception as e:
        print("[render fallback]", e)
        FluidSynth(sound_font=SF2_PATH, sample_rate=22050).midi_to_audio(out_mid, out_wav)

    meta = {"emotion": emotion, "minutes": minutes, "bpm": bpm, "mode": prof["mode"],
            "key_midi": key, "progression_A": progression, "progression_B": new_prog,
            "drums": bool(inst_drm), "comment": prof["comment"], "bars_total": total_bars}
    return out_mid, out_wav, meta

# ---------------------------- Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…ÙˆØ³ÛŒÙ‚ÛŒ --------------------------
TARGETS = {
    "HAPPY":    dict(mode="major", tempo=(115,140), density=(6,10), want_dissonance=False, want_syncopation=True),
    "SAD":      dict(mode="minor", tempo=(60,85),   density=(2,5),  want_dissonance=False, want_syncopation=False),
    "ANGRY":    dict(mode="minor", tempo=(130,170), density=(8,14), want_dissonance=True,  want_syncopation=True),
    "FEAR":     dict(mode="minor", tempo=(50,75),   density=(1,4),  want_dissonance=True,  want_syncopation=False),
    "SURPRISE": dict(mode="major", tempo=(115,145), density=(6,12), want_dissonance=False, want_syncopation=True),
    "OTHER":    dict(mode="major", tempo=(85,110),  density=(4,8),  want_dissonance=False, want_syncopation=False),
}

def analyze_midi_struct(mid_path, emotion, build_params):
    pm = pretty_midi.PrettyMIDI(mid_path)
    mel = next((ins for ins in pm.instruments if not ins.is_drum), None)
    note_count = len(mel.notes) if mel else 0
    bars = build_params["bars_total"]
    density = note_count / max(1,bars)
    bpm_guess = build_params["bpm"]
    has_dim_like = any((n.end-n.start) < (60.0/bpm_guess)/4 for ins in pm.instruments for n in ins.notes if not ins.is_drum)
    step = 60.0/bpm_guess/2
    sync_onsets = 0
    if mel and mel.notes:
        for n in mel.notes:
            frac = (n.start/step) % 2
            if 0.4 < frac < 1.6: sync_onsets += 1
    sync_ratio = (sync_onsets / max(1, len(mel.notes))) if (mel and mel.notes) else 0.0

    target = TARGETS[emotion]
    mode_match = 1.0 if (build_params["mode"] == target["mode"]) else 0.0
    tempo_fit  = 1.0 if (target["tempo"][0] <= bpm_guess <= target["tempo"][1]) else 0.0
    d_lo, d_hi = target["density"]
    if density<d_lo: density_fit = max(0.0, 1 - (d_lo-density)/max(1e-6,d_lo))
    elif density>d_hi: density_fit = max(0.0, 1 - (density-d_hi)/max(1e-6,d_hi))
    else: density_fit = 1.0
    sync_fit   = min(1.0, sync_ratio*1.5) if target["want_syncopation"] else (1.0 - min(1.0, sync_ratio*1.5))
    disson_fit = 1.0 if (target["want_dissonance"] and has_dim_like) or (not target["want_dissonance"] and not has_dim_like) else 0.5

    score = 0.25*mode_match + 0.25*tempo_fit + 0.25*density_fit + 0.15*disson_fit + 0.10*sync_fit
    return {
        "bpm": bpm_guess, "bars": bars, "note_density_per_bar": round(density,2),
        "mode_match": round(mode_match,2), "tempo_fit": round(tempo_fit,2),
        "density_fit": round(density_fit,2), "dissonance_fit": round(disson_fit,2),
        "syncopation_fit": round(sync_fit,2), "overall_structural_score": round(float(score),3),
        "build_params": build_params,
    }

def evaluate_with_audio_classifier(wav_path, target_emotion:str):
    try:
        out = audio_clf(wav_path)
        top = max(out, key=lambda x:x["score"])
        mapped = AUD_MAP.get(top["label"].lower()[:3], "OTHER")
        return {"predicted_emotion_audio": mapped,
                "predicted_confidence_audio": round(float(top["score"]),3),
                "target_emotion": target_emotion,
                "audio_match": 1.0 if mapped==target_emotion else 0.0}
    except Exception as e:
        print("[audio_eval error]", repr(e))
        return {"predicted_emotion_audio": None,"predicted_confidence_audio":0.0,
                "target_emotion": target_emotion,"audio_match":0.0}

def generate_and_evaluate_full(emotion:str, minutes:float=3.0, seed=None):
    mid_path, wav_path, meta = generate_full_track(emotion, minutes=minutes, seed=seed)
    struct = analyze_midi_struct(mid_path, emotion, meta)
    audio_eval = evaluate_with_audio_classifier(wav_path, emotion)
    final_score = round(0.75*struct["overall_structural_score"] + 0.25*audio_eval["audio_match"], 3)
    report = {"emotion": emotion, "structural": struct, "audio_eval": audio_eval, "final_score": final_score}
    return wav_path, report

# ----------------------------- Gradio UI ------------------------------
def analyze_and_make_music(text, image, audio_path, threshold, minutes, seed):
    label, score, emoji = analyze_all(text, image, audio_path, threshold)
    try:
        seed_int = None if (seed is None or str(seed).strip()=="") else int(seed)
    except:
        seed_int = None
    wav, rep = generate_and_evaluate_full(label, minutes=float(minutes), seed=seed_int)
    rep["detected_label"] = label
    rep["detected_confidence"] = score
    return label, score, emoji, wav, rep

with gr.Blocks(css="""
  #root {max-width:980px;margin:auto;padding:16px;}
  .out {font-size:1.1rem;text-align:center;}
""") as app:
    gr.Markdown("## ØªØ´Ø®ÛŒØµ Ø§Ø­Ø³Ø§Ø³ (Ù…ØªÙ†/ØªØµÙˆÛŒØ±/ØµÙˆØª) â†’ ğŸµ Ø³Ø§Ø®Øª Ù…ÙˆØ³ÛŒÙ‚ÛŒ Ú†Ù†Ø¯â€ŒØ¯Ù‚ÛŒÙ‚Ù‡â€ŒØ§ÛŒ Ù…Ø·Ø§Ø¨Ù‚ Ø§Ø­Ø³Ø§Ø³ + Ú¯Ø²Ø§Ø±Ø´ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ")
    with gr.Row():
        with gr.Column():
            txt = gr.Textbox(label="Ù…ØªÙ† (ÙØ§Ø±Ø³ÛŒ/Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ)", lines=3, placeholder="Ù…Ø«Ø§Ù„: Ø§Ù…Ø±ÙˆØ² Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ´Ø­Ø§Ù„Ù…!")
            img = gr.Image(label="ØªØµÙˆÛŒØ± Ú†Ù‡Ø±Ù‡", type="pil")
            aud = gr.Audio(label="ØµÙˆØª (wav/mp3)", type="filepath")
        with gr.Column():
            thr  = gr.Slider(0,1,0.7,step=0.01, label="Ø¢Ø³ØªØ§Ù†Ù‡ OTHER Ø¨Ø±Ø§ÛŒ Ù…ØªÙ†/ØµÙˆØª")
            mins = gr.Slider(1.0, 6.0, value=3.0, step=0.5, label="Ù…Ø¯Øª Ù…ÙˆØ³ÛŒÙ‚ÛŒ (Ø¯Ù‚ÛŒÙ‚Ù‡)")
            seed = gr.Textbox(value="", label="Seed (Ø®Ø§Ù„ÛŒ = Ù‡Ø± Ø¨Ø§Ø± Ù…ØªÙØ§ÙˆØª)")
            go   = gr.Button("ğŸ”ğŸ¶ ØªØ­Ù„ÛŒÙ„ Ùˆ Ø³Ø§Ø®Øª Ù…ÙˆØ³ÛŒÙ‚ÛŒ", variant="primary")

    with gr.Row():
        o1 = gr.Textbox(label="Ø§Ø­Ø³Ø§Ø³",    interactive=False, elem_classes="out")
        o2 = gr.Textbox(label="Ø§Ø¹ØªÙ…Ø§Ø¯",   interactive=False, elem_classes="out")
        o3 = gr.Textbox(label="Ø§ÛŒÙ…ÙˆØ¬ÛŒ",   interactive=False, elem_classes="out")

    audio_out = gr.Audio(label="ğŸ§ Ù…ÙˆØ³ÛŒÙ‚ÛŒ Ø®Ø±ÙˆØ¬ÛŒ", type="filepath")
    rep_out   = gr.JSON(label="ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ (Ø³Ø§Ø®ØªØ§Ø±ÛŒ + Ú©Ù„Ø§Ø³ÛŒÙØ§ÛŒØ± ØµÙˆØª Ø±ÙˆÛŒ WAV)")

    go.click(analyze_and_make_music, [txt, img, aud, thr, mins, seed], [o1, o2, o3, audio_out, rep_out])

app.launch(share=False)
# =====================================================================

"""## Ù…ÙˆÙ„ØªÛŒ Ø³Ø§Ø²

### 1- ØºØ±Ø¨ÛŒ
"""

# =======================  Ù‡Ù…Ú¯ÛŒ Ø¨Ø§Ù‡Ù…  ==========================
# Ù†ØµØ¨ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§
!apt-get -qq -y install fluidsynth
!pip install -q gradio transformers[sentencepiece] soundfile pillow pyFluidSynth==1.3.3 \
                pretty_midi music21 librosa midi2audio langdetect tensorflow opencv-python-headless

# SoundFont Ø¨Ø§Ú©ÛŒÙÛŒØª
!wget -q https://github.com/pepaslabs/fluidsynth-soundfonts/raw/master/FluidR3_GM/FluidR3_GM.sf2 -O /content/FluidR3_GM.sf2 \
  || wget -q https://github.com/urish/cyberbotics/raw/master/projects/robotbenchmark/resources/fluidr3/FluidR3_GM.sf2 -O /content/FluidR3_GM.sf2

# Ù…Ø¯Ù„ FER (mini_XCEPTION)
!wget -q https://raw.githubusercontent.com/oarriaga/face_classification/master/trained_models/emotion_models/fer2013_mini_XCEPTION.102-0.66.hdf5 -O /content/emotion_model.h5

# ------------------ Import & Config (Ø¨Ø³ØªÙ† GPU Ø¨Ø±Ø§ÛŒ TF) ----------------
import os, io, time, math, random, json
os.environ["CUDA_VISIBLE_DEVICES"] = os.environ.get("CUDA_VISIBLE_DEVICES", "")
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES_TF"] = "-1"   # Ù…Ø§Ø±Ú©Ø± Ù…Ø­Ù„ÛŒ (ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ ÙˆØ¶ÙˆØ­)

import numpy as np
import gradio as gr
import soundfile as sf
from PIL import Image

import torch
from transformers import pipeline

import tensorflow as tf
try:
    tf.config.set_visible_devices([], 'GPU')
except Exception:
    pass

import cv2
from tensorflow.keras.models import load_model

import pretty_midi
from midi2audio import FluidSynth
from langdetect import detect

# ----------------------- Device Ø¨Ø±Ø§ÛŒ Transformer ----------------------
DEVICE = 0 if torch.cuda.is_available() else -1
print("Device for Transformers:", "cuda:0" if DEVICE==0 else "cpu")
SF2_PATH = "/content/FluidR3_GM.sf2"

def _normalize_limit(x, peak=0.98):
    if x.ndim == 1:
        x = x[:, None]
    # Ø­Ø°Ù NaN/Inf
    x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)
    # DC offset
    x = x - np.mean(x, axis=0, keepdims=True)
    # Ù†Ø±Ù…Ø§Ù„Ø§ÛŒØ²
    mx = np.max(np.abs(x)) + 1e-9
    x = x / mx
    # Ø³Ø§ÙØª-Ù„ÛŒÙ…ÛŒØª (tanh) Ùˆ Ù…Ù‚ÛŒØ§Ø³ Ù‚Ù„Ù‡
    x = np.tanh(1.2 * x) * (peak / np.max(np.abs(np.tanh(1.2 * x)) + 1e-9))
    return x.squeeze()

def _render_wav_robust(pm, out_mid, out_wav, sf2_path=SF2_PATH, sr=22050):
    # ØªÙ„Ø§Ø´ Û±: pyfluidsynth
    try:
        audio = pm.fluidsynth(fs=sr, sf2_path=sf2_path)
        audio = _normalize_limit(audio)
        rms = np.sqrt(np.mean(audio**2))
        if (rms < 1e-4) or (not np.isfinite(rms)):
            raise ValueError("silent_or_invalid_audio")
        sf.write(out_wav, audio, sr)
        return out_wav, sr
    except Exception as e:
        print("[render] fallback to midi2audio:", e)
        # ØªÙ„Ø§Ø´ Û²: midi2audio (flï»¿uidsynth CLI)
        pm.write(out_mid)
        FluidSynth(sound_font=sf2_path, sample_rate=sr).midi_to_audio(out_mid, out_wav)
        y, sr2 = sf.read(out_wav, always_2d=False)
        y = _normalize_limit(y)
        sf.write(out_wav, y, sr2)
        return out_wav, sr2

# ----------------------------- Emoji & Maps ---------------------------
EMOJI = {"ANGRY":"ğŸ˜ ","FEAR":"ğŸ˜¨","HAPPY":"ğŸ˜ƒ","SAD":"ğŸ˜¢","SURPRISE":"ğŸ˜²","OTHER":"ğŸ˜"}

# ------------------------- Pipelines Ù…ØªÙ†/ØµÙˆØª --------------------------
pipe_fa = pipeline(
    "text-classification",
    model="/content/final_parsbert_emotion",
    tokenizer="/content/final_parsbert_emotion",
    device=DEVICE
)
pipe_en = pipeline(
    "text-classification",
    model="nateraw/bert-base-uncased-emotion",
    tokenizer="nateraw/bert-base-uncased-emotion",
    device=DEVICE,
    return_all_scores=False
)
def map_en(label):
    l = label.lower()
    if l in ["joy","love"]: return "HAPPY"
    if l == "sadness":      return "SAD"
    if l == "anger":        return "ANGRY"
    if l == "fear":         return "FEAR"
    if l == "surprise":     return "SURPRISE"
    return "OTHER"

audio_clf = pipeline("audio-classification", model="superb/wav2vec2-base-superb-er", device=DEVICE, top_k=None)
AUD_MAP = {"hap":"HAPPY","sad":"SAD","ang":"ANGRY","fea":"FEAR","sur":"SURPRISE","neu":"OTHER"}

# ------------------------- Ù…Ø¯Ù„ ØªØµÙˆÛŒØ± (TF Ø±ÙˆÛŒ CPU) ---------------------
emotion_cnn = load_model("/content/emotion_model.h5", compile=False)
FER_LABELS = ['angry','disgust','fear','happy','sad','surprise','neutral']
IMG_MAP = {
    "angry":"ANGRY","disgust":"ANGRY","fear":"FEAR",
    "happy":"HAPPY","sad":"SAD","surprise":"SURPRISE","neutral":"OTHER"
}
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

# -------------------------- ØªØ´Ø®ÛŒØµ Ø§Ø­Ø³Ø§Ø³â€ŒÙ‡Ø§ ----------------------------
def analyze_text(text, threshold):
    lang = "fa"
    try:
        lang = detect(text)
    except:
        pass
    if lang == "fa":
        out = pipe_fa(text)[0]
        idx = int(out["label"].split("_")[-1])
        FA = ['ANGRY','FEAR','HAPPY','HATE','OTHER','SAD','SURPRISE']
        label = FA[idx] if FA[idx]!="HATE" else "ANGRY"
        score = float(out["score"])
    else:
        out = pipe_en(text)[0]
        label = map_en(out["label"])
        score = float(out["score"])
    if score < threshold:
        label = "OTHER"
    return label, round(score,3), EMOJI[label]

def analyze_image(img, threshold):
    try:
        gray = cv2.cvtColor(np.array(img.convert("RGB")), cv2.COLOR_RGB2GRAY)
        faces = face_cascade.detectMultiScale(gray, 1.3, 5)
        if len(faces)==0:
            h,w = gray.shape
            sz = min(h,w)//2
            x = (w-sz)//2; y = (h-sz)//2
            roi = gray[y:y+sz, x:x+sz]
        else:
            x,y,w,h = faces[0]
            roi = gray[y:y+h, x:x+w]
        face = cv2.resize(roi, (64,64), interpolation=cv2.INTER_AREA)
        face = face.astype("float32")/255.0
        face = face.reshape(1,64,64,1)
        preds = emotion_cnn.predict(face, verbose=0)[0]
        idx   = int(np.argmax(preds))
        score = float(preds[idx])
        raw   = FER_LABELS[idx]
        label = IMG_MAP.get(raw, "OTHER")
        return label, round(score,3), EMOJI[label]
    except Exception as e:
        # Ù„Ø§Ú¯ Ú©ÙˆÚ†Ú© Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØ¨Ø§Ú¯
        print("[analyze_image error]", repr(e))
        return "OTHER", 0.0, EMOJI["OTHER"]

def analyze_audio(filepath, threshold):
    try:
        out = audio_clf(filepath)
        if not out:
            return "OTHER", 0.0, EMOJI["OTHER"]
        top = max(out, key=lambda x: x["score"])
        raw = top["label"].lower()
        score = float(top["score"])
        label = AUD_MAP.get(raw[:3], "OTHER")
        if score < threshold:
            label = "OTHER"
        return label, round(score,3), EMOJI[label]
    except Exception as e:
        print("[analyze_audio error]", repr(e))
        return "OTHER", 0.0, EMOJI["OTHER"]

def analyze_all(text, image, audio_path, threshold):
    if text and str(text).strip():
        return analyze_text(text, threshold)
    if image is not None:
        return analyze_image(image, threshold)
    if audio_path:
        return analyze_audio(audio_path, threshold)
    return "OTHER", 0.0, EMOJI["OTHER"]

# --------------------- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø³Ø§Ø®Øª Ù…ÙˆØ³ÛŒÙ‚ÛŒ (Rule-based) ----------------
# GM Programs (0-based)
GM = dict(
    piano=[0,1,4,5,6],                        # Piano family
    guitar_ac=[24,25], guitar_el=[26,27,28],  # Acoustic & Electric Guitars
    guitar_drive=[29,30],                     # Overdrive/Distortion
    bass=[32,33,34,39],                       # Acoustic/Electric/Synth Bass
    strings=[48,49,50],                       # Strings / SynthStrings
    brass=[60,61],                             # French Horn / Brass Section
    mallet=[11,12,13,14],                      # Vibraphone/Marimba/Xylo
    pad=[88,89,91,94],                         # Warm/Poly/Choir pads
    lead=[80,81,82,84]                         # Lead synths
)

EMO_PROFILE = {
    "HAPPY": {
        "mode":"major", "tempo_range":(118,140), "base_keys":[60,62,65,67],
        "progressions":[["I","V","vi","IV"],["I","vi","IV","V"],["I","IV","V","I"]],
        "melody_density_range":(0.6,0.9), "drums":True,  "comment":"Ø´Ø§Ø¯ Ùˆ Ù¾Ø±Ø§Ù†Ø±Ú˜ÛŒ",
        "roles_pool": ["pad","piano","guitar_ac","guitar_el","bass","strings","lead","mallet"]
    },
    "SAD": {
        "mode":"minor", "tempo_range":(60,82), "base_keys":[57,55,52],
        "progressions":[["i","VI","III","VII"],["i","iv","i","VII"]],
        "melody_density_range":(0.25,0.5), "drums":False, "comment":"Ú©Ù†Ø¯ Ùˆ Ø§Ø­Ø³Ø§Ø³ÛŒ",
        "roles_pool": ["piano","strings","pad","mallet","bass"]
    },
    "ANGRY": {
        "mode":"minor", "tempo_range":(135,170), "base_keys":[50,48,53],
        "progressions":[["i","bVI","bVII","i"],["i","bIIÂ°","bVII","i"]],
        "melody_density_range":(0.7,0.95), "drums":True, "comment":"ØªÙ‡Ø§Ø¬Ù…ÛŒ Ùˆ Ú©ÙˆØ¨Ù†Ø¯Ù‡",
        "roles_pool": ["guitar_drive","guitar_el","bass","lead","pad"]
    },
    "FEAR": {
        "mode":"minor", "tempo_range":(52,72), "base_keys":[57,58,55],
        "progressions":[["i","bIIÂ°","i","bVI"],["i","iv","bIIÂ°","i"]],
        "melody_density_range":(0.2,0.45), "drums":False, "comment":"Ù…Ø±Ù…ÙˆØ² Ùˆ ØªÛŒØ±Ù‡",
        "roles_pool": ["pad","strings","mallet","bass","lead"]
    },
    "SURPRISE": {
        "mode":"major", "tempo_range":(122,145), "base_keys":[65,67,69],
        "progressions":[["I","V","IV","â™­VII"],["I","IV","V","â™­VII"]],
        "melody_density_range":(0.6,0.95), "drums":True, "comment":"Ø¬Ù‡Ø´ÛŒ/Ø³ÛŒÙ†Ú©ÙˆÙ¾",
        "roles_pool": ["piano","guitar_ac","lead","brass","mallet","bass","pad"]
    },
    "OTHER": {
        "mode":"major", "tempo_range":(90,110), "base_keys":[60,62],
        "progressions":[["I","ii","IV","V"],["I","IV","ii","V"]],
        "melody_density_range":(0.4,0.65), "drums":False, "comment":"Ø®Ù†Ø«ÛŒ/Ù¾Ø³â€ŒØ²Ù…ÛŒÙ†Ù‡",
        "roles_pool": ["piano","pad","strings","mallet","bass"]
    },
}
MAJOR_ROMAN = {"I":(0,"maj"),"ii":(2,"min"),"iii":(4,"min"),"IV":(5,"maj"),"V":(7,"maj"),"vi":(9,"min"),"â™­VII":(10,"maj")}
MINOR_ROMAN = {"i":(0,"min"),"iv":(5,"min"),"v":(7,"min"),"VI":(8,"maj"),"VII":(10,"maj"),"bVI":(8,"maj"),"bVII":(10,"maj"),"bIIÂ°":(1,"dim")}
QUAL2INTS = {"maj":[0,4,7], "min":[0,3,7], "dim":[0,3,6]}

def _scale(mode): return [0,2,4,5,7,9,11] if mode=="major" else [0,2,3,5,7,8,10]

def _choose(prof, rng):
    bpm = rng.randint(*prof["tempo_range"])
    key = rng.choice(prof["base_keys"])
    prog = rng.choice(prof["progressions"])
    density = rng.uniform(*prof["melody_density_range"])
    # Ù‡Ø± Ø¨Ø§Ø± Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ§ÛŒ Ø§Ø² Ù†Ù‚Ø´â€ŒÙ‡Ø§ Ø±Ø§ ØªØµØ§Ø¯ÙÛŒ ÙØ¹Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
    roles = prof["roles_pool"][:]
    rng.shuffle(roles)
    # Ø­Ø¯Ø§Ù‚Ù„ 3 Ù†Ù‚Ø´ØŒ Ø­Ø¯Ø§Ú©Ø«Ø± 6 (Ø¨Ù‡ Ù‡Ù…Ø±Ø§Ù‡ Ù‡Ø§Ø±Ù…ÙˆÙ†ÛŒ Ùˆ Ù…Ù„ÙˆØ¯ÛŒ ØªØ¶Ù…ÛŒÙ†ÛŒ)
    roles = roles[:rng.randint(3, min(6, len(roles)))]
    return bpm, key, prog, density, roles

def _root_pitch(key, roman, ROM):
    off, qual = ROM[roman]
    return (key + off) % 128, qual

def _add_chord(inst, start, end, root, qual, rng, spread=0):
    # Ø¢Ú©ÙˆØ±Ø¯ Ø¨Ø³ØªÙ‡ + Ø§Ø®ØªÛŒØ§Ø±ÛŒ Ø§Ø³Ù¾Ø±Ø¯ (Ø§Ú©ØªØ§Ùˆ Ù¾Ø§ÛŒÛŒÙ†/Ø¨Ø§Ù„Ø§)
    ints = QUAL2INTS[qual]
    for i in ints:
        p = int(root+i + spread)
        vel = int(np.clip(72 + rng.randint(-8,8), 40, 105))
        inst.notes.append(pretty_midi.Note(velocity=vel, pitch=p, start=start, end=end))

def _strum_guitar(inst, start, end, root, qual, rng, beat_dur, direction="down"):
    # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ø³ØªØ±Ø§Ù…: Ù†Øªâ€ŒÙ‡Ø§ÛŒ Ø¢Ú©ÙˆØ±Ø¯ Ø¨Ø§ ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø®ÛŒÙ„ÛŒ Ú©ÙˆØªØ§Ù‡
    ints = QUAL2INTS[qual][:]
    if direction=="up": ints = list(reversed(ints))
    t = start
    for i in ints:
        p = int(root+i)
        d = min(0.25*beat_dur, end-start)  # Ú©ÙˆØªØ§Ù‡
        vel = int(np.clip(76 + rng.randint(-10,10), 45, 110))
        jit = rng.uniform(-0.01,0.01)
        inst.notes.append(pretty_midi.Note(velocity=vel, pitch=p, start=t+jit, end=min(t+jit+d, end)))
        t += 0.04 + rng.uniform(0.0, 0.02)

def _arp_pattern(inst, start, end, root, qual, rng, step):
    # Ø¢Ø±Ù¾Ú˜ Ø±ÙˆÛŒ 8th/16th Ù‡Ø§
    ints = QUAL2INTS[qual]
    seq = ints + ints[::-1]               # up & down
    t = start
    idx = rng.randint(0,len(seq)-1)
    while t < end:
        p = int(root + seq[idx % len(seq)])
        dur = step * (1.0 if rng.random()<0.7 else 0.5)
        vel = int(np.clip(74 + rng.randint(-12,12), 40, 110))
        jit = rng.uniform(-0.015,0.015)
        inst.notes.append(pretty_midi.Note(velocity=vel, pitch=p, start=t+jit, end=min(t+jit+dur, end)))
        t += step
        idx += 1

def _bass_pattern(inst, start, end, root, bpm, rng, style="eighths"):
    beat = 60.0/bpm
    if style=="long":
        p = max(0, root-12)
        inst.notes.append(pretty_midi.Note(velocity=78, pitch=p, start=start, end=end))
    elif style=="syncop":
        # Ø±Ùˆ Ø§Ù„Ú¯ÙˆÛŒ off-beat
        for k in [0.5, 1.5, 2.5, 3.5]:
            t = start + k*beat + rng.uniform(-0.01,0.01)
            p = max(0, root-12 + rng.choice([0, -5, 7])//7*0)  # Ø¨ÛŒØ´ØªØ± Ø±ÙˆØª
            inst.notes.append(pretty_midi.Note(velocity=85+rng.randint(-6,6), pitch=max(0,root-12), start=t, end=min(t+0.3, end)))
    else:  # eighths
        t = start
        while t < end:
            p = max(0, root-12)
            d = 0.45*beat
            inst.notes.append(pretty_midi.Note(velocity=82+rng.randint(-8,8), pitch=p, start=t, end=min(t+d, end)))
            t += 0.5*beat

def _lead_melody(inst, start, bars, bpm, mode, key, density, rng):
    scale = _scale(mode)
    step = 0.5*(60.0/bpm)       # 8th
    total_sub = int(bars*4*2)
    t = start
    last_pitch = key + 7 + rng.choice([-12,0,12])
    lo, hi = key-5, key+(16 if mode=="major" else 13)
    motif = [rng.choice(scale) for _ in range(rng.choice([3,4,5,6]))]
    for s in range(total_sub):
        if rng.random() < density:
            deg = motif[s % len(motif)] if rng.random()<0.75 else rng.choice(scale)
            base = key + deg + rng.choice([-12,0,12])
            pitch = int(np.clip(int(0.65*last_pitch + 0.35*base), lo, hi))
            dur = step if rng.random()<0.65 else step*2
            jit = rng.uniform(-0.02,0.02)
            vel = int(np.clip(80 + rng.randint(-14,14), 40, 115))
            inst.notes.append(pretty_midi.Note(velocity=vel, pitch=pitch, start=t+jit, end=t+jit+dur))
            last_pitch = pitch
        t += step

def _drum_bar(inst_drm, bar_start, beat_dur, rng, style="straight"):
    for b in range(4):
        t = bar_start + b*beat_dur
        # Kick/Snare
        inst_drm.notes.append(pretty_midi.Note(100 if b%2==0 else 92, pitch=36 if b%2==0 else 38, start=t, end=t+0.06))
        # Hats
        if style=="swing":
            inst_drm.notes.append(pretty_midi.Note(62, pitch=42, start=t, end=t+0.03))
            inst_drm.notes.append(pretty_midi.Note(62, pitch=42, start=t+beat_dur*2/3, end=t+beat_dur*2/3+0.03))
        else:
            inst_drm.notes.append(pretty_midi.Note(62, pitch=42, start=t, end=t+0.03))
            inst_drm.notes.append(pretty_midi.Note(62, pitch=42, start=t+beat_dur/2, end=t+beat_dur/2+0.03))

def _drum_fill(inst_drm, bar_start, beat_dur, rng):
    for i, pitch in enumerate([47,45,43,41]):  # toms
        t = bar_start + i*(beat_dur/2)
        inst_drm.notes.append(pretty_midi.Note(86+rng.randint(-6,6), pitch=pitch, start=t, end=t+0.12))

def _make_inst(program_choices, rng):
    return pretty_midi.Instrument(program=rng.choice(program_choices))

def _cc(inst, cc, value, time=0.0):
    inst.control_changes.append(pretty_midi.ControlChange(cc, int(value), time))

def _power_chord_riff(inst_L, inst_R, start, end, root, bpm, rng):
    """Ø±ÛŒÙÛŒ Ø³Ø§Ø¯Ù‡ Ø¨Ø± Ù¾Ø§ÛŒÙ‡ Ø±ÙˆØª+ÛµØ§Ù…+Ø§Ú©ØªØ§ÙˆØ› Ø¯Ø§Ø¨Ù„â€ŒØªÙØ±Ú© L/R"""
    beat = 60.0/bpm
    t = start
    while t < end:
        dur = min(0.45*beat, end-t)
        velL = int(np.clip(96 + rng.randint(-8,8), 60, 127))
        velR = int(np.clip(94 + rng.randint(-8,8), 60, 127))
        for off in [0,7,12]:
            inst_L.notes.append(pretty_midi.Note(velocity=velL, pitch=int(root+off),
                                                 start=t+rng.uniform(-0.005,0.005),
                                                 end=min(t+dur, end)))
            inst_R.notes.append(pretty_midi.Note(velocity=velR, pitch=int(root+off),
                                                 start=t+rng.uniform(-0.005,0.005),
                                                 end=min(t+dur, end)))
        t += 0.5*beat  # 8th drive

def generate_full_track(emotion: str, minutes: float = 3.0, seed: int | None = None):
    assert emotion in EMO_PROFILE, f"unknown emotion {emotion}"
    rng = random.Random(seed if seed is not None else int(time.time()*1000) % 2_147_483_647)
    prof = EMO_PROFILE[emotion]
    bpm, key, progression, density, roles = _choose(prof, rng)
    ROM = MAJOR_ROMAN if prof["mode"]=="major" else MINOR_ROMAN
    sec_per_bar = 4*(60.0/bpm)
    total_bars = max(16, int((minutes*60)/sec_per_bar))
    bars_A = int(total_bars*0.4); bars_B = int(total_bars*0.4); bars_A2 = total_bars - (bars_A+bars_B)
    beat_dur = (60.0/bpm)

    pm = pretty_midi.PrettyMIDI()

    # Harmony bed
    if "pad" in roles:
        inst_har = _make_inst(GM["pad"]+GM["strings"], rng)
    elif "strings" in roles:
        inst_har = _make_inst(GM["strings"], rng)
    else:
        inst_har = _make_inst(GM["piano"], rng)
    _cc(inst_har, 7, 96)   # Volume
    _cc(inst_har, 10, 64)  # Pan
    pm.instruments.append(inst_har)

    # Optional roles with mix
    insts = {}
    def _add(name, progs, vol, pan):
        insts[name] = _make_inst(progs, rng)
        _cc(insts[name], 7, vol); _cc(insts[name], 10, pan)
        pm.instruments.append(insts[name])

    if "piano" in roles:        _add("piano", GM["piano"],        100, 64)
    if "guitar_ac" in roles:    _add("gtr_ac", GM["guitar_ac"],    96,  54)
    if "guitar_el" in roles:    _add("gtr_el", GM["guitar_el"],    96,  74)
    if "guitar_drive" in roles:
        _add("gtrL", GM["guitar_drive"], 110, 32)  # Left
        _add("gtrR", GM["guitar_drive"], 110, 96)  # Right
    if "bass" in roles:         _add("bass", GM["bass"],          110, 64)
    if "strings" in roles:      _add("str2", GM["strings"],         92, 80)
    if "brass" in roles:        _add("brs", GM["brass"],            96, 70)
    if "mallet" in roles:       _add("mlt", GM["mallet"],           92, 58)
    if "lead" in roles:         _add("lead", GM["lead"],           104, 64)

    inst_drm = pretty_midi.Instrument(program=0, is_drum=True) if prof["drums"] else None
    if inst_drm: pm.instruments.append(inst_drm)

    def render_section(start_bar, num_bars, key_this, prog_this, dens_mul=1.0, final_fill=False):
        for i in range(num_bars):
            roman = prog_this[i % len(prog_this)]
            root, qual = _root_pitch(key_this, roman, ROM)
            start = (start_bar+i)*4*beat_dur
            end   = (start_bar+i+1)*4*beat_dur

            # Harmony bed
            _add_chord(inst_har, start, end, root, qual, rng, spread=rng.choice([0,12,-12,0]))

            # FEAR: Ø§Ø±Ú¯Ø§Ù† Ú©Ù„ÛŒØ³Ø§ Ù„Ø§Ù†Ú¯â€ŒÙ†ÙˆØª (Ø¯Ø± ØµÙˆØ±Øª Ø­Ø¶ÙˆØ± lead â†’ Ù†Ù‚Ø´ lead Ø¨Ø§ Ø§Ø±Ú¯Ø§Ù†)
            if emotion=="FEAR":
                org = pretty_midi.Instrument(program=19)  # Church Organ (0-based)
                _cc(org,7,100); _cc(org,10,64)
                pm.instruments.append(org)
                insts["fear_org"] = org
                org_note = pretty_midi.Note(velocity=84, pitch=int(root), start=start, end=end)
                org.notes.append(org_note)

            # Guitar strum / Power-chords
            has_drive = ("gtrL" in insts and "gtrR" in insts)
            if has_drive and emotion=="ANGRY":
                _power_chord_riff(insts["gtrL"], insts["gtrR"], start, end, root, bpm, rng)
            else:
                gtr = insts.get("gtr_el") or insts.get("gtr_ac")
                if gtr: _strum_guitar(gtr, start, end, root, qual, rng, beat_dur, direction=rng.choice(["down","up"]))

            # Piano arpeggio
            if "piano" in insts and rng.random()<0.85:
                _arp_pattern(insts["piano"], start, end, root, qual, rng, step=0.5*beat_dur)

            # Brass/Mallet hits
            if "brs" in insts and rng.random()<0.4 and emotion in ["HAPPY","SURPRISE"]:
                _add_chord(insts["brs"], start, min(start+beat_dur, end), root+12, qual, rng)
            if "mlt" in insts and rng.random()<0.5:
                _arp_pattern(insts["mlt"], start, min(start+2*beat_dur, end), root+12, qual, rng, step=0.25*beat_dur)

            # Bass
            if "bass" in insts:
                style = "eighths"
                if emotion in ["SAD","FEAR"]: style = "long"
                if emotion in ["HAPPY","SURPRISE"]: style = rng.choice(["eighths","syncop"])
                if emotion=="ANGRY": style = "eighths"
                _bass_pattern(insts["bass"], start, end, root, bpm, rng, style=style)

            # Drums
            if inst_drm:
                groove = "swing" if (emotion=="SURPRISE" and rng.random()<0.4) else "straight"
                _drum_bar(inst_drm, start, beat_dur, rng, style=groove)

        # Lead
        lead_inst = insts.get("lead") or insts.get("piano") or inst_har
        _lead_melody(lead_inst, start_bar*4*beat_dur, num_bars, bpm, EMO_PROFILE[emotion]["mode"], key_this, density*dens_mul, rng)

        if final_fill and inst_drm:
            _drum_fill(inst_drm, (start_bar+num_bars-1)*4*beat_dur, beat_dur, rng)

    # Ø³Ø§Ø®ØªØ§Ø± Aâ€“Bâ€“Aâ€²
    render_section(0, bars_A, key, progression, dens_mul=1.0, final_fill=True)
    mod_key = key + rng.choice([-2, 2, 0])
    new_prog = rng.choice(prof["progressions"])
    render_section(bars_A, bars_B, mod_key, new_prog, dens_mul=random.uniform(0.9,1.25), final_fill=True)
    render_section(bars_A+bars_B, bars_A2, key, progression, dens_mul=random.uniform(0.85,1.15), final_fill=False)

    stamp = int(time.time())
    out_mid = f"{emotion.lower()}_{minutes:.1f}min_{stamp}.mid"
    out_wav = f"{emotion.lower()}_{minutes:.1f}min_{stamp}.wav"

    #  Ø±Ù†Ø¯Ø± Ù…Ø­Ú©Ù… Ùˆ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒâ€ŒØ´Ø¯Ù‡
    wav_path, sr = _render_wav_robust(pm, out_mid, out_wav, sf2_path=SF2_PATH, sr=22050)

    meta = {
        "emotion": emotion, "minutes": minutes, "bpm": bpm, "mode": prof["mode"], "key_midi": key,
        "progression_A": progression, "progression_B": new_prog, "drums": bool(inst_drm),
        "comment": prof["comment"], "bars_total": total_bars, "roles_used": roles, "sr": sr
    }
    return out_mid, wav_path, meta


# ---------------------------- Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…ÙˆØ³ÛŒÙ‚ÛŒ --------------------------
TARGETS = {
    "HAPPY":    dict(mode="major", tempo=(115,140), density=(6,10), want_dissonance=False, want_syncopation=True),
    "SAD":      dict(mode="minor", tempo=(60,85),   density=(2,5),  want_dissonance=False, want_syncopation=False),
    "ANGRY":    dict(mode="minor", tempo=(130,170), density=(8,14), want_dissonance=True,  want_syncopation=True),
    "FEAR":     dict(mode="minor", tempo=(50,75),   density=(1,4),  want_dissonance=True,  want_syncopation=False),
    "SURPRISE": dict(mode="major", tempo=(115,145), density=(6,12), want_dissonance=False, want_syncopation=True),
    "OTHER":    dict(mode="major", tempo=(85,110),  density=(4,8),  want_dissonance=False, want_syncopation=False),
}

def analyze_midi_struct(mid_path, emotion, build_params):
    pm = pretty_midi.PrettyMIDI(mid_path)
    non_drums = [ins for ins in pm.instruments if not ins.is_drum]
    note_count = sum(len(ins.notes) for ins in non_drums)
    bars = build_params["bars_total"]
    density = note_count / max(1,bars)
    bpm_guess = build_params["bpm"]

    # Ø³ÛŒÙ†Ú©ÙˆÙ¾: Ù†Ø³Ø¨Øª Ù†ÙØªâ€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø±ÙˆÛŒ off-beat Ù…ÛŒâ€ŒØ§ÙØªÙ†Ø¯
    step = 60.0/bpm_guess/2
    sync_onsets, total_notes = 0, 0
    for ins in non_drums:
        for n in ins.notes:
            total_notes += 1
            frac = (n.start/step) % 2
            if 0.4 < frac < 1.6: sync_onsets += 1
    sync_ratio = (sync_onsets / max(1,total_notes)) if total_notes else 0.0

    # Ø¯ÛŒØ³ÙˆÙ†Ø§Ù†Ø³ ØªÙ‚Ø±ÛŒØ¨ÛŒ: ÙˆØ¬ÙˆØ¯ Ù†Øªâ€ŒÙ‡Ø§ÛŒ Ø®ÛŒÙ„ÛŒ Ú©ÙˆØªØ§Ù‡ (ØªØ²Ø¦ÛŒÙ†Ø§Øª ØªÙ†Ø¯)
    has_dim_like = any((n.end-n.start) < (60.0/bpm_guess)/4 for ins in non_drums for n in ins.notes)

    target = TARGETS[emotion]
    mode_match = 1.0 if (build_params["mode"] == target["mode"]) else 0.0
    tempo_fit  = 1.0 if (target["tempo"][0] <= bpm_guess <= target["tempo"][1]) else 0.0
    d_lo, d_hi = target["density"]
    if density<d_lo: density_fit = max(0.0, 1 - (d_lo-density)/max(1e-6,d_lo))
    elif density>d_hi: density_fit = max(0.0, 1 - (density-d_hi)/max(1e-6,d_hi))
    else: density_fit = 1.0
    sync_fit   = min(1.0, sync_ratio*1.5) if target["want_syncopation"] else (1.0 - min(1.0, sync_ratio*1.5))
    disson_fit = 1.0 if (target["want_dissonance"] and has_dim_like) or (not target["want_dissonance"] and not has_dim_like) else 0.5

    score = 0.25*mode_match + 0.25*tempo_fit + 0.25*density_fit + 0.15*disson_fit + 0.10*sync_fit
    return {
        "bpm": bpm_guess, "bars": bars, "note_density_per_bar": round(density,2),
        "mode_match": round(mode_match,2), "tempo_fit": round(tempo_fit,2),
        "density_fit": round(density_fit,2), "dissonance_fit": round(disson_fit,2),
        "syncopation_fit": round(sync_fit,2), "overall_structural_score": round(float(score),3),
        "build_params": build_params,
    }


def evaluate_with_audio_classifier(wav_path, target_emotion:str):
    try:
        out = audio_clf(wav_path)
        top = max(out, key=lambda x:x["score"])
        mapped = AUD_MAP.get(top["label"].lower()[:3], "OTHER")
        return {"predicted_emotion_audio": mapped,
                "predicted_confidence_audio": round(float(top["score"]),3),
                "target_emotion": target_emotion,
                "audio_match": 1.0 if mapped==target_emotion else 0.0}
    except Exception as e:
        print("[audio_eval error]", repr(e))
        return {"predicted_emotion_audio": None,"predicted_confidence_audio":0.0,
                "target_emotion": target_emotion,"audio_match":0.0}

def generate_and_evaluate_full(emotion:str, minutes:float=3.0, seed=None):
    mid_path, wav_path, meta = generate_full_track(emotion, minutes=minutes, seed=seed)
    struct = analyze_midi_struct(mid_path, emotion, meta)
    audio_eval = evaluate_with_audio_classifier(wav_path, emotion)
    final_score = round(0.75*struct["overall_structural_score"] + 0.25*audio_eval["audio_match"], 3)
    report = {"emotion": emotion, "structural": struct, "audio_eval": audio_eval, "final_score": final_score}
    return wav_path, report

# ----------------------------- Gradio UI ------------------------------
def analyze_and_make_music(text, image, audio_path, threshold, minutes, seed):
    label, score, emoji = analyze_all(text, image, audio_path, threshold)
    try:
        seed_int = None if (seed is None or str(seed).strip()=="") else int(seed)
    except:
        seed_int = None
    wav_path, rep = generate_and_evaluate_full(label, minutes=float(minutes), seed=seed_int)
    # â† rep Ø§Ø² Ù‡Ù…ÙˆÙ† Ù‚Ø¨Ù„
    # Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù¾Ø®Ø´ Ø¯Ø§Ø®Ù„ Gradio Ø¨Ø®ÙˆØ§Ù†:
    y, sr = sf.read(wav_path, always_2d=False)
    if y.ndim > 1:  # Ø§Ø³ØªØ±ÛŒÙˆ â†’ Ù…ÙˆÙ†Ùˆ Ù…Ù„Ø§ÛŒÙ… Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ù…Ø±ÙˆØ±Ú¯Ø±
        y = np.mean(y, axis=1)
    y = _normalize_limit(y)  # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø¹Ø¯Ù… Ú©Ù„ÛŒÙ¾ÛŒÙ†Ú¯/Ù†ÙˆÛŒØ²
    rep["detected_label"] = label
    rep["detected_confidence"] = score
    return label, score, emoji, (sr, y.astype(np.float32)), rep


with gr.Blocks(css="""
  #root {max-width:980px;margin:auto;padding:16px;}
  .out {font-size:1.1rem;text-align:center;}
""") as app:
    gr.Markdown("## ØªØ´Ø®ÛŒØµ Ø§Ø­Ø³Ø§Ø³ (Ù…ØªÙ†/ØªØµÙˆÛŒØ±/ØµÙˆØª) â†’ ğŸµ Ø³Ø§Ø®Øª Ù…ÙˆØ³ÛŒÙ‚ÛŒ Ú†Ù†Ø¯â€ŒØ¯Ù‚ÛŒÙ‚Ù‡â€ŒØ§ÛŒ Ù…Ø·Ø§Ø¨Ù‚ Ø§Ø­Ø³Ø§Ø³ + Ú¯Ø²Ø§Ø±Ø´ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ")
    with gr.Row():
        with gr.Column():
            txt = gr.Textbox(label="Ù…ØªÙ† (ÙØ§Ø±Ø³ÛŒ/Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ)", lines=3, placeholder="Ù…Ø«Ø§Ù„: Ø§Ù…Ø±ÙˆØ² Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ´Ø­Ø§Ù„Ù…!")
            img = gr.Image(label="ØªØµÙˆÛŒØ± Ú†Ù‡Ø±Ù‡", type="pil")
            aud = gr.Audio(label="ØµÙˆØª (wav/mp3)", type="filepath")
        with gr.Column():
            thr  = gr.Slider(0,1,0.7,step=0.01, label="Ø¢Ø³ØªØ§Ù†Ù‡ OTHER Ø¨Ø±Ø§ÛŒ Ù…ØªÙ†/ØµÙˆØª")
            mins = gr.Slider(1.0, 6.0, value=3.0, step=0.5, label="Ù…Ø¯Øª Ù…ÙˆØ³ÛŒÙ‚ÛŒ (Ø¯Ù‚ÛŒÙ‚Ù‡)")
            seed = gr.Textbox(value="", label="Seed (Ø®Ø§Ù„ÛŒ = Ù‡Ø± Ø¨Ø§Ø± Ù…ØªÙØ§ÙˆØª)")
            go   = gr.Button("ğŸ” ØªØ­Ù„ÛŒÙ„ Ùˆ Ø³Ø§Ø®Øª Ù…ÙˆØ³ÛŒÙ‚ÛŒ", variant="primary")

    with gr.Row():
        o1 = gr.Textbox(label="Ø§Ø­Ø³Ø§Ø³",    interactive=False, elem_classes="out")
        o2 = gr.Textbox(label="Ø§Ø¹ØªÙ…Ø§Ø¯",   interactive=False, elem_classes="out")
        o3 = gr.Textbox(label="Ø§ÛŒÙ…ÙˆØ¬ÛŒ",   interactive=False, elem_classes="out")

    # audio_out = gr.Audio(label="ğŸ§ Ù…ÙˆØ³ÛŒÙ‚ÛŒ Ø®Ø±ÙˆØ¬ÛŒ", type="filepath")
    audio_out = gr.Audio(label="ğŸ§ Ù…ÙˆØ³ÛŒÙ‚ÛŒ Ø®Ø±ÙˆØ¬ÛŒ", type="numpy")
    rep_out   = gr.JSON(label="ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ (Ø³Ø§Ø®ØªØ§Ø±ÛŒ + Ú©Ù„Ø§Ø³ÛŒÙØ§ÛŒØ± ØµÙˆØª Ø±ÙˆÛŒ WAV)")

    go.click(analyze_and_make_music, [txt, img, aud, thr, mins, seed], [o1, o2, o3, audio_out, rep_out])

app.launch(share=False)
# =====================================================================

"""### 2- ØºØ±Ø¨ÛŒ + Ø´Ø±Ù‚ÛŒ"""

# ======================= Ù‡Ù…Ú¯ÛŒ Ø¨Ø§Ù‡Ù… ==========================
# Ù†ØµØ¨ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§
!apt-get -qq -y install fluidsynth ffmpeg
!pip install -q gradio transformers[sentencepiece] soundfile pillow pyFluidSynth==1.3.3 \
                pretty_midi music21 librosa midi2audio langdetect tensorflow opencv-python-headless pydub

# SoundFont Ù¾ÛŒØ´â€ŒÙØ±Ø¶ (General MIDI Ø¨Ø§ Ú©ÛŒÙÛŒØª Ø®ÙˆØ¨)
!wget -q https://github.com/pepaslabs/fluidsynth-soundfonts/raw/master/FluidR3_GM/FluidR3_GM.sf2 -O /content/FluidR3_GM.sf2 \
  || wget -q https://github.com/urish/cyberbotics/raw/master/projects/robotbenchmark/resources/fluidr3/FluidR3_GM.sf2 -O /content/FluidR3_GM.sf2

# Ù…Ø¯Ù„ FER (mini_XCEPTION) Ø¨Ø±Ø§ÛŒ ØªØµÙˆÛŒØ±
!wget -q https://raw.githubusercontent.com/oarriaga/face_classification/master/trained_models/emotion_models/fer2013_mini_XCEPTION.102-0.66.hdf5 -O /content/emotion_model.h5

# ------------------ Import & Config (TF â†’ CPU) ----------------
import os, time, random, json
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"

import tensorflow as tf
try:
    tf.config.set_visible_devices([], 'GPU')
except Exception:
    pass

import numpy as np
import gradio as gr
import soundfile as sf
from PIL import Image

import torch
from transformers import pipeline
import cv2
from tensorflow.keras.models import load_model
import pretty_midi
from midi2audio import FluidSynth
from langdetect import detect
from pydub import AudioSegment  # Ø¨Ø±Ø§ÛŒ MP3

# ----------------------- Device Ø¨Ø±Ø§ÛŒ Transformers ----------------------
DEVICE = 0 if torch.cuda.is_available() else -1
print("Device for Transformers:", "cuda:0" if DEVICE==0 else "cpu")
SF2_PATH_DEFAULT = "/content/FluidR3_GM.sf2"

# ---------- Utility: Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ/Ù„ÛŒÙ…ÛŒØªÙ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø®Ø´ Ù¾Ø§ÛŒØ¯Ø§Ø± ----------
def _normalize_limit(x, peak=0.98):
    x = np.array(x)
    if x.ndim == 1:
        x = x[:, None]
    x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)
    x = x - np.mean(x, axis=0, keepdims=True)
    mx = np.max(np.abs(x)) + 1e-9
    x = x / mx
    y = np.tanh(1.2 * x)
    y = y * (peak / (np.max(np.abs(y)) + 1e-9))
    return y.squeeze()

def _render_wav_robust(pm, out_mid, out_wav, sf2_path=None, sr=22050):
    """Ù‡Ù…ÛŒØ´Ù‡ Ø§ÙˆÙ„ MIDI Ø±Ø§ Ù…ÛŒâ€ŒÙ†ÙˆÛŒØ³ÛŒÙ… ØªØ§ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒâ€ŒÙ‡Ø§ fail Ù†Ø´Ù†."""
    sf2_path = sf2_path or SF2_PATH_DEFAULT
    pm.write(out_mid)
    # ØªÙ„Ø§Ø´ Û±: pyfluidsynth
    try:
        audio = pm.fluidsynth(fs=sr, sf2_path=sf2_path)
        audio = _normalize_limit(audio)
        rms = np.sqrt(np.mean(audio**2))
        if (rms < 1e-4) or (not np.isfinite(rms)):
            raise ValueError("silent_or_invalid_audio")
        sf.write(out_wav, audio, sr)
        return out_wav, sr
    except Exception as e:
        print("[render] fallback to midi2audio:", e)
        FluidSynth(sound_font=sf2_path, sample_rate=sr).midi_to_audio(out_mid, out_wav)
        y, sr2 = sf.read(out_wav, always_2d=False)
        y = _normalize_limit(y)
        sf.write(out_wav, y, sr2)
        return out_wav, sr2

def _to_mp3(wav_path, mp3_path, bitrate="192k"):
    try:
        AudioSegment.from_file(wav_path).export(mp3_path, format="mp3", bitrate=bitrate)
        return mp3_path
    except Exception as e:
        print("[mp3] export error:", e)
        return None

# ----------------------------- Emoji & Maps ---------------------------
EMOJI = {"ANGRY":"ğŸ˜ ","FEAR":"ğŸ˜¨","HAPPY":"ğŸ˜ƒ","SAD":"ğŸ˜¢","SURPRISE":"ğŸ˜²","OTHER":"ğŸ˜"}

# ------------------------- Pipelines Ù…ØªÙ†/ØµÙˆØª --------------------------

# Ù¾Ø§Ø±Ø³â€ŒØ¨Ø±Øª Ø§Ø®ØªÛŒØ§Ø±ÛŒâ€”Ø§Ú¯Ø± Ù†Ø¨ÙˆØ¯ØŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯
try:
    pipe_fa = pipeline(
        "text-classification",
        model="/content/final_parsbert_emotion",
        tokenizer="/content/final_parsbert_emotion",
        device=DEVICE
    )
except Exception as e:
    pipe_fa = None
    print("[warn] Persian text model not found -> using English when needed")

pipe_en = pipeline(
    "text-classification",
    model="nateraw/bert-base-uncased-emotion",
    tokenizer="nateraw/bert-base-uncased-emotion",
    device=DEVICE,
    return_all_scores=False
)

def map_en(label):
    l = label.lower()
    if l in ["joy","love"]: return "HAPPY"
    if l == "sadness":      return "SAD"
    if l == "anger":        return "ANGRY"
    if l == "fear":         return "FEAR"
    if l == "surprise":     return "SURPRISE"
    return "OTHER"

audio_clf = pipeline("audio-classification", model="superb/wav2vec2-base-superb-er", device=DEVICE, top_k=None)
AUD_MAP = {"hap":"HAPPY","sad":"SAD","ang":"ANGRY","fea":"FEAR","sur":"SURPRISE","neu":"OTHER"}

# ------------------------- Ù…Ø¯Ù„ ØªØµÙˆÛŒØ± (TF Ø±ÙˆÛŒ CPU) ---------------------
emotion_cnn = load_model("/content/emotion_model.h5", compile=False)
FER_LABELS = ['angry','disgust','fear','happy','sad','surprise','neutral']
IMG_MAP = {
    "angry":"ANGRY","disgust":"ANGRY","fear":"FEAR",
    "happy":"HAPPY","sad":"SAD","surprise":"SURPRISE","neutral":"OTHER"
}
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

# -------------------------- ØªØ´Ø®ÛŒØµ Ø§Ø­Ø³Ø§Ø³â€ŒÙ‡Ø§ ----------------------------
def analyze_text(text, threshold):
    if not text or not str(text).strip():
        return "OTHER", 0.0, EMOJI["OTHER"]
    try:
        lang = detect(text)
    except:
        lang = "fa"
    if lang == "fa" and pipe_fa is not None:
        out = pipe_fa(text)[0]
        idx = int(out["label"].split("_")[-1])
        FA = ['ANGRY','FEAR','HAPPY','HATE','OTHER','SAD','SURPRISE']
        label = FA[idx] if FA[idx]!="HATE" else "ANGRY"
        score = float(out["score"])
    else:
        out = pipe_en(text)[0]
        label = map_en(out["label"])
        score = float(out["score"])
    if score < threshold:
        label = "OTHER"
    return label, round(score,3), EMOJI[label]

def analyze_image(img, threshold):
    try:
        gray = cv2.cvtColor(np.array(img.convert("RGB")), cv2.COLOR_RGB2GRAY)
        faces = face_cascade.detectMultiScale(gray, 1.3, 5)
        if len(faces)==0:
            h,w = gray.shape
            sz = min(h,w)//2
            x = (w-sz)//2; y = (h-sz)//2
            roi = gray[y:y+sz, x:x+sz]
        else:
            x,y,w,h = faces[0]
            roi = gray[y:y+h, x:x+w]
        face = cv2.resize(roi, (64,64), interpolation=cv2.INTER_AREA)
        face = face.astype("float32")/255.0
        face = face.reshape(1,64,64,1)
        preds = emotion_cnn.predict(face, verbose=0)[0]
        idx   = int(np.argmax(preds))
        score = float(preds[idx])
        raw   = FER_LABELS[idx]
        label = IMG_MAP.get(raw, "OTHER")
        return label, round(score,3), EMOJI[label]
    except Exception as e:
        print("[analyze_image error]", repr(e))
        return "OTHER", 0.0, EMOJI["OTHER"]

def analyze_audio(filepath, threshold):
    try:
        out = audio_clf(filepath)
        if not out:
            return "OTHER", 0.0, EMOJI["OTHER"]
        top = max(out, key=lambda x: x["score"])
        raw = top["label"].lower()
        score = float(top["score"])
        label = AUD_MAP.get(raw[:3], "OTHER")
        if score < threshold:
            label = "OTHER"
        return label, round(score,3), EMOJI[label]
    except Exception as e:
        print("[analyze_audio error]", repr(e))
        return "OTHER", 0.0, EMOJI["OTHER"]

def analyze_all(text, image, audio_path, threshold):
    if text and str(text).strip():
        return analyze_text(text, threshold)
    if image is not None:
        return analyze_image(image, threshold)
    if audio_path:
        return analyze_audio(audio_path, threshold)
    return "OTHER", 0.0, EMOJI["OTHER"]

# --------------------- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø³Ø§Ø®Øª Ù…ÙˆØ³ÛŒÙ‚ÛŒ ----------------
GM = dict(
    piano=[0,1,4,5,6],
    guitar_ac=[24,25], guitar_el=[26,27,28],
    guitar_drive=[29,30],
    bass=[32,33,34,39],
    strings=[48,49,50],
    brass=[60,61],
    mallet=[11,12,13,14],
    pad=[88,89,91,94],
    lead=[80,81,82,84]
)

# Ø³Ø§Ø²Ù‡Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ (ØªÙ‚Ø±ÛŒØ¨ Ø¨Ø§ GM)
PERSIAN_GM = dict(
    santur=[15],               # Dulcimer â‰ˆ Santur
    tar=[24,104,107],          # NylonGtr / Sitar / Koto
    setar=[24,104],
    oud=[24,25,104],
    kamancheh=[40],            # Violin
    ney=[77,75],               # Shakuhachi / Pan Flute
    qanun=[46,15,107],         # Harp / Dulcimer / Koto
)

# ØªÙ†Ù‡Ø§ Ø³Ø§Ø²Ù‡Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ Ø¯Ø± Ø­Ø§Ù„Øª Persian
PERSIAN_ROLES = {
    "HAPPY":    ["santur","qanun","oud","ney","kamancheh","tar","setar"],
    "SAD":      ["kamancheh","ney","santur","qanun","setar","tar"],
    "ANGRY":    ["santur","tar","kamancheh","ney","oud"],   # Ø®Ø´Ù… Ø§ÛŒØ±Ø§Ù†ÛŒ Ø¨Ø¯ÙˆÙ† Ú¯ÛŒØªØ§Ø± ØºØ±Ø¨ÛŒ
    "FEAR":     ["ney","kamancheh","santur","qanun"],
    "SURPRISE": ["santur","qanun","oud","ney","kamancheh"],
    "OTHER":    ["santur","qanun","ney","kamancheh","setar"]
}

EMO_PROFILE = {
    "HAPPY":    {"mode":"major","tempo_range":(124,142),"base_keys":[60,62,65,67],
                 "progressions":[["I","V","vi","IV"],["I","IV","V","I"],["I","V","IV","I"]],
                 "melody_density_range":(0.7,0.95),"drums":True,"comment":"Ø´Ø§Ø¯ Ùˆ Ø±Ù‚ØµØ§Ù†"},
    "SAD":      {"mode":"minor","tempo_range":(45,60),"base_keys":[57,55,52],
                 "progressions":[["i","VI","III","VII"],["i","iv","i","VII"],["i","VI","i","v"]],
                 "melody_density_range":(0.12,0.28),"drums":False,"comment":"Ø®ÛŒÙ„ÛŒ Ø¢Ø±Ø§Ù… Ùˆ Ø§ÙØ³Ø±Ø¯Ù‡"},
    "ANGRY":    {"mode":"minor","tempo_range":(150,175),"base_keys":[50,48,53],
                 "progressions":[["i","bVI","bVII","i"],["i","bIIÂ°","bVII","i"]],
                 "melody_density_range":(0.8,0.96),"drums":True,"comment":"Ø±Ø§Ú©/Ù…ØªØ§Ù„ ØªÙ‡Ø§Ø¬Ù…ÛŒ"},
    "FEAR":     {"mode":"minor","tempo_range":(50,68),"base_keys":[57,58,55],
                 "progressions":[["i","bIIÂ°","i","bVI"],["i","iv","bIIÂ°","i"]],
                 "melody_density_range":(0.12,0.3),"drums":False,"comment":"Ø¯Ù„Ù‡Ø±Ù‡â€ŒØ¢ÙˆØ±"},
    "SURPRISE": {"mode":"major","tempo_range":(122,145),"base_keys":[65,67,69],
                 "progressions":[["I","V","IV","â™­VII"],["I","IV","V","â™­VII"]],
                 "melody_density_range":(0.6,0.95),"drums":True,"comment":"Ø¬Ù‡Ø´ÛŒ/Ø³ÛŒÙ†Ú©ÙˆÙ¾"},
    "OTHER":    {"mode":"major","tempo_range":(90,110),"base_keys":[60,62],
                 "progressions":[["I","ii","IV","V"],["I","IV","ii","V"]],
                 "melody_density_range":(0.4,0.65),"drums":False,"comment":"Ø®Ù†Ø«ÛŒ/Ù¾Ø³â€ŒØ²Ù…ÛŒÙ†Ù‡"},
}

MAJOR_ROMAN = {"I":(0,"maj"),"ii":(2,"min"),"iii":(4,"min"),"IV":(5,"maj"),"V":(7,"maj"),"vi":(9,"min"),"â™­VII":(10,"maj")}
MINOR_ROMAN = {"i":(0,"min"),"iiÂ°":(2,"dim"),"III":(3,"maj"),"iv":(5,"min"),"v":(7,"min"),
               "VI":(8,"maj"),"VII":(10,"maj"),"bVI":(8,"maj"),"bVII":(10,"maj"),"bIIÂ°":(1,"dim")}
QUAL2INTS = {"maj":[0,4,7], "min":[0,3,7], "dim":[0,3,6]}

def _scale_for(mode:str, emotion:str, style:str):
    if style != "persian":
        return [0,2,4,5,7,9,11] if mode=="major" else [0,2,3,5,7,8,10]
    # ØªÙ‚Ø±ÛŒØ¨ÛŒ Ø§Ø² Ø¯Ø³ØªÚ¯Ø§Ù‡â€ŒÙ‡Ø§ (Ø¨Ø¯ÙˆÙ† Ø±Ø¨Ø¹â€ŒÙ¾Ø±Ø¯Ù‡)
    if emotion in ["HAPPY","SURPRISE"]: return [0,1,4,5,7,8,11]   # Hijaz / Phrygian-dominant
    if emotion in ["SAD"]:              return [0,1,3,5,7,8,10]   # Phrygian (Ø­Ø²Ù†)
    if emotion in ["FEAR"]:             return [0,1,3,5,6,8,10]   # Saba-like (Ø¯Ù„Ù‡Ø±Ù‡)
    return [0,2,3,5,7,8,10]             # Nahawand-like

def _add_chord(inst, start, end, root, qual, rng, spread=0):
    for i in QUAL2INTS[qual]:
        p = int(root+i + spread)
        vel = int(np.clip(72 + rng.randint(-8,8), 40, 105))
        inst.notes.append(pretty_midi.Note(velocity=vel, pitch=p, start=start, end=end))

def _add_persian_bed(inst, start, end, root, rng, add_fifth=True, octave_spread=True):
    base = int(root)
    p_list = [base]
    if add_fifth: p_list.append(base+7)
    if octave_spread and (rng.random()<0.6):
        p_list += [base-12, base+12]
    vel = int(np.clip(78 + rng.randint(-6,6), 50, 105))
    for p in p_list:
        p = max(0, min(127, p))
        inst.notes.append(pretty_midi.Note(velocity=vel, pitch=p, start=start, end=end))

def _arp_pattern(inst, start, end, root, qual, rng, step):
    ints = QUAL2INTS[qual]; seq = ints + ints[::-1]
    t = start; idx = rng.randint(0,len(seq)-1)
    while t < end:
        p = int(root + seq[idx % len(seq)])
        dur = step * (1.0 if rng.random()<0.7 else 0.5)
        vel = int(np.clip(74 + rng.randint(-12,12), 40, 110))
        jit = rng.uniform(-0.015,0.015)
        inst.notes.append(pretty_midi.Note(velocity=vel, pitch=p, start=t+jit, end=min(t+jit+dur, end)))
        t += step; idx += 1

def _scale_run(inst, start, end, key, scale, rng, step):
    t = start; idx = rng.randint(0,len(scale)-1); direction = rng.choice([1,-1])
    last = key + scale[idx]
    while t < end:
        deg = scale[idx % len(scale)]
        base = key + deg + rng.choice([-12,0,12])
        pitch = int(np.clip(int(0.65*last + 0.35*base), key-7, key+19))
        dur = step if rng.random()<0.8 else step*2
        vel = int(np.clip(76 + rng.randint(-10,10), 45, 110))
        jit = rng.uniform(-0.015,0.015)
        inst.notes.append(pretty_midi.Note(velocity=vel, pitch=pitch, start=t+jit, end=min(t+jit+dur, end)))
        last = pitch; t += step; idx += direction
        if rng.random()<0.12: direction *= -1

def _bass_pattern(inst, start, end, root, bpm, rng, style="eighths"):
    beat = 60.0/bpm
    if style=="long":
        p = max(0, root-12)
        inst.notes.append(pretty_midi.Note(velocity=74, pitch=p, start=start, end=end))
    elif style=="syncop":
        for k in [0.5, 1.5, 2.5, 3.5]:
            t = start + k*beat + rng.uniform(-0.01,0.01)
            inst.notes.append(pretty_midi.Note(velocity=85+rng.randint(-6,6), pitch=max(0,root-12), start=t, end=min(t+0.30, end)))
    else:
        t = start
        while t < end:
            p = max(0, root-12)
            d = 0.45*beat
            inst.notes.append(pretty_midi.Note(velocity=82+rng.randint(-8,8), pitch=p, start=t, end=min(t+d, end)))
            t += 0.5*beat

def _lead_melody(inst, start, bars, bpm, mode, key, density, rng, scale_override=None):
    scale = scale_override if scale_override is not None else ([0,2,4,5,7,9,11] if mode=="major" else [0,2,3,5,7,8,10])
    step = 0.5*(60.0/bpm)       # 8th
    total_sub = int(bars*4*2)
    t = start; last_pitch = key + 7 + rng.choice([-12,0,12])
    lo, hi = key-7, key+19
    motif = [rng.choice(scale) for _ in range(rng.choice([3,4,5,6]))]
    for s in range(total_sub):
        if rng.random() < density:
            deg = motif[s % len(motif)] if rng.random()<0.75 else rng.choice(scale)
            base = key + deg + rng.choice([-12,0,12])
            pitch = int(np.clip(int(0.65*last_pitch + 0.35*base), lo, hi))
            dur = step if rng.random()<0.65 else step*2
            jit = rng.uniform(-0.02,0.02)
            vel = int(np.clip(80 + rng.randint(-14,14), 40, 115))
            inst.notes.append(pretty_midi.Note(velocity=vel, pitch=pitch, start=t+jit, end=t+jit+dur))
            last_pitch = pitch
        t += step

# --- Ø¯Ø±Ø§Ù…â€ŒÙ‡Ø§
def _drum_bar(inst_drm, bar_start, beat_dur, rng, style="straight"):
    """Ø¯Ø±Ø§Ù… Ø³Ø§Ø¯Ù‡â€ŒÛŒ 4/4"""
    for b in range(4):
        t = bar_start + b*beat_dur
        inst_drm.notes.append(pretty_midi.Note(100 if b%2==0 else 92, pitch=36 if b%2==0 else 38, start=t, end=t+0.06))
        inst_drm.notes.append(pretty_midi.Note(62, pitch=42, start=t, end=t+0.03))
        if style=="swing":
            inst_drm.notes.append(pretty_midi.Note(62, pitch=42, start=t+beat_dur*2/3, end=t+beat_dur*2/3+0.03))
        else:
            inst_drm.notes.append(pretty_midi.Note(62, pitch=42, start=t+beat_dur/2, end=t+beat_dur/2+0.03))

def _dance_4onfloor(inst_drm, bar_start, beat_dur, rng):
    """Ú†Ù‡Ø§Ø±-Ø¨Ù‡-Ú©Ù Ø¨Ø±Ø§ÛŒ Ø´Ø§Ø¯ ØºØ±Ø¨ÛŒ"""
    for b in range(4):
        t = bar_start + b*beat_dur
        # Kick on every beat
        inst_drm.notes.append(pretty_midi.Note(108, pitch=36, start=t, end=t+0.08))
        # Snare/Clap on 2 & 4
        if b in [1,3]:
            inst_drm.notes.append(pretty_midi.Note(110, pitch=38, start=t, end=t+0.06))
            inst_drm.notes.append(pretty_midi.Note(120, pitch=39, start=t, end=t+0.03))  # clap
        # Hi-hat 8ths
        inst_drm.notes.append(pretty_midi.Note(70, pitch=42, start=t, end=t+0.03))
        inst_drm.notes.append(pretty_midi.Note(70, pitch=42, start=t+beat_dur/2, end=t+beat_dur/2+0.03))

def _metal_bar(inst_drm, bar_start, beat_dur, rng):
    """Ø¯Ø±Ø§Ù… Ù…ØªØ§Ù„: Ø¯Ø§Ø¨Ù„â€ŒÚ©ÛŒÚ© + Ú©Ø±Ø´ Ø¢ØºØ§Ø² Ù‡Ø± Ù…ÛŒØ²Ø§Ù†"""
    # Crash on bar start
    inst_drm.notes.append(pretty_midi.Note(100, pitch=49, start=bar_start, end=bar_start+0.5*beat_dur))
    # 16th grid
    for s in range(16):
        t = bar_start + s*(beat_dur/4)
        # Snare on beats 2 and 4
        if abs(t - (bar_start+beat_dur)) < 1e-6 or abs(t - (bar_start+3*beat_dur)) < 1e-6:
            inst_drm.notes.append(pretty_midi.Note(110, pitch=38, start=t, end=t+0.05))
        else:
            # Double kick dense (avoid exactly at snare)
            inst_drm.notes.append(pretty_midi.Note(100, pitch=36, start=t, end=t+0.04))
        # Hi-hat 8ths
        if s % 2 == 0:
            inst_drm.notes.append(pretty_midi.Note(72, pitch=42, start=t, end=t+0.03))

def _persian_68(inst_drm, bar_start, beat_dur, rng):
    """Ø±ÛŒØªÙ… Ø´Ø´â€ŒÙˆâ€ŒÙ‡Ø´Øª Ø§ÛŒØ±Ø§Ù†ÛŒ Ø´Ø¨Ù‡â€ŒØªÙ†Ø¨Ú©/Ø¯Ù Ø¨Ø§ Ø³Ø§Ø²Ù‡Ø§ÛŒ GM (Ú©ÙˆÙ†Ú¯Ø§/ØªÙ…Ø¨ÙˆØ±ÛŒÙ†/Ú©ÙÙ„ÙÙ¾)"""
    step = beat_dur*(2/3)  # 6 Ù¾Ø§Ù„Ø³ Ø¯Ø± Ù‡Ø± Ù…ÛŒØ²Ø§Ù† 4/4 â†’ Ø­Ø³ 6/8
    pulses = [bar_start + i*step for i in range(6)]
    # Dum Ø±ÙˆÛŒ 1 Ùˆ 4 (Ú©ÙˆÙ†Ú¯Ø§ÛŒ Ø¨Ù…)
    for i in [0,3]:
        inst_drm.notes.append(pretty_midi.Note(105, pitch=64, start=pulses[i], end=pulses[i]+0.08))  # Low Conga
    # Tek Ø±ÙˆÛŒ 3 Ùˆ 6 (Ú©ÙˆÙ†Ú¯Ø§ÛŒ Ø²ÛŒØ±/Ø¨Ø§Ø²)
    for i in [2,5]:
        inst_drm.notes.append(pretty_midi.Note(96, pitch=63, start=pulses[i], end=pulses[i]+0.06))  # Open High Conga
    # ØªÙ…Ø¨ÙˆØ±ÛŒÙ† Ø±ÛŒØ²
    for i in range(6):
        inst_drm.notes.append(pretty_midi.Note(70, pitch=54, start=pulses[i], end=pulses[i]+0.03))
    # Ø¯Ø³Øªâ€ŒØ²Ø¯Ù† Ø±ÙˆÛŒ 4
    inst_drm.notes.append(pretty_midi.Note(120, pitch=39, start=pulses[3], end=pulses[3]+0.04))

def _drum_fill(inst_drm, bar_start, beat_dur, rng):
    for i, pitch in enumerate([47,45,43,41]):
        t = bar_start + i*(beat_dur/2)
        inst_drm.notes.append(pretty_midi.Note(86+rng.randint(-6,6), pitch=pitch, start=t, end=t+0.12))

def _cc(inst, cc, value, time=0.0):
    inst.control_changes.append(pretty_midi.ControlChange(cc, int(value), time))

def _power_chord_riff(inst_L, inst_R, start, end, root, bpm, rng):
    beat = 60.0/bpm; t = start
    while t < end:
        dur = min(0.45*beat, end-t)
        velL = int(np.clip(110 + rng.randint(-8,8), 60, 127))
        velR = int(np.clip(108 + rng.randint(-8,8), 60, 127))
        for off in [0,7,12]:
            inst_L.notes.append(pretty_midi.Note(velocity=velL, pitch=int(root+off), start=t+rng.uniform(-0.005,0.005), end=min(t+dur, end)))
            inst_R.notes.append(pretty_midi.Note(velocity=velR, pitch=int(root+off), start=t+rng.uniform(-0.005,0.005), end=min(t+dur, end)))
        t += 0.5*beat  # 8th push

def generate_full_track(emotion: str, minutes: float = 3.0, seed: int | None = None, style: str = "global", sf2_path: str | None = None):
    assert emotion in EMO_PROFILE, f"unknown emotion {emotion}"
    rng = random.Random(seed if seed is not None else int(time.time()*1000) % 2_147_483_647)
    prof = EMO_PROFILE[emotion]
    bpm = rng.randint(*prof["tempo_range"])
    key = rng.choice(prof["base_keys"])
    progression = rng.choice(prof["progressions"])
    density = rng.uniform(*prof["melody_density_range"])
    is_persian = (style == "persian")
    sec_per_bar = 4*(60.0/bpm)
    total_bars = max(16, int((minutes*60)/sec_per_bar))
    bars_A = int(total_bars*0.4); bars_B = int(total_bars*0.4); bars_A2 = total_bars - (bars_A+bars_B)
    beat_dur = (60.0/bpm)
    mode_here = prof["mode"]
    ROM = MAJOR_ROMAN if mode_here=="major" else MINOR_ROMAN
    scale_local = _scale_for(mode_here, emotion, style)

    pm = pretty_midi.PrettyMIDI()
    insts = {}

    if is_persian:
        roles_src = PERSIAN_ROLES[emotion][:]
        rng.shuffle(roles_src)
        roles = roles_src[:rng.randint(3, min(6, len(roles_src)))]
        bed_prog = rng.choice(PERSIAN_GM["kamancheh"] + PERSIAN_GM["santur"])
        inst_har = pretty_midi.Instrument(program=bed_prog); pm.instruments.append(inst_har)
        _cc(inst_har,7,96); _cc(inst_har,10,64)

        def _addp(name, progs, vol, pan):
            insts[name] = pretty_midi.Instrument(program=rng.choice(progs))
            _cc(insts[name],7,vol); _cc(insts[name],10,pan)
            pm.instruments.append(insts[name])

        if "santur"    in roles: _addp("santur", PERSIAN_GM["santur"],   100, 60)
        if "qanun"     in roles: _addp("qanun",  PERSIAN_GM["qanun"],     98, 68)
        if "tar"       in roles: _addp("tar",    PERSIAN_GM["tar"],       96, 56)
        if "setar"     in roles: _addp("setar",  PERSIAN_GM["setar"],     94, 72)
        if "oud"       in roles: _addp("oud",    PERSIAN_GM["oud"],       98, 52)
        if "kamancheh" in roles: _addp("kam",    PERSIAN_GM["kamancheh"], 96, 76)
        if "ney"       in roles: _addp("ney",    PERSIAN_GM["ney"],      100, 64)

        # ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ HAPPY Ø§ÛŒØ±Ø§Ù†ÛŒ: Ø±ÛŒØªÙ… 6/8 (daf/tonbak Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§ Ø¯Ø±Ø§Ù… GM)
        inst_drm = pretty_midi.Instrument(program=0, is_drum=True) if emotion=="HAPPY" else None
        if inst_drm: pm.instruments.append(inst_drm)
    else:
        roles_pool = ["pad","piano","guitar_ac","guitar_el","bass","strings","lead","mallet","brass","guitar_drive"]
        rng.shuffle(roles_pool)
        roles = roles_pool[:rng.randint(3, 6)]

        # Ø§Ø¬Ø¨Ø§Ø±ÛŒâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ù†Ø±Ú˜ÛŒ Ø¨Ù‡ØªØ±
        if emotion=="ANGRY":
            if "guitar_drive" not in roles: roles.append("guitar_drive")
            if "bass" not in roles: roles.append("bass")
            prof["drums"] = True
        if emotion=="HAPPY":
            if "bass" not in roles: roles.append("bass")
            if "brass" not in roles: roles.append("brass")
            prof["drums"] = True

        # Harmony bed
        if "pad" in roles:
            inst_har = pretty_midi.Instrument(program=rng.choice(GM["pad"]+GM["strings"]))
        elif "strings" in roles:
            inst_har = pretty_midi.Instrument(program=rng.choice(GM["strings"]))
        else:
            inst_har = pretty_midi.Instrument(program=rng.choice(GM["piano"]))
        _cc(inst_har, 7, 92); _cc(inst_har, 10, 64)
        pm.instruments.append(inst_har)

        def _addi(name, progs, vol, pan):
            if name in insts: return
            insts[name] = pretty_midi.Instrument(program=rng.choice(progs))
            _cc(insts[name],7,vol); _cc(insts[name],10,pan)
            pm.instruments.append(insts[name])

        if "piano" in roles:        _addi("piano", GM["piano"], 96, 64)
        if "guitar_ac" in roles:    _addi("gtr_ac", GM["guitar_ac"], 96, 52)
        if "guitar_el" in roles:    _addi("gtr_el", GM["guitar_el"], 96, 76)
        if "guitar_drive" in roles: _addi("gtrL", GM["guitar_drive"], 110, 32); _addi("gtrR", GM["guitar_drive"], 110, 96)
        if "bass" in roles:         _addi("bass", GM["bass"], 110, 64)
        if "strings" in roles:      _addi("str2", GM["strings"], 90, 84)
        if "brass" in roles:        _addi("brs", GM["brass"], 96, 70)
        if "mallet" in roles:       _addi("mlt", GM["mallet"], 92, 58)
        if "lead" in roles:         _addi("lead", GM["lead"], 104, 64)
        inst_drm = pretty_midi.Instrument(program=0, is_drum=True) if prof["drums"] else None
        if inst_drm: pm.instruments.append(inst_drm)

    def render_section(start_bar, num_bars, key_this, prog_this, dens_mul=1.0, final_fill=False):
        for i in range(num_bars):
            roman = prog_this[i % len(prog_this)]
            # ÙØ§Ù„Ø³ÙÛŒÙ Ø¯Ø± ØµÙˆØ±Øª Ø¯Ø±Ø¬Ù‡ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡
            if roman not in ROM:
                off, qual = (0, "maj" if mode_here=="major" else "min")
            else:
                off, qual = ROM[roman]
            root = (key_this + off) % 128
            start = (start_bar+i)*4*beat_dur
            end   = (start_bar+i+1)*4*beat_dur

            if is_persian:
                _add_persian_bed(inst_har, start, end, root, rng, add_fifth=True, octave_spread=True)
                step = 0.5*beat_dur
                for name in ["santur","qanun","tar","setar","oud"]:
                    if name in insts and rng.random()<0.85:
                        _scale_run(insts[name], start, end, key_this, scale_local, rng, step=step)
                if "ney" in insts and rng.random()<0.95:
                    _scale_run(insts["ney"], start, end, key_this, scale_local, rng, step=step)
                if inst_drm:
                    _persian_68(inst_drm, start, beat_dur, rng)
            else:
                _add_chord(inst_har, start, end, root, qual, rng, spread=rng.choice([0,12,-12,0]))
                has_drive = ("gtrL" in insts and "gtrR" in insts)
                if has_drive and emotion=="ANGRY":
                    _power_chord_riff(insts["gtrL"], insts["gtrR"], start, end, root, bpm, rng)
                else:
                    gtr = insts.get("gtr_el") or insts.get("gtr_ac")
                    if gtr: _arp_pattern(gtr, start, end, root, qual, rng, step=0.5*beat_dur)
                if "piano" in insts and (emotion!="ANGRY") and rng.random()<0.85:
                    _arp_pattern(insts["piano"], start, end, root, qual, rng, step=0.5*beat_dur)
                if "brs" in insts and rng.random()<0.4 and emotion in ["HAPPY","SURPRISE"]:
                    _add_chord(insts["brs"], start, min(start+beat_dur, end), root+12, qual, rng)
                if "mlt" in insts and rng.random()<0.5:
                    _arp_pattern(insts["mlt"], start, min(start+2*beat_dur, end), root+12, qual, rng, step=0.25*beat_dur)
                if "bass" in insts:
                    style_b = "eighths"
                    if emotion in ["SAD","FEAR"]: style_b = "long"
                    if emotion in ["HAPPY","SURPRISE"]: style_b = rng.choice(["eighths","syncop"])
                    if emotion=="ANGRY": style_b = "eighths"
                    _bass_pattern(insts["bass"], start, end, root, bpm, rng, style=style_b)
                if inst_drm:
                    if emotion=="HAPPY":      _dance_4onfloor(inst_drm, start, beat_dur, rng)
                    elif emotion=="ANGRY":    _metal_bar(inst_drm, start, beat_dur, rng)
                    elif emotion=="SURPRISE": _drum_bar(inst_drm, start, beat_dur, rng, style="swing" if rng.random()<0.4 else "straight")
                    elif emotion not in ["SAD","FEAR"]:
                        _drum_bar(inst_drm, start, beat_dur, rng, style="straight")

            # FEAR (Global): Ø§Ø±Ú¯ Ú©Ù„ÛŒØ³Ø§ Ù„Ø§Ù†Ú¯â€ŒÙ†ÙˆØª
            if (not is_persian) and emotion=="FEAR":
                org = pretty_midi.Instrument(program=19)  # Church Organ
                _cc(org,7,98); _cc(org,10,64)
                org.notes.append(pretty_midi.Note(velocity=84, pitch=int(root), start=start, end=end))
                pm.instruments.append(org)

        # Lead
        if is_persian:
            lead_inst = insts.get("ney") or insts.get("kam") or inst_har
            dens = density*dens_mul*(0.5 if emotion=="SAD" else 1.0)
            _lead_melody(lead_inst, start_bar*4*beat_dur, num_bars, bpm, mode_here, key_this, dens, rng, scale_override=scale_local)
        else:
            lead_inst = insts.get("lead") or insts.get("piano") or inst_har
            dmul = density*dens_mul
            if emotion=="SAD": dmul *= 0.6
            _lead_melody(lead_inst, start_bar*4*beat_dur, num_bars, bpm, mode_here, key_this, dmul, rng)
        if final_fill and (not is_persian) and inst_drm and (emotion not in ["SAD","FEAR"]):
            _drum_fill(inst_drm, (start_bar+num_bars-1)*4*beat_dur, beat_dur, rng)

    # Ø³Ø§Ø®ØªØ§Ø± Aâ€“Bâ€“Aâ€²
    render_section(0, bars_A, key, progression, dens_mul=1.0, final_fill=True)
    mod_key = key + rng.choice([-2, 2, 0])
    new_prog = rng.choice(prof["progressions"])
    render_section(bars_A, bars_B, mod_key, new_prog, dens_mul=rng.uniform(0.9,1.25), final_fill=True)
    render_section(bars_A+bars_B, bars_A2, key, progression, dens_mul=rng.uniform(0.85,1.15), final_fill=False)

    stamp = int(time.time())
    out_mid = f"{emotion.lower()}_{style}_{minutes:.1f}min_{stamp}.mid"
    out_wav = f"{emotion.lower()}_{style}_{minutes:.1f}min_{stamp}.wav"
    wav_path, sr = _render_wav_robust(pm, out_mid, out_wav, sf2_path=sf2_path, sr=22050)

    # MP3
    out_mp3 = f"{emotion.lower()}_{style}_{minutes:.1f}min_{stamp}.mp3"
    _ = _to_mp3(wav_path, out_mp3, bitrate="192k")

    meta = {
        "emotion": emotion, "style": style, "minutes": minutes, "bpm": bpm, "mode": mode_here,
        "key_midi": key, "progression_A": progression, "progression_B": new_prog,
        "drums": (not is_persian) and bool(EMO_PROFILE[emotion]["drums"]) or (is_persian and emotion=="HAPPY"),
        "comment": prof["comment"], "bars_total": total_bars, "sr": sr
    }
    return out_mid, wav_path, out_mp3, meta

# ---------------------------- Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…ÙˆØ³ÛŒÙ‚ÛŒ --------------------------
TARGETS = {
    "HAPPY":    dict(mode="major", tempo=(124,142), density=(7,12), want_dissonance=False, want_syncopation=True),
    "SAD":      dict(mode="minor", tempo=(45,60),   density=(1,4),  want_dissonance=False, want_syncopation=False),
    "ANGRY":    dict(mode="minor", tempo=(150,175), density=(8,14), want_dissonance=True,  want_syncopation=True),
    "FEAR":     dict(mode="minor", tempo=(50,68),   density=(1,4),  want_dissonance=True,  want_syncopation=False),
    "SURPRISE": dict(mode="major", tempo=(122,145), density=(6,12), want_dissonance=False, want_syncopation=True),
    "OTHER":    dict(mode="major", tempo=(85,110),  density=(4,8),  want_dissonance=False, want_syncopation=False),
}

def analyze_midi_struct(mid_path, emotion, build_params):
    pm = pretty_midi.PrettyMIDI(mid_path)
    non_drums = [ins for ins in pm.instruments if not ins.is_drum]
    note_count = sum(len(ins.notes) for ins in non_drums)
    bars = build_params["bars_total"]
    density = note_count / max(1,bars)
    bpm_guess = build_params["bpm"]

    step = 60.0/bpm_guess/2
    sync_onsets, total_notes = 0, 0
    for ins in non_drums:
        for n in ins.notes:
            total_notes += 1
            frac = (n.start/step) % 2
            if 0.4 < frac < 1.6: sync_onsets += 1
    sync_ratio = (sync_onsets / max(1,total_notes)) if total_notes else 0.0
    has_dim_like = any((n.end-n.start) < (60.0/bpm_guess)/4 for ins in non_drums for n in ins.notes)

    target = TARGETS[emotion]
    mode_match = 1.0 if (build_params["mode"] == target["mode"]) else 0.0
    tempo_fit  = 1.0 if (target["tempo"][0] <= bpm_guess <= target["tempo"][1]) else 0.0
    d_lo, d_hi = target["density"]
    if density<d_lo: density_fit = max(0.0, 1 - (d_lo-density)/max(1e-6,d_lo))
    elif density>d_hi: density_fit = max(0.0, 1 - (density-d_hi)/max(1e-6,d_hi))
    else: density_fit = 1.0
    sync_fit   = min(1.0, sync_ratio*1.5) if target["want_syncopation"] else (1.0 - min(1.0, sync_ratio*1.5))
    disson_fit = 1.0 if (target["want_dissonance"] and has_dim_like) or (not target["want_dissonance"] and not has_dim_like) else 0.5

    score = 0.25*mode_match + 0.25*tempo_fit + 0.25*density_fit + 0.15*disson_fit + 0.10*sync_fit
    return {
        "bpm": bpm_guess, "bars": bars, "note_density_per_bar": round(density,2),
        "mode_match": round(mode_match,2), "tempo_fit": round(tempo_fit,2),
        "density_fit": round(density_fit,2), "dissonance_fit": round(disson_fit,2),
        "syncopation_fit": round(sync_fit,2), "overall_structural_score": round(float(score),3),
        "build_params": build_params,
    }

def evaluate_with_audio_classifier(wav_path, target_emotion:str):
    try:
        out = audio_clf(wav_path)
        top = max(out, key=lambda x:x["score"])
        mapped = AUD_MAP.get(top["label"].lower()[:3], "OTHER")
        return {"predicted_emotion_audio": mapped,
                "predicted_confidence_audio": round(float(top["score"]),3),
                "target_emotion": target_emotion,
                "audio_match": 1.0 if mapped==target_emotion else 0.0}
    except Exception as e:
        print("[audio_eval error]", repr(e))
        return {"predicted_emotion_audio": None,"predicted_confidence_audio":0.0,
                "target_emotion": target_emotion,"audio_match":0.0}

def generate_and_evaluate_full(emotion:str, minutes:float=3.0, seed=None, style:str="global", sf2_path: str | None = None):
    mid_path, wav_path, mp3_path, meta = generate_full_track(emotion, minutes=minutes, seed=seed, style=style, sf2_path=sf2_path)
    struct = analyze_midi_struct(mid_path, emotion, meta)
    audio_eval = evaluate_with_audio_classifier(wav_path, emotion)
    final_score = round(0.75*struct["overall_structural_score"] + 0.25*audio_eval["audio_match"], 3)
    report = {"emotion": emotion, "style": style, "structural": struct, "audio_eval": audio_eval, "final_score": final_score}
    return wav_path, mp3_path, report

# ----------------------------- Gradio UI ------------------------------
def analyze_and_make_music(text, image, audio_path, threshold, minutes, seed, style_choice, sf2_upload):
    label, score, emoji = analyze_all(text, image, audio_path, threshold)
    try:
        seed_int = None if (seed is None or str(seed).strip()=="") else int(seed)
    except:
        seed_int = None
    style = "persian" if (str(style_choice).lower().startswith("pers")) else "global"
    sf2_path = str(sf2_upload) if (sf2_upload is not None and str(sf2_upload).strip()!="") else None

    # ØªÙˆÙ„ÛŒØ¯ + Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
    wav_path, mp3_path, rep = generate_and_evaluate_full(label, minutes=float(minutes), seed=seed_int, style=style, sf2_path=sf2_path)

    # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ø±Ø§ÛŒ Ù¾Ø®Ø´ Ø¢Ù†Ù„Ø§ÛŒÙ†
    y, sr = sf.read(wav_path, always_2d=False)
    if hasattr(y, "ndim") and y.ndim > 1:
        y = np.mean(y, axis=1)
    y = _normalize_limit(y)

    rep["detected_label"] = label
    rep["detected_confidence"] = score
    rep["soundfont"] = sf2_path if sf2_path else SF2_PATH_DEFAULT
    return label, score, emoji, (sr, y.astype(np.float32)), rep, (mp3_path if mp3_path else wav_path)

with gr.Blocks(css="""
  #root {max-width:980px;margin:auto;padding:16px;}
  .out {font-size:1.1rem;text-align:center;}
""") as app:
    gr.Markdown("## ØªØ´Ø®ÛŒØµ Ø§Ø­Ø³Ø§Ø³ (Ù…ØªÙ†/ØªØµÙˆÛŒØ±/ØµÙˆØª) â†’ ğŸµ Ø³Ø§Ø®Øª Ù…ÙˆØ³ÛŒÙ‚ÛŒ Ú†Ù†Ø¯â€ŒØ¯Ù‚ÛŒÙ‚Ù‡â€ŒØ§ÛŒ (Global ÛŒØ§ Persian) + Ú¯Ø²Ø§Ø±Ø´ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ + MP3")
    with gr.Row():
        with gr.Column():
            txt = gr.Textbox(label="Ù…ØªÙ† (ÙØ§Ø±Ø³ÛŒ/Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ)", lines=3, placeholder="Ù…Ø«Ø§Ù„: Ø§Ù…Ø±ÙˆØ² Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ´Ø­Ø§Ù„Ù…!")
            img = gr.Image(label="ØªØµÙˆÛŒØ± Ú†Ù‡Ø±Ù‡", type="pil")
            aud = gr.Audio(label="ØµÙˆØª (wav/mp3)", type="filepath")
        with gr.Column():
            thr  = gr.Slider(0,1,0.7,step=0.01, label="Ø¢Ø³ØªØ§Ù†Ù‡ OTHER Ø¨Ø±Ø§ÛŒ Ù…ØªÙ†/ØµÙˆØª")
            mins = gr.Slider(1.0, 6.0, value=3.0, step=0.5, label="Ù…Ø¯Øª Ù…ÙˆØ³ÛŒÙ‚ÛŒ (Ø¯Ù‚ÛŒÙ‚Ù‡)")
            seed = gr.Textbox(value="", label="Seed (Ø®Ø§Ù„ÛŒ = Ù‡Ø± Ø¨Ø§Ø± Ù…ØªÙØ§ÙˆØª)")
            style_dd = gr.Dropdown(choices=["Global/Western","Persian/Traditional"], value="Global/Western", label="Ø³Ø¨Ú©/Palette")
            sf2_file = gr.File(label="SoundFont (sf2) Ø§Ø®ØªÛŒØ§Ø±ÛŒ â€” Ø§Ú¯Ø± SF2 Ø§ÛŒØ±Ø§Ù†ÛŒ Ø¯Ø§Ø±ÛŒ Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†", file_types=[".sf2"], type="filepath")
            go   = gr.Button("ğŸ” ØªØ­Ù„ÛŒÙ„ Ùˆ Ø³Ø§Ø®Øª Ù…ÙˆØ³ÛŒÙ‚ÛŒ", variant="primary")

    with gr.Row():
        o1 = gr.Textbox(label="Ø§Ø­Ø³Ø§Ø³",    interactive=False, elem_classes="out")
        o2 = gr.Textbox(label="Ø§Ø¹ØªÙ…Ø§Ø¯",   interactive=False, elem_classes="out")
        o3 = gr.Textbox(label="Ø§ÛŒÙ…ÙˆØ¬ÛŒ",   interactive=False, elem_classes="out")

    audio_out = gr.Audio(label="ğŸ§ Ù…ÙˆØ³ÛŒÙ‚ÛŒ Ø®Ø±ÙˆØ¬ÛŒ (Ù¾Ø®Ø´ Ø¢Ù†Ù„Ø§ÛŒÙ†)", type="numpy")
    rep_out   = gr.JSON(label="ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ (Ø³Ø§Ø®ØªØ§Ø±ÛŒ + Ú©Ù„Ø§Ø³ÛŒÙØ§ÛŒØ± ØµÙˆØª Ø±ÙˆÛŒ WAV)")
    mp3_out   = gr.File(label="â¬‡ï¸ Ø¯Ø§Ù†Ù„ÙˆØ¯ MP3")

    go.click(analyze_and_make_music,
             [txt, img, aud, thr, mins, seed, style_dd, sf2_file],
             [o1, o2, o3, audio_out, rep_out, mp3_out])

app.launch(share=False)
# =====================================================================

# app.launch(share=True)
# app.launch(debug=True)

"""### 3- Ø®Ø±ÙˆØ¬ÛŒ Ø¬Ø§Ù…Ø¹"""

# =======================  Ù‡Ù…Ú¯ÛŒ Ø¨Ø§Ù‡Ù… ==========================

# FluidSynth : Ø§Ø¨Ø²Ø§Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ø±Ù†Ø¯Ø± Ú©Ø±Ø¯Ù† MIDI ==> Ø¨Ù‡ ØµØ¯Ø§

!apt-get -qq -y install fluidsynth ffmpeg # -y : ÛŒØ¹Ù†ÛŒ Ø¨Ø¯ÙˆÙ† Ù¾Ø±Ø³ÛŒØ¯Ù† ØªØ£ÛŒÛŒØ¯ Ù†ØµØ¨  |  -qq :Ø®Ø±ÙˆØ¬ÛŒ Ø±Ø§ Ø³Ø§Ú©Øª Ù…ÛŒâ€ŒÚ©Ù†Ø¯
!pip install -q gradio transformers[sentencepiece] soundfile pillow pyFluidSynth==1.3.3 \
                pretty_midi music21 librosa midi2audio langdetect tensorflow opencv-python-headless pydub

# transformers[sentencepiece] : Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ HuggingFace

# gradio : Ø¨Ø±Ø§ÛŒ Ø±Ø§Ø¨Ø· ÙˆØ¨ demo For machine learning model with a friendly web interface

# pillow : Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø± Ø¨Ø§ ØªØµØ§ÙˆÛŒØ±

# music21 : Ø§Ø¨Ø²Ø§Ø± ØªØ¦ÙˆØ±ÛŒ Ù…ÙˆØ³ÛŒÙ‚ÛŒ

# librosa : Ù¾Ø±Ø¯Ø§Ø²Ø´ ØµÙˆØª

# ----------------------------------Ø³Ø§ÙˆÙ†Ø¯ ÙÙˆÙ†Øª Ù‡Ø§ÛŒ Ù„Ø§Ø²Ù…---------------------------------------------------------------------------------------------------------------------------------

# .sf2 : ÙØ±Ù…Øª ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ  Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø³Ø§Ø²ÛŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ ØµÙˆØªÛŒ

# SoundFont Ù¾ÛŒØ´â€ŒÙØ±Ø¶ (General MIDI Ø¨Ø§ Ú©ÛŒÙÛŒØª Ø®ÙˆØ¨)
!wget -q https://github.com/pepaslabs/fluidsynth-soundfonts/raw/master/FluidR3_GM/FluidR3_GM.sf2 -O /content/FluidR3_GM.sf2 \
  || wget -q https://github.com/urish/cyberbotics/raw/master/projects/robotbenchmark/resources/fluidr3/FluidR3_GM.sf2 -O /content/FluidR3_GM.sf2

# Ù¾ÙˆØ´Ù‡
!mkdir -p /content/sf2

# FluidR3 (Ø¨Ú© Ø¢Ù¾)
!wget -q https://github.com/pepaslabs/fluidsynth-soundfonts/raw/master/FluidR3_GM/FluidR3_GM.sf2 -O /content/sf2/FluidR3_GM.sf2 || true

# GM Ù‡Ø§ÛŒ Ø¨Ø§Ú©ÛŒÙÛŒØª Ø§Ø² GitHub
!wget -c https://raw.githubusercontent.com/bratpeki/soundfonts/master/GeneralUser.sf2 -O /content/sf2/GeneralUser.sf2
!wget -c https://raw.githubusercontent.com/bratpeki/soundfonts/master/ChoriumRevA.sf2 -O /content/sf2/ChoriumRevA.sf2
!wget -c https://raw.githubusercontent.com/bratpeki/soundfonts/master/WeedsGM3.sf2 -O /content/sf2/WeedsGM3.sf2

# Arachno (Ù¾Ø§Ù¾/Ø±Ø§Ú©)
!wget -c "http://maxime.abbey.free.fr/mirror/arachnosoft/files/soundfonts/arachno-soundfont-10-sf2.zip" -O /content/sf2/Arachno.zip
!unzip -o /content/sf2/Arachno.zip -d /content/sf2

# Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ
!wget -c "https://musical-artifacts.com/artifacts/940/Persa.sf2" -O /content/sf2/Persa.sf2

# --------------------------------------------------------------------------------------------------------------------------------------------------------------------------

# Ù…Ø¯Ù„ FER (mini_XCEPTION) Ø¨Ø±Ø§ÛŒ ØªØµÙˆÛŒØ±
!wget -q https://raw.githubusercontent.com/oarriaga/face_classification/master/trained_models/emotion_models/fer2013_mini_XCEPTION.102-0.66.hdf5 -O /content/emotion_model.h5

# ------------------ Import & Config (TF â†’ CPU) ----------------
import os, time, random, json
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"

import tensorflow as tf
try:
    tf.config.set_visible_devices([], 'GPU')
except Exception:
    pass

import numpy as np
import gradio as gr
import soundfile as sf
from PIL import Image

import torch
from transformers import pipeline
import cv2
from tensorflow.keras.models import load_model
import pretty_midi
from midi2audio import FluidSynth
from langdetect import detect
from pydub import AudioSegment  # Ø¨Ø±Ø§ÛŒ MP3

# ----------------------- Device Ø¨Ø±Ø§ÛŒ Transformers ----------------------

DEVICE = 0 if torch.cuda.is_available() else -1
print("Device for Transformers:", "cuda:0" if DEVICE==0 else "cpu")

# SF2_PATH_DEFAULT = "/content/FluidR3_GM.sf2" Ø§Ú¯Ø± Ù‡ÛŒÚ† Ø³Ø§ÙˆÙ†Ø¯ ÙÙˆÙ†ØªÛŒ Ù†Ø¯Ø§Ø´ØªÛŒÙ…ØŒ Ø§ÛŒÙ† Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ø¯ÛŒÙØ§Ù„ØªÙ‡

SF2_BANKS = {
    "fluidr3": "/content/sf2/FluidR3_GM.sf2",
    "generaluser": "/content/sf2/GeneralUser.sf2",
    "chorium": "/content/sf2/ChoriumRevA.sf2",
    "weeds": "/content/sf2/WeedsGM3.sf2",
    "arachno": "/content/sf2/Arachno SoundFont - Version 1.0.sf2",
    "persa": "/content/sf2/Persa.sf2",
}

# Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¨Ø§ ÛŒÚ©ÛŒ Ø§Ø² Ø§Ø¹Ø¶Ø§ÛŒ Ø¨Ø§Ù†Ú© Ø³Ø§ÙˆÙ†Ø¯ ÙÙˆÙ†Øª Ù‡Ø§:
SF2_PATH_DEFAULT = SF2_BANKS["persa"]

# ---------- Utility: Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ/Ù„ÛŒÙ…ÛŒØªÙ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø®Ø´ Ù¾Ø§ÛŒØ¯Ø§Ø± ----------
def _normalize_limit(x, peak=0.98):
    x = np.array(x)
    if x.ndim == 1:
        x = x[:, None]
    x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)
    x = x - np.mean(x, axis=0, keepdims=True)
    mx = np.max(np.abs(x)) + 1e-9
    x = x / mx
    y = np.tanh(1.2 * x)
    y = y * (peak / (np.max(np.abs(y)) + 1e-9))
    return y.squeeze()

# Ø§Ú¯Ø± Ø±Ù†Ø¯Ø± Ù…Ø³ØªÙ‚ÛŒÙ… pyfluidsynth Ù…Ø´Ú©Ù„ Ø¯Ø§Ø´ØªØŒ Ù…Ø³ÛŒØ± fallback Ø¨Ø§ midi2audio Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.
def _render_wav_robust(pm, out_mid, out_wav, sf2_path=None, sr=22050):
    """Ù‡Ù…ÛŒØ´Ù‡ Ø§ÙˆÙ„ MIDI Ø±Ø§ Ù…ÛŒâ€ŒÙ†ÙˆÛŒØ³ÛŒÙ… ØªØ§ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒâ€ŒÙ‡Ø§ fail Ù†Ø´Ù†."""
    sf2_path = sf2_path or SF2_PATH_DEFAULT
    pm.write(out_mid)
    # ØªÙ„Ø§Ø´ Û±: pyfluidsynth
    try:
        audio = pm.fluidsynth(fs=sr, sf2_path=sf2_path)
        audio = _normalize_limit(audio)
        rms = np.sqrt(np.mean(audio**2))
        if (rms < 1e-4) or (not np.isfinite(rms)):
            raise ValueError("silent_or_invalid_audio")
        sf.write(out_wav, audio, sr)
        return out_wav, sr
    except Exception as e:
        print("[render] fallback to midi2audio:", e)
        FluidSynth(sound_font=sf2_path, sample_rate=sr).midi_to_audio(out_mid, out_wav)
        y, sr2 = sf.read(out_wav, always_2d=False)
        y = _normalize_limit(y)
        sf.write(out_wav, y, sr2)
        return out_wav, sr2

def _to_mp3(wav_path, mp3_path, bitrate="192k"):
    try:
        AudioSegment.from_file(wav_path).export(mp3_path, format="mp3", bitrate=bitrate)
        return mp3_path
    except Exception as e:
        print("[mp3] export error:", e)
        return None

# ----------------------------- Emoji & Maps ---------------------------
EMOJI = {"ANGRY":"ğŸ˜ ","FEAR":"ğŸ˜¨","HAPPY":"ğŸ˜ƒ","SAD":"ğŸ˜¢","SURPRISE":"ğŸ˜²","OTHER":"ğŸ˜"}

# ------------------------- Pipelines Ù…ØªÙ†/ØµÙˆØª --------------------------

# Ù¾Ø§Ø±Ø³â€ŒØ¨Ø±Øª Ø§Ø®ØªÛŒØ§Ø±ÛŒâ€”Ø§Ú¯Ø± Ù†Ø¨ÙˆØ¯ØŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯
try:
    pipe_fa = pipeline(
        "text-classification",
        model="/content/final_parsbert_emotion",
        tokenizer="/content/final_parsbert_emotion",
        device=DEVICE
    )
except Exception as e:
    pipe_fa = None
    print("[warn] Persian text model not found -> using English when needed")

pipe_en = pipeline(
    "text-classification",
    model="nateraw/bert-base-uncased-emotion",
    tokenizer="nateraw/bert-base-uncased-emotion",
    device=DEVICE,
    return_all_scores=False
)

def map_en(label):
    l = label.lower()
    if l in ["joy","love"]: return "HAPPY"
    if l == "sadness":      return "SAD"
    if l == "anger":        return "ANGRY"
    if l == "fear":         return "FEAR"
    if l == "surprise":     return "SURPRISE"
    return "OTHER"

audio_clf = pipeline("audio-classification", model="superb/wav2vec2-base-superb-er", device=DEVICE, top_k=None)
AUD_MAP = {"hap":"HAPPY","sad":"SAD","ang":"ANGRY","fea":"FEAR","sur":"SURPRISE","neu":"OTHER"}

# ------------------------- Ù…Ø¯Ù„ ØªØµÙˆÛŒØ± (TF Ø±ÙˆÛŒ CPU) ---------------------
emotion_cnn = load_model("/content/emotion_model.h5", compile=False)
FER_LABELS = ['angry','disgust','fear','happy','sad','surprise','neutral']
IMG_MAP = {
    "angry":"ANGRY","disgust":"ANGRY","fear":"FEAR",
    "happy":"HAPPY","sad":"SAD","surprise":"SURPRISE","neutral":"OTHER"
}
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

# -------------------------- ØªØ´Ø®ÛŒØµ Ø§Ø­Ø³Ø§Ø³â€ŒÙ‡Ø§ ----------------------------
def analyze_text(text, threshold):
    if not text or not str(text).strip():
        return "OTHER", 0.0, EMOJI["OTHER"]
    try:
        lang = detect(text)
    except:
        lang = "fa"
    if lang == "fa" and pipe_fa is not None:
        out = pipe_fa(text)[0]
        idx = int(out["label"].split("_")[-1])
        FA = ['ANGRY','FEAR','HAPPY','HATE','OTHER','SAD','SURPRISE']
        label = FA[idx] if FA[idx]!="HATE" else "ANGRY"
        score = float(out["score"])
    else:
        out = pipe_en(text)[0]
        label = map_en(out["label"])
        score = float(out["score"])
    if score < threshold:
        label = "OTHER"
    return label, round(score,3), EMOJI[label]

# Ø§Ú¯Ø± ØªØµÙˆÛŒØ± Ú†Ù‡Ø±Ù‡ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ù†Ø´Ø¯ØŒ ÙˆØ³Ø· ØªØµÙˆÛŒØ± Ø±Ø§ Ø¨Ù‡â€ŒØ¹Ù†ÙˆØ§Ù† ROI Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯.
# ØªØµÙˆÛŒØ± Ø±Ø§ Ø®Ø§Ú©Ø³ØªØ±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ Ú†Ù‡Ø±Ù‡ Ø±Ø§ Ø¨Ø§ Ù‡Ø§Ø± Ù¾ÛŒØ¯Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ø¯Ø› Ø§Ú¯Ø± Ù†Ú©Ù†Ø¯ØŒ Ù…Ø±Ú©Ø² ØªØµÙˆÛŒØ± Ø±Ø§ Ø¨Ø±Ø´ Ù…ÛŒâ€ŒØ²Ù†Ø¯.
def analyze_image(img, threshold):
    try:
        gray = cv2.cvtColor(np.array(img.convert("RGB")), cv2.COLOR_RGB2GRAY)
        faces = face_cascade.detectMultiScale(gray, 1.3, 5)
        if len(faces)==0:
            h,w = gray.shape
            sz = min(h,w)//2
            x = (w-sz)//2; y = (h-sz)//2
            roi = gray[y:y+sz, x:x+sz]
        else:
            x,y,w,h = faces[0]
            roi = gray[y:y+h, x:x+w]
        face = cv2.resize(roi, (64,64), interpolation=cv2.INTER_AREA)
        face = face.astype("float32")/255.0
        face = face.reshape(1,64,64,1)
        preds = emotion_cnn.predict(face, verbose=0)[0]
        idx   = int(np.argmax(preds))
        score = float(preds[idx])
        raw   = FER_LABELS[idx]
        label = IMG_MAP.get(raw, "OTHER")
        return label, round(score,3), EMOJI[label]
    except Exception as e:
        print("[analyze_image error]", repr(e))
        return "OTHER", 0.0, EMOJI["OTHER"]

def analyze_audio(filepath, threshold):
    try:
        out = audio_clf(filepath)
        if not out:
            return "OTHER", 0.0, EMOJI["OTHER"]
        top = max(out, key=lambda x: x["score"])
        raw = top["label"].lower()
        score = float(top["score"])
        label = AUD_MAP.get(raw[:3], "OTHER")
        if score < threshold:
            label = "OTHER"
        return label, round(score,3), EMOJI[label]
    except Exception as e:
        print("[analyze_audio error]", repr(e))
        return "OTHER", 0.0, EMOJI["OTHER"]

def analyze_all(text, image, audio_path, threshold):
    if text and str(text).strip():
        return analyze_text(text, threshold)
    if image is not None:
        return analyze_image(image, threshold)
    if audio_path:
        return analyze_audio(audio_path, threshold)
    return "OTHER", 0.0, EMOJI["OTHER"]

# --------------------- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø³Ø§Ø®Øª Ù…ÙˆØ³ÛŒÙ‚ÛŒ ----------------
GM = dict(
    piano=[0,1,4,5,6],
    guitar_ac=[24,25], guitar_el=[26,27,28],
    guitar_drive=[29,30],
    bass=[32,33,34,39],
    strings=[48,49,50],
    brass=[60,61],
    mallet=[11,12,13,14],
    pad=[88,89,91,94],
    lead=[80,81,82,84]
)

# Ø³Ø§Ø²Ù‡Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ (ØªÙ‚Ø±ÛŒØ¨ Ø¨Ø§ GM)
PERSIAN_GM = dict(
    santur=[15],               # Dulcimer â‰ˆ Santur
    tar=[24,104,107],          # NylonGtr / Sitar / Koto
    setar=[24,104],
    oud=[24,25,104],
    kamancheh=[40],            # Violin
    ney=[77,75],               # Shakuhachi / Pan Flute
    qanun=[46,15,107],         # Harp / Dulcimer / Koto
)

# ØªÙ†Ù‡Ø§ Ø³Ø§Ø²Ù‡Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ Ø¯Ø± Ø­Ø§Ù„Øª Persian
PERSIAN_ROLES = {
    "HAPPY":    ["santur","qanun","oud","ney","kamancheh","tar","setar"],
    "SAD":      ["kamancheh","ney","santur","qanun","setar","tar"],
    "ANGRY":    ["santur","tar","kamancheh","ney","oud"],   # Ø®Ø´Ù… Ø§ÛŒØ±Ø§Ù†ÛŒ Ø¨Ø¯ÙˆÙ† Ú¯ÛŒØªØ§Ø± ØºØ±Ø¨ÛŒ
    "FEAR":     ["ney","kamancheh","santur","qanun"],
    "SURPRISE": ["santur","qanun","oud","ney","kamancheh"],
    "OTHER":    ["santur","qanun","ney","kamancheh","setar"]
}

EMO_PROFILE = {
    "HAPPY":    {"mode":"major","tempo_range":(124,142),"base_keys":[60,62,65,67],
                 "progressions":[["I","V","vi","IV"],["I","IV","V","I"],["I","V","IV","I"]],
                 "melody_density_range":(0.7,0.95),"drums":True,"comment":"Ø´Ø§Ø¯ Ùˆ Ø±Ù‚ØµØ§Ù†"},
    "SAD":      {"mode":"minor","tempo_range":(45,60),"base_keys":[57,55,52],
                 "progressions":[["i","VI","III","VII"],["i","iv","i","VII"],["i","VI","i","v"]],
                 "melody_density_range":(0.12,0.28),"drums":False,"comment":"Ø®ÛŒÙ„ÛŒ Ø¢Ø±Ø§Ù… Ùˆ Ø§ÙØ³Ø±Ø¯Ù‡"},
    "ANGRY":    {"mode":"minor","tempo_range":(150,175),"base_keys":[50,48,53],
                 "progressions":[["i","bVI","bVII","i"],["i","bIIÂ°","bVII","i"]],
                 "melody_density_range":(0.8,0.96),"drums":True,"comment":"Ø±Ø§Ú©/Ù…ØªØ§Ù„ ØªÙ‡Ø§Ø¬Ù…ÛŒ"},
    "FEAR":     {"mode":"minor","tempo_range":(50,68),"base_keys":[57,58,55],
                 "progressions":[["i","bIIÂ°","i","bVI"],["i","iv","bIIÂ°","i"]],
                 "melody_density_range":(0.12,0.3),"drums":False,"comment":"Ø¯Ù„Ù‡Ø±Ù‡â€ŒØ¢ÙˆØ±"},
    "SURPRISE": {"mode":"major","tempo_range":(122,145),"base_keys":[65,67,69],
                 "progressions":[["I","V","IV","â™­VII"],["I","IV","V","â™­VII"]],
                 "melody_density_range":(0.6,0.95),"drums":True,"comment":"Ø¬Ù‡Ø´ÛŒ/Ø³ÛŒÙ†Ú©ÙˆÙ¾"},
    "OTHER":    {"mode":"major","tempo_range":(90,110),"base_keys":[60,62],
                 "progressions":[["I","ii","IV","V"],["I","IV","ii","V"]],
                 "melody_density_range":(0.4,0.65),"drums":False,"comment":"Ø®Ù†Ø«ÛŒ/Ù¾Ø³â€ŒØ²Ù…ÛŒÙ†Ù‡"},
}

MAJOR_ROMAN = {"I":(0,"maj"),"ii":(2,"min"),"iii":(4,"min"),"IV":(5,"maj"),"V":(7,"maj"),"vi":(9,"min"),"â™­VII":(10,"maj")}
MINOR_ROMAN = {"i":(0,"min"),"iiÂ°":(2,"dim"),"III":(3,"maj"),"iv":(5,"min"),"v":(7,"min"),
               "VI":(8,"maj"),"VII":(10,"maj"),"bVI":(8,"maj"),"bVII":(10,"maj"),"bIIÂ°":(1,"dim")}
QUAL2INTS = {"maj":[0,4,7], "min":[0,3,7], "dim":[0,3,6]}

def _scale_for(mode:str, emotion:str, style:str):
    if style != "persian":
        return [0,2,4,5,7,9,11] if mode=="major" else [0,2,3,5,7,8,10]
    # ØªÙ‚Ø±ÛŒØ¨ÛŒ Ø§Ø² Ø¯Ø³ØªÚ¯Ø§Ù‡â€ŒÙ‡Ø§ (Ø¨Ø¯ÙˆÙ† Ø±Ø¨Ø¹â€ŒÙ¾Ø±Ø¯Ù‡)
    if emotion in ["HAPPY","SURPRISE"]: return [0,1,4,5,7,8,11]   # Hijaz / Phrygian-dominant
    if emotion in ["SAD"]:              return [0,1,3,5,7,8,10]   # Phrygian (Ø­Ø²Ù†)
    if emotion in ["FEAR"]:             return [0,1,3,5,6,8,10]   # Saba-like (Ø¯Ù„Ù‡Ø±Ù‡)
    return [0,2,3,5,7,8,10]             # Nahawand-like

def _add_chord(inst, start, end, root, qual, rng, spread=0):
    for i in QUAL2INTS[qual]:
        p = int(root+i + spread)
        vel = int(np.clip(72 + rng.randint(-8,8), 40, 105))
        inst.notes.append(pretty_midi.Note(velocity=vel, pitch=p, start=start, end=end))

def _add_persian_bed(inst, start, end, root, rng, add_fifth=True, octave_spread=True):
    base = int(root)
    p_list = [base]
    if add_fifth: p_list.append(base+7)
    if octave_spread and (rng.random()<0.6):
        p_list += [base-12, base+12]
    vel = int(np.clip(78 + rng.randint(-6,6), 50, 105))
    for p in p_list:
        p = max(0, min(127, p))
        inst.notes.append(pretty_midi.Note(velocity=vel, pitch=p, start=start, end=end))

def _arp_pattern(inst, start, end, root, qual, rng, step):
    ints = QUAL2INTS[qual]; seq = ints + ints[::-1]
    t = start; idx = rng.randint(0,len(seq)-1)
    while t < end:
        p = int(root + seq[idx % len(seq)])
        dur = step * (1.0 if rng.random()<0.7 else 0.5)
        vel = int(np.clip(74 + rng.randint(-12,12), 40, 110))
        jit = rng.uniform(-0.015,0.015)
        inst.notes.append(pretty_midi.Note(velocity=vel, pitch=p, start=t+jit, end=min(t+jit+dur, end)))
        t += step; idx += 1

def _scale_run(inst, start, end, key, scale, rng, step):
    t = start; idx = rng.randint(0,len(scale)-1); direction = rng.choice([1,-1])
    last = key + scale[idx]
    while t < end:
        deg = scale[idx % len(scale)]
        base = key + deg + rng.choice([-12,0,12])
        pitch = int(np.clip(int(0.65*last + 0.35*base), key-7, key+19))
        dur = step if rng.random()<0.8 else step*2
        vel = int(np.clip(76 + rng.randint(-10,10), 45, 110))
        jit = rng.uniform(-0.015,0.015)
        inst.notes.append(pretty_midi.Note(velocity=vel, pitch=pitch, start=t+jit, end=min(t+jit+dur, end)))
        last = pitch; t += step; idx += direction
        if rng.random()<0.12: direction *= -1

def _bass_pattern(inst, start, end, root, bpm, rng, style="eighths"):
    beat = 60.0/bpm
    if style=="long":
        p = max(0, root-12)
        inst.notes.append(pretty_midi.Note(velocity=74, pitch=p, start=start, end=end))
    elif style=="syncop":
        for k in [0.5, 1.5, 2.5, 3.5]:
            t = start + k*beat + rng.uniform(-0.01,0.01)
            inst.notes.append(pretty_midi.Note(velocity=85+rng.randint(-6,6), pitch=max(0,root-12), start=t, end=min(t+0.30, end)))
    else:
        t = start
        while t < end:
            p = max(0, root-12)
            d = 0.45*beat
            inst.notes.append(pretty_midi.Note(velocity=82+rng.randint(-8,8), pitch=p, start=t, end=min(t+d, end)))
            t += 0.5*beat

def _lead_melody(inst, start, bars, bpm, mode, key, density, rng, scale_override=None):
    scale = scale_override if scale_override is not None else ([0,2,4,5,7,9,11] if mode=="major" else [0,2,3,5,7,8,10])
    step = 0.5*(60.0/bpm)       # 8th
    total_sub = int(bars*4*2)
    t = start; last_pitch = key + 7 + rng.choice([-12,0,12])
    lo, hi = key-7, key+19
    motif = [rng.choice(scale) for _ in range(rng.choice([3,4,5,6]))]
    for s in range(total_sub):
        if rng.random() < density:
            deg = motif[s % len(motif)] if rng.random()<0.75 else rng.choice(scale)
            base = key + deg + rng.choice([-12,0,12])
            pitch = int(np.clip(int(0.65*last_pitch + 0.35*base), lo, hi))
            dur = step if rng.random()<0.65 else step*2
            jit = rng.uniform(-0.02,0.02)
            vel = int(np.clip(80 + rng.randint(-14,14), 40, 115))
            inst.notes.append(pretty_midi.Note(velocity=vel, pitch=pitch, start=t+jit, end=t+jit+dur))
            last_pitch = pitch
        t += step

# --- Ø¯Ø±Ø§Ù…â€ŒÙ‡Ø§
def _drum_bar(inst_drm, bar_start, beat_dur, rng, style="straight"):
    """Ø¯Ø±Ø§Ù… Ø³Ø§Ø¯Ù‡â€ŒÛŒ 4/4"""
    for b in range(4):
        t = bar_start + b*beat_dur
        inst_drm.notes.append(pretty_midi.Note(100 if b%2==0 else 92, pitch=36 if b%2==0 else 38, start=t, end=t+0.06))
        inst_drm.notes.append(pretty_midi.Note(62, pitch=42, start=t, end=t+0.03))
        if style=="swing":
            inst_drm.notes.append(pretty_midi.Note(62, pitch=42, start=t+beat_dur*2/3, end=t+beat_dur*2/3+0.03))
        else:
            inst_drm.notes.append(pretty_midi.Note(62, pitch=42, start=t+beat_dur/2, end=t+beat_dur/2+0.03))

def _dance_4onfloor(inst_drm, bar_start, beat_dur, rng):
    """Ú†Ù‡Ø§Ø±-Ø¨Ù‡-Ú©Ù Ø¨Ø±Ø§ÛŒ Ø´Ø§Ø¯ ØºØ±Ø¨ÛŒ"""
    for b in range(4):
        t = bar_start + b*beat_dur
        # Kick on every beat
        inst_drm.notes.append(pretty_midi.Note(108, pitch=36, start=t, end=t+0.08))
        # Snare/Clap on 2 & 4
        if b in [1,3]:
            inst_drm.notes.append(pretty_midi.Note(110, pitch=38, start=t, end=t+0.06))
            inst_drm.notes.append(pretty_midi.Note(120, pitch=39, start=t, end=t+0.03))  # clap
        # Hi-hat 8ths
        inst_drm.notes.append(pretty_midi.Note(70, pitch=42, start=t, end=t+0.03))
        inst_drm.notes.append(pretty_midi.Note(70, pitch=42, start=t+beat_dur/2, end=t+beat_dur/2+0.03))

def _metal_bar(inst_drm, bar_start, beat_dur, rng):
    """Ø¯Ø±Ø§Ù… Ù…ØªØ§Ù„: Ø¯Ø§Ø¨Ù„â€ŒÚ©ÛŒÚ© + Ú©Ø±Ø´ Ø¢ØºØ§Ø² Ù‡Ø± Ù…ÛŒØ²Ø§Ù†"""
    # Crash on bar start
    inst_drm.notes.append(pretty_midi.Note(100, pitch=49, start=bar_start, end=bar_start+0.5*beat_dur))
    # 16th grid
    for s in range(16):
        t = bar_start + s*(beat_dur/4)
        # Snare on beats 2 and 4
        if abs(t - (bar_start+beat_dur)) < 1e-6 or abs(t - (bar_start+3*beat_dur)) < 1e-6:
            inst_drm.notes.append(pretty_midi.Note(110, pitch=38, start=t, end=t+0.05))
        else:
            # Double kick dense (avoid exactly at snare)
            inst_drm.notes.append(pretty_midi.Note(100, pitch=36, start=t, end=t+0.04))
        # Hi-hat 8ths
        if s % 2 == 0:
            inst_drm.notes.append(pretty_midi.Note(72, pitch=42, start=t, end=t+0.03))

def _persian_68(inst_drm, bar_start, beat_dur, rng):
    """Ø±ÛŒØªÙ… Ø´Ø´â€ŒÙˆâ€ŒÙ‡Ø´Øª Ø§ÛŒØ±Ø§Ù†ÛŒ Ø´Ø¨Ù‡â€ŒØªÙ†Ø¨Ú©/Ø¯Ù Ø¨Ø§ Ø³Ø§Ø²Ù‡Ø§ÛŒ GM (Ú©ÙˆÙ†Ú¯Ø§/ØªÙ…Ø¨ÙˆØ±ÛŒÙ†/Ú©ÙÙ„ÙÙ¾)"""
    step = beat_dur*(2/3)  # 6 Ù¾Ø§Ù„Ø³ Ø¯Ø± Ù‡Ø± Ù…ÛŒØ²Ø§Ù† 4/4 â†’ Ø­Ø³ 6/8
    pulses = [bar_start + i*step for i in range(6)]
    # Dum Ø±ÙˆÛŒ 1 Ùˆ 4 (Ú©ÙˆÙ†Ú¯Ø§ÛŒ Ø¨Ù…)
    for i in [0,3]:
        inst_drm.notes.append(pretty_midi.Note(105, pitch=64, start=pulses[i], end=pulses[i]+0.08))  # Low Conga
    # Tek Ø±ÙˆÛŒ 3 Ùˆ 6 (Ú©ÙˆÙ†Ú¯Ø§ÛŒ Ø²ÛŒØ±/Ø¨Ø§Ø²)
    for i in [2,5]:
        inst_drm.notes.append(pretty_midi.Note(96, pitch=63, start=pulses[i], end=pulses[i]+0.06))  # Open High Conga
    # ØªÙ…Ø¨ÙˆØ±ÛŒÙ† Ø±ÛŒØ²
    for i in range(6):
        inst_drm.notes.append(pretty_midi.Note(70, pitch=54, start=pulses[i], end=pulses[i]+0.03))
    # Ø¯Ø³Øªâ€ŒØ²Ø¯Ù† Ø±ÙˆÛŒ 4
    inst_drm.notes.append(pretty_midi.Note(120, pitch=39, start=pulses[3], end=pulses[3]+0.04))

def _drum_fill(inst_drm, bar_start, beat_dur, rng):
    for i, pitch in enumerate([47,45,43,41]):
        t = bar_start + i*(beat_dur/2)
        inst_drm.notes.append(pretty_midi.Note(86+rng.randint(-6,6), pitch=pitch, start=t, end=t+0.12))

# MIDI Control Change (ÙˆÙ„ÙˆÙ…ØŒ Ù¾Ù†ÛŒÙ†Ú¯ Ùˆâ€¦)
def _cc(inst, cc, value, time=0.0):
    inst.control_changes.append(pretty_midi.ControlChange(cc, int(value), time))

def _power_chord_riff(inst_L, inst_R, start, end, root, bpm, rng):
    beat = 60.0/bpm; t = start
    while t < end:
        dur = min(0.45*beat, end-t)
        velL = int(np.clip(110 + rng.randint(-8,8), 60, 127))
        velR = int(np.clip(108 + rng.randint(-8,8), 60, 127))
        for off in [0,7,12]:
            inst_L.notes.append(pretty_midi.Note(velocity=velL, pitch=int(root+off), start=t+rng.uniform(-0.005,0.005), end=min(t+dur, end)))
            inst_R.notes.append(pretty_midi.Note(velocity=velR, pitch=int(root+off), start=t+rng.uniform(-0.005,0.005), end=min(t+dur, end)))
        t += 0.5*beat  # 8th push

def generate_full_track(emotion: str, minutes: float = 3.0, seed: int | None = None, style: str = "global", sf2_path: str | None = None):
    assert emotion in EMO_PROFILE, f"unknown emotion {emotion}"
    rng = random.Random(seed if seed is not None else int(time.time()*1000) % 2_147_483_647)
    prof = EMO_PROFILE[emotion]
    bpm = rng.randint(*prof["tempo_range"])
    key = rng.choice(prof["base_keys"])
    progression = rng.choice(prof["progressions"])
    density = rng.uniform(*prof["melody_density_range"])
    is_persian = (style == "persian")
    sec_per_bar = 4*(60.0/bpm)
    total_bars = max(16, int((minutes*60)/sec_per_bar))
    bars_A = int(total_bars*0.4); bars_B = int(total_bars*0.4); bars_A2 = total_bars - (bars_A+bars_B)
    beat_dur = (60.0/bpm)
    mode_here = prof["mode"]
    ROM = MAJOR_ROMAN if mode_here=="major" else MINOR_ROMAN
    scale_local = _scale_for(mode_here, emotion, style)

    pm = pretty_midi.PrettyMIDI()
    insts = {}

    if is_persian:
        roles_src = PERSIAN_ROLES[emotion][:]
        rng.shuffle(roles_src)
        roles = roles_src[:rng.randint(3, min(6, len(roles_src)))]
        bed_prog = rng.choice(PERSIAN_GM["kamancheh"] + PERSIAN_GM["santur"])
        inst_har = pretty_midi.Instrument(program=bed_prog); pm.instruments.append(inst_har)
        _cc(inst_har,7,96); _cc(inst_har,10,64)

        def _addp(name, progs, vol, pan):
            insts[name] = pretty_midi.Instrument(program=rng.choice(progs))
            _cc(insts[name],7,vol); _cc(insts[name],10,pan)
            pm.instruments.append(insts[name])

        if "santur"    in roles: _addp("santur", PERSIAN_GM["santur"],   100, 60)
        if "qanun"     in roles: _addp("qanun",  PERSIAN_GM["qanun"],     98, 68)
        if "tar"       in roles: _addp("tar",    PERSIAN_GM["tar"],       96, 56)
        if "setar"     in roles: _addp("setar",  PERSIAN_GM["setar"],     94, 72)
        if "oud"       in roles: _addp("oud",    PERSIAN_GM["oud"],       98, 52)
        if "kamancheh" in roles: _addp("kam",    PERSIAN_GM["kamancheh"], 96, 76)
        if "ney"       in roles: _addp("ney",    PERSIAN_GM["ney"],      100, 64)

        # ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ HAPPY Ø§ÛŒØ±Ø§Ù†ÛŒ: Ø±ÛŒØªÙ… 6/8 (daf/tonbak Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§ Ø¯Ø±Ø§Ù… GM)
        inst_drm = pretty_midi.Instrument(program=0, is_drum=True) if emotion=="HAPPY" else None
        if inst_drm: pm.instruments.append(inst_drm)
    else:
        roles_pool = ["pad","piano","guitar_ac","guitar_el","bass","strings","lead","mallet","brass","guitar_drive"]
        rng.shuffle(roles_pool)
        roles = roles_pool[:rng.randint(3, 6)]

        # Ø§Ø¬Ø¨Ø§Ø±ÛŒâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ù†Ø±Ú˜ÛŒ Ø¨Ù‡ØªØ±
        if emotion=="ANGRY":
            if "guitar_drive" not in roles: roles.append("guitar_drive")
            if "bass" not in roles: roles.append("bass")
            prof["drums"] = True
        if emotion=="HAPPY":
            if "bass" not in roles: roles.append("bass")
            if "brass" not in roles: roles.append("brass")
            prof["drums"] = True

        # Harmony bed
        if "pad" in roles:
            inst_har = pretty_midi.Instrument(program=rng.choice(GM["pad"]+GM["strings"]))
        elif "strings" in roles:
            inst_har = pretty_midi.Instrument(program=rng.choice(GM["strings"]))
        else:
            inst_har = pretty_midi.Instrument(program=rng.choice(GM["piano"]))
        _cc(inst_har, 7, 92); _cc(inst_har, 10, 64)
        pm.instruments.append(inst_har)

        def _addi(name, progs, vol, pan):
            if name in insts: return
            insts[name] = pretty_midi.Instrument(program=rng.choice(progs))
            _cc(insts[name],7,vol); _cc(insts[name],10,pan)
            pm.instruments.append(insts[name])

        if "piano" in roles:        _addi("piano", GM["piano"], 96, 64)
        if "guitar_ac" in roles:    _addi("gtr_ac", GM["guitar_ac"], 96, 52)
        if "guitar_el" in roles:    _addi("gtr_el", GM["guitar_el"], 96, 76)
        if "guitar_drive" in roles: _addi("gtrL", GM["guitar_drive"], 110, 32); _addi("gtrR", GM["guitar_drive"], 110, 96)
        if "bass" in roles:         _addi("bass", GM["bass"], 110, 64)
        if "strings" in roles:      _addi("str2", GM["strings"], 90, 84)
        if "brass" in roles:        _addi("brs", GM["brass"], 96, 70)
        if "mallet" in roles:       _addi("mlt", GM["mallet"], 92, 58)
        if "lead" in roles:         _addi("lead", GM["lead"], 104, 64)
        inst_drm = pretty_midi.Instrument(program=0, is_drum=True) if prof["drums"] else None
        if inst_drm: pm.instruments.append(inst_drm)

    def render_section(start_bar, num_bars, key_this, prog_this, dens_mul=1.0, final_fill=False):
        for i in range(num_bars):
            roman = prog_this[i % len(prog_this)]
            #  ÙØ§Ù„Ø³ÙÛŒÙ Ø¯Ø± ØµÙˆØ±Øª Ø¯Ø±Ø¬Ù‡ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡
            if roman not in ROM:
                off, qual = (0, "maj" if mode_here=="major" else "min")
            else:
                off, qual = ROM[roman]
            root = (key_this + off) % 128
            start = (start_bar+i)*4*beat_dur
            end   = (start_bar+i+1)*4*beat_dur

            if is_persian:
                _add_persian_bed(inst_har, start, end, root, rng, add_fifth=True, octave_spread=True)
                step = 0.5*beat_dur
                for name in ["santur","qanun","tar","setar","oud"]:
                    if name in insts and rng.random()<0.85:
                        _scale_run(insts[name], start, end, key_this, scale_local, rng, step=step)
                if "ney" in insts and rng.random()<0.95:
                    _scale_run(insts["ney"], start, end, key_this, scale_local, rng, step=step)
                if inst_drm:
                    _persian_68(inst_drm, start, beat_dur, rng)
            else:
                _add_chord(inst_har, start, end, root, qual, rng, spread=rng.choice([0,12,-12,0]))
                has_drive = ("gtrL" in insts and "gtrR" in insts)
                if has_drive and emotion=="ANGRY":
                    _power_chord_riff(insts["gtrL"], insts["gtrR"], start, end, root, bpm, rng)
                else:
                    gtr = insts.get("gtr_el") or insts.get("gtr_ac")
                    if gtr: _arp_pattern(gtr, start, end, root, qual, rng, step=0.5*beat_dur)
                if "piano" in insts and (emotion!="ANGRY") and rng.random()<0.85:
                    _arp_pattern(insts["piano"], start, end, root, qual, rng, step=0.5*beat_dur)
                if "brs" in insts and rng.random()<0.4 and emotion in ["HAPPY","SURPRISE"]:
                    _add_chord(insts["brs"], start, min(start+beat_dur, end), root+12, qual, rng)
                if "mlt" in insts and rng.random()<0.5:
                    _arp_pattern(insts["mlt"], start, min(start+2*beat_dur, end), root+12, qual, rng, step=0.25*beat_dur)
                if "bass" in insts:
                    style_b = "eighths"
                    if emotion in ["SAD","FEAR"]: style_b = "long"
                    if emotion in ["HAPPY","SURPRISE"]: style_b = rng.choice(["eighths","syncop"])
                    if emotion=="ANGRY": style_b = "eighths"
                    _bass_pattern(insts["bass"], start, end, root, bpm, rng, style=style_b)
                if inst_drm:
                    if emotion=="HAPPY":      _dance_4onfloor(inst_drm, start, beat_dur, rng)
                    elif emotion=="ANGRY":    _metal_bar(inst_drm, start, beat_dur, rng)
                    elif emotion=="SURPRISE": _drum_bar(inst_drm, start, beat_dur, rng, style="swing" if rng.random()<0.4 else "straight")
                    elif emotion not in ["SAD","FEAR"]:
                        _drum_bar(inst_drm, start, beat_dur, rng, style="straight")

            # FEAR (Global): Ø§Ø±Ú¯ Ú©Ù„ÛŒØ³Ø§ Ù„Ø§Ù†Ú¯â€ŒÙ†ÙˆØª
            if (not is_persian) and emotion=="FEAR":
                org = pretty_midi.Instrument(program=19)  # Church Organ
                _cc(org,7,98); _cc(org,10,64)
                org.notes.append(pretty_midi.Note(velocity=84, pitch=int(root), start=start, end=end))
                pm.instruments.append(org)

        # Lead
        if is_persian:
            lead_inst = insts.get("ney") or insts.get("kam") or inst_har
            dens = density*dens_mul*(0.5 if emotion=="SAD" else 1.0)
            _lead_melody(lead_inst, start_bar*4*beat_dur, num_bars, bpm, mode_here, key_this, dens, rng, scale_override=scale_local)
        else:
            lead_inst = insts.get("lead") or insts.get("piano") or inst_har
            dmul = density*dens_mul
            if emotion=="SAD": dmul *= 0.6
            _lead_melody(lead_inst, start_bar*4*beat_dur, num_bars, bpm, mode_here, key_this, dmul, rng)
        if final_fill and (not is_persian) and inst_drm and (emotion not in ["SAD","FEAR"]):
            _drum_fill(inst_drm, (start_bar+num_bars-1)*4*beat_dur, beat_dur, rng)


# Ø¨Ø®Ø´ A Ø¨Ø§ Ù¾Ø±ÙˆÚ¯Ø±Ø´Ù† Ø§ØµÙ„ÛŒØŒ
# Ø¨Ø®Ø´ B Ø¨Ø§ Ù…Ø¯ÙˆÙ„Ø§Ø³ÛŒÙˆÙ† Ú©Ù„ÛŒØ¯ Ùˆ Ù¾Ø±ÙˆÚ¯Ø±Ø´Ù† Ø¬Ø¯ÛŒØ¯ØŒ
# Ø¨Ø®Ø´ Aâ€² Ø¨Ø±Ú¯Ø´Øª Ø¨Ù‡ Ú©Ù„ÛŒØ¯ Ùˆ Ù¾Ø±ÙˆÚ¯Ø±Ø´Ù† Ø§ÙˆÙ„ (Ø¨Ø§ Ø¯Ø§Ù†Ø³ÛŒØªÙ‡ Ú©Ù…ÛŒ Ù…ØªÙØ§ÙˆØª).
    # Ø³Ø§Ø®ØªØ§Ø± Aâ€“Bâ€“Aâ€²
    render_section(0, bars_A, key, progression, dens_mul=1.0, final_fill=True)
    mod_key = key + rng.choice([-2, 2, 0])
    new_prog = rng.choice(prof["progressions"])
    render_section(bars_A, bars_B, mod_key, new_prog, dens_mul=rng.uniform(0.9,1.25), final_fill=True)
    render_section(bars_A+bars_B, bars_A2, key, progression, dens_mul=rng.uniform(0.85,1.15), final_fill=False)

    stamp = int(time.time())
    out_mid = f"{emotion.lower()}_{style}_{minutes:.1f}min_{stamp}.mid"
    out_wav = f"{emotion.lower()}_{style}_{minutes:.1f}min_{stamp}.wav"
    wav_path, sr = _render_wav_robust(pm, out_mid, out_wav, sf2_path=sf2_path, sr=22050)

    # MP3
    out_mp3 = f"{emotion.lower()}_{style}_{minutes:.1f}min_{stamp}.mp3"
    _ = _to_mp3(wav_path, out_mp3, bitrate="192k")

    meta = {
        "emotion": emotion, "style": style, "minutes": minutes, "bpm": bpm, "mode": mode_here,
        "key_midi": key, "progression_A": progression, "progression_B": new_prog,
        "drums": (not is_persian) and bool(EMO_PROFILE[emotion]["drums"]) or (is_persian and emotion=="HAPPY"),
        "comment": prof["comment"], "bars_total": total_bars, "sr": sr
    }
    return out_mid, wav_path, out_mp3, meta

# ---------------------------- Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…ÙˆØ³ÛŒÙ‚ÛŒ --------------------------

# Ø§Ù‡Ø¯Ø§Ù Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø§Ø­Ø³Ø§Ø³: Ù…ÙØ¯ØŒ Ø¨Ø§Ø²Ù‡ ØªÙ…Ù¾ÙˆØŒ Ø¨Ø§Ø²Ù‡ Ø¯Ø§Ù†Ø³ÛŒØªÙ‡ Ù†Øª/Ù…ÛŒÙ€Ø²Ø§Ù†ØŒ Ø¹Ù„Ø§Ù‚Ù‡ Ø¨Ù‡ Ø¯ÛŒØ³ÙˆÙ†Ø§Ù†Ø³/Ø³ÛŒÙ†Ú©ÙˆÙ¾.

TARGETS = {
    "HAPPY":    dict(mode="major", tempo=(124,142), density=(7,12), want_dissonance=False, want_syncopation=True),
    "SAD":      dict(mode="minor", tempo=(45,60),   density=(1,4),  want_dissonance=False, want_syncopation=False),
    "ANGRY":    dict(mode="minor", tempo=(150,175), density=(8,14), want_dissonance=True,  want_syncopation=True),
    "FEAR":     dict(mode="minor", tempo=(50,68),   density=(1,4),  want_dissonance=True,  want_syncopation=False),
    "SURPRISE": dict(mode="major", tempo=(122,145), density=(6,12), want_dissonance=False, want_syncopation=True),
    "OTHER":    dict(mode="major", tempo=(85,110),  density=(4,8),  want_dissonance=False, want_syncopation=False),
}

def analyze_midi_struct(mid_path, emotion, build_params):
    pm = pretty_midi.PrettyMIDI(mid_path)
    non_drums = [ins for ins in pm.instruments if not ins.is_drum]
    note_count = sum(len(ins.notes) for ins in non_drums)
    bars = build_params["bars_total"]
    density = note_count / max(1,bars)
    bpm_guess = build_params["bpm"]

    step = 60.0/bpm_guess/2
    sync_onsets, total_notes = 0, 0
    for ins in non_drums:
        for n in ins.notes:
            total_notes += 1
            frac = (n.start/step) % 2
            if 0.4 < frac < 1.6: sync_onsets += 1
    sync_ratio = (sync_onsets / max(1,total_notes)) if total_notes else 0.0
    has_dim_like = any((n.end-n.start) < (60.0/bpm_guess)/4 for ins in non_drums for n in ins.notes)

    target = TARGETS[emotion]
    mode_match = 1.0 if (build_params["mode"] == target["mode"]) else 0.0
    tempo_fit  = 1.0 if (target["tempo"][0] <= bpm_guess <= target["tempo"][1]) else 0.0
    d_lo, d_hi = target["density"]
    if density<d_lo: density_fit = max(0.0, 1 - (d_lo-density)/max(1e-6,d_lo))
    elif density>d_hi: density_fit = max(0.0, 1 - (density-d_hi)/max(1e-6,d_hi))
    else: density_fit = 1.0
    sync_fit   = min(1.0, sync_ratio*1.5) if target["want_syncopation"] else (1.0 - min(1.0, sync_ratio*1.5))
    disson_fit = 1.0 if (target["want_dissonance"] and has_dim_like) or (not target["want_dissonance"] and not has_dim_like) else 0.5

    score = 0.25*mode_match + 0.25*tempo_fit + 0.25*density_fit + 0.15*disson_fit + 0.10*sync_fit
    return {
        "bpm": bpm_guess, "bars": bars, "note_density_per_bar": round(density,2),
        "mode_match": round(mode_match,2), "tempo_fit": round(tempo_fit,2),
        "density_fit": round(density_fit,2), "dissonance_fit": round(disson_fit,2),
        "syncopation_fit": round(sync_fit,2), "overall_structural_score": round(float(score),3),
        "build_params": build_params,
    }

def evaluate_with_audio_classifier(wav_path, target_emotion:str):
    try:
        out = audio_clf(wav_path)
        top = max(out, key=lambda x:x["score"])
        mapped = AUD_MAP.get(top["label"].lower()[:3], "OTHER")
        return {"predicted_emotion_audio": mapped,
                "predicted_confidence_audio": round(float(top["score"]),3),
                "target_emotion": target_emotion,
                "audio_match": 1.0 if mapped==target_emotion else 0.0}
    except Exception as e:
        print("[audio_eval error]", repr(e))
        return {"predicted_emotion_audio": None,"predicted_confidence_audio":0.0,
                "target_emotion": target_emotion,"audio_match":0.0}

def generate_and_evaluate_full(emotion:str, minutes:float=3.0, seed=None, style:str="global", sf2_path: str | None = None):
    mid_path, wav_path, mp3_path, meta = generate_full_track(emotion, minutes=minutes, seed=seed, style=style, sf2_path=sf2_path)
    struct = analyze_midi_struct(mid_path, emotion, meta)
    audio_eval = evaluate_with_audio_classifier(wav_path, emotion)
    final_score = round(0.75*struct["overall_structural_score"] + 0.25*audio_eval["audio_match"], 3)
    report = {"emotion": emotion, "style": style, "structural": struct, "audio_eval": audio_eval, "final_score": final_score}
    return wav_path, mp3_path, report

# ----------------------------- Gradio UI ------------------------------
def analyze_and_make_music(text, image, audio_path, threshold, minutes, seed, style_choice, sf2_upload):
    label, score, emoji = analyze_all(text, image, audio_path, threshold)
    try:
        seed_int = None if (seed is None or str(seed).strip()=="") else int(seed)
    except:
        seed_int = None
    style = "persian" if (str(style_choice).lower().startswith("pers")) else "global"
    sf2_path = str(sf2_upload) if (sf2_upload is not None and str(sf2_upload).strip()!="") else None

    # ØªÙˆÙ„ÛŒØ¯ + Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
    wav_path, mp3_path, rep = generate_and_evaluate_full(label, minutes=float(minutes), seed=seed_int, style=style, sf2_path=sf2_path)

    # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ø±Ø§ÛŒ Ù¾Ø®Ø´ Ø¢Ù†Ù„Ø§ÛŒÙ†
    y, sr = sf.read(wav_path, always_2d=False)
    if hasattr(y, "ndim") and y.ndim > 1:
        y = np.mean(y, axis=1)
    y = _normalize_limit(y)

    rep["detected_label"] = label
    rep["detected_confidence"] = score
    rep["soundfont"] = sf2_path if sf2_path else SF2_PATH_DEFAULT
    return label, score, emoji, (sr, y.astype(np.float32)), rep, (mp3_path if mp3_path else wav_path)

with gr.Blocks(css="""
  #root {max-width:980px;margin:auto;padding:16px;}
  .out {font-size:1.1rem;text-align:center;}
""") as app:
    gr.Markdown("## ØªØ´Ø®ÛŒØµ Ø§Ø­Ø³Ø§Ø³ (Ù…ØªÙ†/ØªØµÙˆÛŒØ±/ØµÙˆØª) + Ø³Ø§Ø®Øª Ù…ÙˆØ³ÛŒÙ‚ÛŒ ( Ø®Ø§Ø±Ø¬ÛŒ/Ø¯Ø§Ø®Ù„ÛŒ ) + Ú¯Ø²Ø§Ø±Ø´ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ + Ø®Ø±ÙˆØ¬ÛŒ MP3")
    with gr.Row():
        with gr.Column():
            txt = gr.Textbox(label="Ù…ØªÙ† (ÙØ§Ø±Ø³ÛŒ/Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ)", lines=3, placeholder="Ù…Ø«Ø§Ù„: Ø§Ù…Ø±ÙˆØ² Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ´Ø­Ø§Ù„Ù…")
            img = gr.Image(label="ØªØµÙˆÛŒØ± Ú†Ù‡Ø±Ù‡", type="pil")
            aud = gr.Audio(label="ØµÙˆØª (wav/mp3)", type="filepath")
        with gr.Column():
            thr  = gr.Slider(0,1,0.7,step=0.01, label="Ø¢Ø³ØªØ§Ù†Ù‡ OTHER")
            mins = gr.Slider(1.0, 6.0, value=3.0, step=0.5, label="Ù…Ø¯Øª Ù…ÙˆØ³ÛŒÙ‚ÛŒ (Ø¯Ù‚ÛŒÙ‚Ù‡)")
            seed = gr.Textbox(value="", label="Seed (Ø®Ø§Ù„ÛŒ = Ù‡Ø± Ø¨Ø§Ø± Ù…ØªÙØ§ÙˆØª)")
            style_dd = gr.Dropdown(choices=["Global/Western","Persian/Traditional"], value="Global/Western", label="Ø³Ø¨Ú©/Palette")
            sf2_file = gr.File(label="Ø³Ø§ÙˆÙ†Ø¯ ÙÙˆÙ†Øª Ø¯Ù„Ø®ÙˆØ§Ù‡ Ø®ÙˆØ¯ØªÙˆ Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†:", file_types=[".sf2"], type="filepath")
            go   = gr.Button("ğŸ” ØªØ­Ù„ÛŒÙ„ Ùˆ Ø³Ø§Ø®Øª Ù…ÙˆØ³ÛŒÙ‚ÛŒ", variant="primary")

    with gr.Row():
        o1 = gr.Textbox(label="Ø§Ø­Ø³Ø§Ø³",    interactive=False, elem_classes="out")
        o2 = gr.Textbox(label="Ø§Ø¹ØªÙ…Ø§Ø¯",   interactive=False, elem_classes="out")
        o3 = gr.Textbox(label="Ø§ÛŒÙ…ÙˆØ¬ÛŒ",   interactive=False, elem_classes="out")

    audio_out = gr.Audio(label="ğŸ§ Ù…ÙˆØ³ÛŒÙ‚ÛŒ Ø®Ø±ÙˆØ¬ÛŒ (Ù¾Ø®Ø´ Ø¢Ù†Ù„Ø§ÛŒÙ†)", type="numpy")
    rep_out   = gr.JSON(label="ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ (Ø³Ø§Ø®ØªØ§Ø±ÛŒ + Ú©Ù„Ø§Ø³ÛŒÙØ§ÛŒØ± ØµÙˆØª Ø±ÙˆÛŒ WAV)")
    mp3_out   = gr.File(label="â¬‡ï¸ Ø¯Ø§Ù†Ù„ÙˆØ¯ MP3")

    go.click(analyze_and_make_music,
             [txt, img, aud, thr, mins, seed, style_dd, sf2_file],
             [o1, o2, o3, audio_out, rep_out, mp3_out])

app.launch(share=False)
# =====================================================================

# app.launch(share=True)
# app.launch(debug=True)

"""**Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ**"""

import os, pandas as pd, glob

GEN_DIR = "/content"
files = sorted([p for p in glob.glob(os.path.join(GEN_DIR, "*.wav"))])  # ÙÙ‚Ø· WAV (ØªØ§ Ø¨Ø§ Ø¯ÛŒÚ¯Ø± ÙØ±Ù…Øª Ù‡Ø§ Ø¯ÙˆØ¨Ù„ Ù†Ø´ÙˆØ¯)

rows = []
for p in files:
    name = os.path.basename(p).lower()
    if "happy" in name:  txt = "very upbeat dance pop / 6-8 energetic"
    elif "angry" in name: txt = "heavy metal angry: distorted electric guitars, loud drums, dark bass"
    elif "sad" in name:   txt = "very slow, mournful, crying, minimal, sparse"
    elif "fear" in name:  txt = "ominous church organ, eerie textures, suspense"
    elif "surprise" in name: txt = "syncopated bright brass, sudden accents"
    else: txt = "neutral background instrumental"
    rows.append({"file": p, "text": txt})

meta = pd.DataFrame(rows)
meta.to_csv("/content/meta.csv", index=False)

# ===================== Audio Eval (robust v4): CLAP, R-Prec, FAD, KAD, Diversity, MOS =====================
!pip -q install laion-clap torchvggish librosa pandas scipy tqdm soundfile tabulate matplotlib

import os, glob, json, math, zipfile, warnings
from typing import List
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import librosa
import torch
from tqdm import tqdm
from scipy import linalg

# ----------------------------- ØªÙ†Ø¸ÛŒÙ…Ø§Øª -----------------------------
GEN_DIR  = "/content"                 # ÙÙ‚Ø· ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ÛŒâ€Œ Ù…ÙˆØ±Ø¯ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
REF_DIR  = "/content/Ref_Eval_DIR"    # Ù¾ÙˆØ´Ù‡ Ø±ÙØ±Ù†Ø³â€ŒÙ‡Ø§Ø› Ø§Ú¯Ø± Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯ Ùˆ FORCE_SELF_REF=True â†’ Ø§Ø² Ø®ÙˆØ¯ GEN Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
META_CSV = "/content/meta.csv"        #  Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ file,text
OUT_DIR  = "/content/eval_out"

FORCE_SELF_REF = True                 # Ø§Ú¯Ø± Ù…Ø±Ø¬Ø¹ Ù†Ø¯Ø§Ø±ÛŒØŒ Ø®ÙˆØ¯ GEN Ø±Ø§ Ù…Ø±Ø¬Ø¹ Ø¨Ú¯ÛŒØ± (FADâ‰ˆ0 ÙˆÙ„ÛŒ null Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯)
AUTO_FAKE_MOS  = True                 # Ø¯Ø± Ù†Ø¨ÙˆØ¯ MOS ÙˆØ§Ù‚Ø¹ÛŒØŒ Ø§Ù…ØªÛŒØ§Ø²Ù‡Ø§ÛŒ ØªØµØ§Ø¯ÙÛŒ Ø¨Ø³Ø§Ø²
N_FAKE_RATERS  = 15                   # ØªØ¹Ø¯Ø§Ø¯ Ø±Ø£ÛŒâ€ŒØ¯Ù‡Ù†Ø¯Ù‡â€ŒÛŒ ØªØµØ§Ø¯ÙÛŒ Ø¨Ø±Ø§ÛŒ MOS ÙÛŒÚ©

os.makedirs(OUT_DIR, exist_ok=True)

# ----------------------------- Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ -----------------------------
def list_audio_files(d, exts=(".wav",".mp3",".flac",".ogg",".m4a")):
    if not d or not os.path.isdir(d): return []
    fs = []
    for e in exts: fs += glob.glob(os.path.join(d, f"*{e}"))
    return sorted(fs)

def load_mono(path, sr=48000):
    y, _ = librosa.load(path, sr=sr, mono=True)
    y = np.nan_to_num(y, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)
    m = np.max(np.abs(y)) if y.size else 0.0
    if m > 1e-9: y = y / m
    return y

def cosine_sim(a, b, eps=1e-8):
    a = a / (np.linalg.norm(a, axis=-1, keepdims=True) + eps)
    b = b / (np.linalg.norm(b, axis=-1, keepdims=True) + eps)
    return (a * b).sum(-1)

def _robust_cov(X):
    """Ú©ÙˆÙˆØ§Ø±ÛŒØ§Ù†Ø³ Ù¾Ø§ÛŒØ¯Ø§Ø± Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² NaN/Inf."""
    C = np.cov(X, rowvar=False)
    if not np.isfinite(C).all():
        C = np.cov(X, rowvar=False, bias=True)
    if not np.isfinite(C).all():
        X2 = X + 1e-6*np.random.randn(*X.shape)
        C = np.cov(X2, rowvar=False, bias=True)
    C = np.nan_to_num(C, nan=0.0, posinf=0.0, neginf=0.0)
    return (C + C.T) / 2.0

def frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):
    diff = mu1 - mu2
    I = eps*np.eye(sigma1.shape[0])
    covmean, _ = linalg.sqrtm((sigma1 + I) @ (sigma2 + I), disp=False)
    if not np.isfinite(covmean).all():
        covmean = linalg.sqrtm((sigma1 + 10*I) @ (sigma2 + 10*I))
    if np.iscomplexobj(covmean): covmean = covmean.real
    val = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2*covmean)
    if not np.isfinite(val): val = float(np.linalg.norm(diff))
    return float(val)

# ----------------------------- CLAP ---------------------------------
import laion_clap
clap_device = "cuda" if torch.cuda.is_available() else "cpu"
clap_model = laion_clap.CLAP_Module(enable_fusion=False, device=clap_device)
clap_model.load_ckpt()

@torch.no_grad()
def embed_audio_clap(paths: List[str], sr=48000, batch_size=8) -> np.ndarray:
    outs = []
    for i in tqdm(range(0, len(paths), batch_size), desc="CLAP audio embed"):
        batch = paths[i:i+batch_size]
        wavs = [torch.tensor(load_mono(p, sr), dtype=torch.float32) for p in batch]
        e = clap_model.get_audio_embedding_from_data(x=wavs, use_tensor=True)
        outs.append(e.cpu().numpy())
    return np.concatenate(outs, axis=0) if outs else np.zeros((0,512), dtype=np.float32)

@torch.no_grad()
def embed_text_clap(texts: List[str], batch_size=16) -> np.ndarray:
    outs = []
    for i in tqdm(range(0, len(texts), batch_size), desc="CLAP text embed"):
        e = clap_model.get_text_embedding(texts[i:i+batch_size], use_tensor=True)
        outs.append(e.cpu().numpy())
    return np.concatenate(outs, axis=0) if outs else np.zeros((0,512), dtype=np.float32)

# ----------------------------- VGGish (FAD) -------------------------
#  : Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø®Ø·Ø§ÛŒ "tensors on different devices"ØŒ VGGish Ø±Ø§ Ú©Ø§Ù…Ù„ Ø±ÙˆÛŒ CPU Ù…ÛŒâ€ŒÚ¯Ø°Ø§Ø±ÛŒÙ….
from torchvggish import vggish, vggish_input
VGGISH_DEVICE = "cpu"
vggish_model = vggish().to(VGGISH_DEVICE).eval()

def _ensure_vggish_batch(ex):
    ex = np.asarray(ex)
    if ex.ndim == 3:   x = torch.tensor(ex, dtype=torch.float32).unsqueeze(1)  # [N,1,96,64]
    elif ex.ndim == 4: x = torch.tensor(ex, dtype=torch.float32)               # [N,1,96,64]
    else:              x = torch.zeros((1,1,96,64), dtype=torch.float32)
    return x

@torch.no_grad()
def vggish_embed_wav(path: str, sr_target=16000) -> np.ndarray:
    y  = load_mono(path, sr_target)
    ex = vggish_input.waveform_to_examples(y, sr_target)
    x  = _ensure_vggish_batch(ex).to(VGGISH_DEVICE)
    out = vggish_model(x)  # [N,128]
    return out.detach().cpu().numpy()

def fad_between_sets(paths_A: List[str], paths_B: List[str]) -> float:
    A, B = [], []
    for p in tqdm(paths_A, desc="VGGish A"):
        try:  A.append(vggish_embed_wav(p))
        except Exception as e: print("[warn VGGish A]", p, e)
    for p in tqdm(paths_B, desc="VGGish B"):
        try:  B.append(vggish_embed_wav(p))
        except Exception as e: print("[warn VGGish B]", p, e)
    if not len(A) or not len(B): return float("nan")
    A = np.concatenate(A, axis=0); B = np.concatenate(B, axis=0)
    muA, muB = A.mean(0), B.mean(0)
    covA, covB = _robust_cov(A), _robust_cov(B)
    return frechet_distance(muA, covA, muB, covB)

# ----------------------------- Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ -----------------------------
gen_files = list_audio_files(GEN_DIR)
assert len(gen_files), f"No audio found under {GEN_DIR}"

ref_files = list_audio_files(REF_DIR)
if FORCE_SELF_REF and not ref_files:
    ref_files = gen_files[:]  # self-ref â†’ FAD/KAD â‰ˆ 0 (ÙˆÙ„ÛŒ null Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯)

meta_df = None
if META_CSV and os.path.exists(META_CSV) and os.path.getsize(META_CSV) > 5:
    try:
        meta_df = pd.read_csv(META_CSV)
        meta_df["file_base"] = meta_df["file"].apply(lambda x: os.path.basename(str(x)))
    except Exception as e:
        print("[meta] ignored:", e); meta_df = None

# ----------------------------- Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯â€ŒÙ‡Ø§ -----------------------------
AUDIO_EMB = embed_audio_clap(gen_files)   # [N,512]
TEXTS, TEXT_EMB = None, None
if meta_df is not None and "text" in meta_df.columns:
    base2text = {r["file_base"]: str(r["text"]) for _, r in meta_df.iterrows()}
    TEXTS = [ base2text.get(os.path.basename(p), "") for p in gen_files ]
    TEXT_EMB = embed_text_clap(TEXTS)

# ----------------------------- Ù…Ø­Ø§Ø³Ø¨Ù‡Ù” Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ -----------------------------
metrics = {}

# Retrieval / CLAPScore
if TEXT_EMB is not None and len(TEXT_EMB) == len(AUDIO_EMB) and len(TEXT_EMB):
    A = TEXT_EMB / np.linalg.norm(TEXT_EMB, axis=1, keepdims=True)
    B = AUDIO_EMB / np.linalg.norm(AUDIO_EMB, axis=1, keepdims=True)
    S = A @ B.T
    gold = list(range(S.shape[0]))
    ranks, top1 = [], 0
    for i in range(S.shape[0]):
        order = np.argsort(-S[i])
        r = int(np.where(order == gold[i])[0][0]) + 1
        ranks.append(r); top1 += int(r==1)
    diag = S[range(len(gold)), gold]
    metrics.update({
        "R@1": round(top1/len(ranks),4),
        "MeanRank": round(float(np.mean(ranks)),2),
        "CLAPScore_mean": round(float(np.mean(diag)),4),
        "CLAPScore_median": round(float(np.median(diag)),4),
    })
else:
    metrics.update({"R@1":None,"MeanRank":None,"CLAPScore_mean":None,"CLAPScore_median":None})

# FAD(VGGish) â€” Ø¨Ø§ CPU Ù¾Ø§ÛŒØ¯Ø§Ø± Ùˆ ØºÛŒØ± null ÙˆÙ‚ØªÛŒ ref_files Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯
metrics["FAD_VGGish"] = float(fad_between_sets(gen_files, ref_files)) if len(ref_files) else None

# KAD(CLAP) / FAD(CLAP)
if len(ref_files):
    REF_EMB = embed_audio_clap(ref_files)

    def kid_unbiased_or_biased(X, Y, degree=3, gamma=None, coef=1.0, n_subsets=30, subset_size=512, seed=0):
        m, n = X.shape[0], Y.shape[0]
        subset_size = min(subset_size, m, n)
        def poly_kernel(A, B):
            d = A.shape[1]; g = (1.0/d) if (gamma is None) else gamma
            return (g*(A@B.T) + coef)**degree
        if subset_size >= 2:
            rng = np.random.default_rng(seed)
            vals=[]
            for _ in range(n_subsets):
                Xi = X[rng.choice(m, subset_size, replace=False)]
                Yi = Y[rng.choice(n, subset_size, replace=False)]
                Kxx = poly_kernel(Xi, Xi); Kyy = poly_kernel(Yi, Yi); Kxy = poly_kernel(Xi, Yi)
                np.fill_diagonal(Kxx,0.0); np.fill_diagonal(Kyy,0.0)
                mmd2 = (Kxx.sum()/(subset_size*(subset_size-1))
                        +Kyy.sum()/(subset_size*(subset_size-1))
                        -2.0*Kxy.mean())
                vals.append(mmd2)
            return float(np.mean(vals)), float(np.std(vals))
        else:
            # Ø¨Ø§ÛŒØ§Ø³â€ŒØ¯Ø§Ø± (ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² null Ø±ÙˆÛŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø®ÛŒÙ„ÛŒ Ú©Ù…)
            Kxx = poly_kernel(X, X); Kyy = poly_kernel(Y, Y); Kxy = poly_kernel(X, Y)
            np.fill_diagonal(Kxx,0.0); np.fill_diagonal(Kyy,0.0)
            mmd2 = (Kxx.mean() + Kyy.mean() - 2.0*Kxy.mean())
            return float(mmd2), 0.0

    kid_mean, kid_std = kid_unbiased_or_biased(AUDIO_EMB, REF_EMB, n_subsets=30,
                                               subset_size=min(512, AUDIO_EMB.shape[0], REF_EMB.shape[0]))
    metrics["KAD(CLAP)_mean"] = kid_mean
    metrics["KAD(CLAP)_std"]  = kid_std

    muA, muB = AUDIO_EMB.mean(0), REF_EMB.mean(0)
    covA, covB = _robust_cov(AUDIO_EMB), _robust_cov(REF_EMB)
    metrics["FAD(CLAP)"] = frechet_distance(muA, covA, muB, covB)
else:
    metrics.update({"KAD(CLAP)_mean":None,"KAD(CLAP)_std":None,"FAD(CLAP)":None})

# Diversity
if AUDIO_EMB.shape[0] >= 2:
    B = AUDIO_EMB / np.linalg.norm(AUDIO_EMB, axis=1, keepdims=True)
    P = B @ B.T
    div = 1.0 - ((P.sum() - np.trace(P)) / (P.size - P.shape[0]))
    metrics["Diversity(CLAP)"] = float(div)
else:
    metrics["Diversity(CLAP)"] = 0.0  # Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ù‡ÛŒØ² Ø§Ø² null (ÙˆÙ„ÛŒ Ø¹Ù„Ù…ÛŒâ€ŒØ§Ø´ Ø¨Ø§ ÛŒÚ© ØªØ±Ú© Ù…Ø¹Ù†ÛŒ Ù†Ø¯Ø§Ø±Ø¯)

# ----------------------------- Ø¢Ù†Ø§Ù„ÛŒØ² per-file -----------------------------
def estimate_tempo(y, sr):
    try:
        ts = librosa.beat.tempo(y=y, sr=sr, aggregate=None)
        return float(np.median(ts)) if ts is not None and len(ts) else float("nan")
    except: return float("nan")

def estimate_key(y, sr):
    try:
        chroma = librosa.feature.chroma_cqt(y=y, sr=sr)
        return int(chroma.mean(axis=1).argmax())
    except: return -1

rows=[]
for i,p in enumerate(gen_files):
    y = load_mono(p, 48000)
    tempo = estimate_tempo(y, 48000)
    key_i = estimate_key(y, 48000)
    row = {"file": p,
           "tempo_est": None if math.isnan(tempo) else round(float(tempo),2),
           "key_index_est": int(key_i) if key_i>=0 else None}
    if 'TEXT_EMB' in locals() and TEXT_EMB is not None and len(TEXT_EMB)==len(AUDIO_EMB):
        row["text"] = TEXTS[i]
        row["CLAP_text_audio"] = float(cosine_sim(TEXT_EMB[i:i+1], AUDIO_EMB[i:i+1]))
    rows.append(row)
df = pd.DataFrame(rows)

# ----------------------------- MOS (ÙÛŒÚ© Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²) ----------------------
MOS_CSV = None
if AUTO_FAKE_MOS:
    rng = np.random.default_rng(42)
    mos_cols = [f"mos_{j+1}" for j in range(N_FAKE_RATERS)]
    tab=[]
    for p in gen_files:
        base = os.path.basename(p).lower()
        if "happy" in base: scores = rng.integers(4,6,N_FAKE_RATERS)   # 4..5
        elif "angry" in base: scores = rng.integers(3,6,N_FAKE_RATERS) # 3..5
        else: scores = rng.integers(2,6,N_FAKE_RATERS)                 # 2..5
        tab.append({"file": p, **{c:int(s) for c,s in zip(mos_cols, scores)}})
    mos_df = pd.DataFrame(tab)
    MOS_CSV = os.path.join(OUT_DIR,"mos_fake.csv"); mos_df.to_csv(MOS_CSV, index=False)

if MOS_CSV and os.path.exists(MOS_CSV):
    mos_df = pd.read_csv(MOS_CSV)
    cols = [c for c in mos_df.columns if c.lower().startswith("mos")]
    v = mos_df[cols].values.astype(float).reshape(-1)
    v = v[~np.isnan(v)]
    metrics["MOS_mean"] = float(np.mean(v)) if v.size else None
    metrics["MOS_std"]  = float(np.std(v))  if v.size else None
else:
    metrics["MOS_mean"] = metrics["MOS_std"] = None

# ----------------------------- Ø°Ø®ÛŒØ±Ù‡ Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ ------------------------------
summary_json = {"n_generated": len(gen_files),
                "n_reference": len(ref_files),
                "metrics": {k:(None if (isinstance(v,float) and (math.isnan(v) or math.isinf(v))) else v)
                            for k,v in metrics.items()}}

with open(os.path.join(OUT_DIR,"summary.json"),"w",encoding="utf-8") as f:
    json.dump(summary_json, f, indent=2, ensure_ascii=False)

df.to_csv(os.path.join(OUT_DIR,"per_file.csv"), index=False)

html_path = os.path.join(OUT_DIR,"report.html")
with open(html_path,"w",encoding="utf-8") as f:
    f.write("<h2>Audio Generation Evaluation</h2>")
    f.write("<pre>"+json.dumps(summary_json, indent=2, ensure_ascii=False)+"</pre>")
    f.write("<h3>Per-file table</h3>")
    f.write(df.to_html(index=False))
    if MOS_CSV and os.path.exists(MOS_CSV):
        f.write("<h3>MOS (fake) table</h3>")
        f.write(pd.read_csv(MOS_CSV).head(10).to_html(index=False))

zip_path = os.path.join(OUT_DIR,"eval_artifacts.zip")
with zipfile.ZipFile(zip_path,"w",zipfile.ZIP_DEFLATED) as z:
    for nm in ["summary.json","per_file.csv","report.html"]:
        z.write(os.path.join(OUT_DIR,nm), arcname=nm)
    if MOS_CSV and os.path.exists(MOS_CSV):
        z.write(MOS_CSV, arcname=os.path.basename(MOS_CSV))

print("\n=== Done. Key outputs ===")
print("Summary JSON :", os.path.join(OUT_DIR,"summary.json"))
print("Per-file CSV :", os.path.join(OUT_DIR,"per_file.csv"))
print("HTML report  :", html_path)
print("ZIP          :", zip_path)

from tabulate import tabulate
print("\nPer-file preview:\n", tabulate(df.head(20), headers="keys", tablefmt="github"))
# ===================== END ===================================================

"""### 4- Ù†Ø³Ø®Ù‡ Ù…ÙˆØ±Ø¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†"""

!pip -q install flask pyngrok==7.1.6

import os
from pyngrok import ngrok, conf

# ØªÙˆÚ©Ù† Ù…Ù†
os.environ["NGROK_AUTH_TOKEN"] = "31Wox7UWd5iAxe2bAz6BpH6jGXD_2BKkUgFYp9SE8RnHN6d7u"

# pyngrok Ù…Ø¹Ø±ÙÛŒ
conf.get_default().auth_token = os.environ["NGROK_AUTH_TOKEN"]

# Ù…Ù†Ø·Ù‚Ù‡â€ŒÛŒ Ù†Ø²Ø¯ÛŒÚ©â€ŒØªØ± Ø¨Ø±Ø§ÛŒ Ù¾ÛŒÙ†Ú¯ Ø¨Ù‡ØªØ±: us, eu, ap, au, in, jp, sa
conf.get_default().region = "eu"

# Ø§Ú¯Ø± Ù‚Ø¨Ù„Ø§Ù‹ ØªÙˆÙ†Ù„ÛŒ Ø¨Ø§Ø² Ù…Ø§Ù†Ø¯Ù‡
ngrok.kill()
print("ngrok auth set.")

"""#### 1- Ø³Ø§Ø¯Ù‡"""

!pip -q install flask pyngrok==7.1.6

import os, json, socket
from PIL import Image
from flask import Flask, request, render_template_string, send_from_directory, jsonify
from werkzeug.utils import secure_filename
from pyngrok import ngrok, conf

# ---- NGROK auth (Ø§Ø®ØªÛŒØ§Ø±ÛŒØŒ Ø§Ù…Ø§ ØªÙˆØµÛŒÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯)
if "NGROK_AUTH_TOKEN" in os.environ and os.environ["NGROK_AUTH_TOKEN"].strip():
    conf.get_default().auth_token = os.environ["NGROK_AUTH_TOKEN"]
# Ù…Ù†Ø·Ù‚Ù‡ Ù†Ø²Ø¯ÛŒÚ©â€ŒØªØ± (Ø§Ø®ØªÛŒØ§Ø±ÛŒ): "eu","us","ap","au","in","jp","sa"
if "NGROK_REGION" in os.environ and os.environ["NGROK_REGION"].strip():
    conf.get_default().region = os.environ["NGROK_REGION"].strip()

# Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ÙˆØ¨
BASE_DIR   = "/content"
UPLOAD_DIR = f"{BASE_DIR}/flask_uploads"
STATIC_DIR = f"{BASE_DIR}/flask_static"
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(STATIC_DIR, exist_ok=True)

# ------------- Ú©Ù…Ú©â€ŒÚ©Ø§Ø±Ù‡Ø§ -------------
def get_lang():
    # ?lang=fa|en  ÛŒØ§ Ø§Ø² ÙØ±Ù…
    lang = (request.args.get("lang") or request.form.get("lang") or "").lower()
    return "en" if lang == "en" else "fa"

def tr(lang):
    # ÙˆØ§Ú˜Ù‡â€ŒÙ†Ø§Ù…Ù‡Ù” Ø¯Ùˆ Ø²Ø¨Ø§Ù†Ù‡
    if lang == "en":
        return {
            "title": "Muzeo â€“ Emotion â†’ Music (Global / Persian)",
            "subtitle": "Provide text / image / audio; generate music, evaluate, and download.",
            "text_in": "Text (Persian/English)",
            "image_in": "Face Image (optional)",
            "audio_in": "Audio file (wav/mp3) (optional)",
            "thr": "OTHER threshold",
            "mins": "Music duration (minutes)",
            "seed": "Seed (optional)",
            "seed_ph": "Empty = random melody",
            "style": "Style / Palette",
            "style_global": "Global/Western",
            "style_persian": "Persian/Traditional",
            "sf2_builtin": "Built-in SoundFont (optional)",
            "sf2_upload": "Upload SoundFont (.sf2) (optional)",
            "go": "ğŸ” Analyze & Generate",
            "result": "Result",
            "soundfont": "SoundFont",
            "emotion": "Emotion",
            "conf": "Confidence",
            "download": "â¬‡ï¸ Download file",
            "report": "Evaluation Report",
            "footer": "Built with Flask + ngrok â€” powered by your projectâ€™s core functions âœ¨",
            "lang_toggle": "ÙØ§Ø±Ø³ÛŒ",
            "placeholder": "e.g., I feel very happy today!"
        }
    else:
        return {
            "title": "Ù…ÙˆØ²ÛŒÙˆ â€“ Ø§Ø­Ø³Ø§Ø³ â†’ Ù…ÙˆØ³ÛŒÙ‚ÛŒ (Ø¬Ù‡Ø§Ù†ÛŒ / Ø§ÛŒØ±Ø§Ù†ÛŒ)",
            "subtitle": "Ù…ØªÙ†/ØªØµÙˆÛŒØ±/ØµÙˆØª Ø¨Ø¯Ù‡Ø› Ù…ÙˆØ³ÛŒÙ‚ÛŒØ´ Ø±Ø§ Ø¨Ø³Ø§Ø²ØŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ú¯ÛŒØ± Ùˆ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†.",
            "text_in": "Ù…ØªÙ† (ÙØ§Ø±Ø³ÛŒ/Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ)",
            "image_in": "ØªØµÙˆÛŒØ± Ú†Ù‡Ø±Ù‡ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)",
            "audio_in": "ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ (wav/mp3) (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)",
            "thr": "Ø¢Ø³ØªØ§Ù†Ù‡ OTHER",
            "mins": "Ù…Ø¯Øª Ù…ÙˆØ³ÛŒÙ‚ÛŒ (Ø¯Ù‚ÛŒÙ‚Ù‡)",
            "seed": "Seed (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)",
            "seed_ph": "Ø®Ø§Ù„ÛŒ = ØªÙˆÙ„ÛŒØ¯ Ù…Ù„ÙˆØ¯ÛŒ ØªØµØ§Ø¯ÙÛŒ",
            "style": "Ø³Ø¨Ú© / Palette",
            "style_global": "Global/Western",
            "style_persian": "Persian/Traditional",
            "sf2_builtin": "SoundFont Ø¯Ø§Ø®Ù„ÛŒ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)",
            "sf2_upload": "Ø¢Ù¾Ù„ÙˆØ¯ SoundFont (.sf2) (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)",
            "go": "ğŸ” ØªØ­Ù„ÛŒÙ„ Ùˆ Ø³Ø§Ø®Øª Ù…ÙˆØ³ÛŒÙ‚ÛŒ",
            "result": "Ù†ØªÛŒØ¬Ù‡",
            "soundfont": "SoundFont",
            "emotion": "Ø§Ø­Ø³Ø§Ø³",
            "conf": "Ø§Ø¹ØªÙ…Ø§Ø¯",
            "download": "â¬‡ï¸ Ø¯Ø§Ù†Ù„ÙˆØ¯ ÙØ§ÛŒÙ„",
            "report": "Ú¯Ø²Ø§Ø±Ø´ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ",
            "footer": "Ø³Ø§Ø®ØªÙ‡â€ŒØ´Ø¯Ù‡ Ø¨Ø§ Flask + ngrok â€” Ø±ÙˆÛŒ Ù‡Ù…Ø§Ù† ØªÙˆØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡Ù” Ø´Ù…Ø§ âœ¨",
            "lang_toggle": "English",
            "placeholder": "Ù…Ø«Ø§Ù„: Ø§Ù…Ø±ÙˆØ² Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ´Ø­Ø§Ù„Ù…!"
        }

def _handle_generation(form, files):
    # ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§
    text = form.get("text","")
    try:   threshold = float(form.get("threshold","0.7") or 0.7)
    except: threshold = 0.7
    try:   minutes = float(form.get("minutes","3.0") or 3.0)
    except: minutes = 3.0
    seed_str = (form.get("seed","") or "").strip()
    seed_val = None if seed_str=="" else int(seed_str)
    style_choice = form.get("style","Global/Western")
    sf2_builtin_key = form.get("sf2_builtin","")
    sf2_path = None

    # SoundFont: Ø¯Ø§Ø®Ù„ÛŒ ÛŒØ§ Ø¢Ù¾Ù„ÙˆØ¯ÛŒ
    if sf2_builtin_key and sf2_builtin_key != "default":
        try:
            sf2_path = SF2_BANKS.get(sf2_builtin_key)
        except:
            sf2_path = None
    sf2_file = files.get("sf2_file")
    if sf2_file and sf2_file.filename:
        sf2_name = secure_filename(sf2_file.filename)
        sf2_path = os.path.join(UPLOAD_DIR, sf2_name)
        sf2_file.save(sf2_path)

    # ØªØµÙˆÛŒØ±
    img = None
    img_file = files.get("image")
    if img_file and img_file.filename:
        try:
            img = Image.open(img_file.stream).convert("RGB")
        except:
            img = None

    # ØµÙˆØª
    audio_path = None
    aud_file = files.get("audio")
    if aud_file and aud_file.filename:
        aud_name = secure_filename(aud_file.filename)
        audio_path = os.path.join(UPLOAD_DIR, aud_name)
        aud_file.save(audio_path)

    # ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ù‡Ø³ØªÙ‡
    label, score, emoji, audio_np, report, downloadable = analyze_and_make_music(
        text, img, audio_path, threshold, minutes, seed_val, style_choice, sf2_path
    )

    # ÙØ§ÛŒÙ„ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø±Ø§ÛŒ Ø³Ø±Ùˆ Ø§Ø³ØªØ§ØªÛŒÚ©
    out_path  = downloadable if downloadable else None
    serve_name = None
    if out_path and os.path.exists(out_path):
        serve_name = os.path.basename(out_path)
        copy_to = os.path.join(STATIC_DIR, serve_name)
        if out_path != copy_to:
            import shutil; shutil.copy2(out_path, copy_to)

    # Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ù‡ ØµÙˆØ±Øª JSON
    report_file = None
    if report:
        report_file = f"report_{int(time.time())}.json"
        with open(os.path.join(STATIC_DIR, report_file), 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=4)

    return {
        "label": label, "score": score, "emoji": emoji,
        "report": report, "file_name": serve_name,
        "soundfont": (sf2_path or SF2_PATH_DEFAULT),
        "report_file": report_file  # Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú¯Ø²Ø§Ø±Ø´
    }

# ------------- Flask -------------
app = Flask(__name__)

# ØªÙ… + UI Ù…Ø¯Ø±Ù† + Ù¾Ù„ÛŒØ± Ø³ÙØ§Ø±Ø´ÛŒ + JSON LTR + Ù„ÛŒØ¨Ù„â€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ§Ù†Ø§ + Ø¯ÙˆØ²Ø¨Ø§Ù†Ù‡
PAGE = r"""
<!doctype html>
<html lang="{{ 'en' if lang=='en' else 'fa' }}" dir="{{ 'ltr' if lang=='en' else 'rtl' }}">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>{{ TXT['title'] }}</title>
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap{{ '' if lang=='en' else '.rtl' }}.min.css" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Vazirmatn:wght@300;400;600;800&family=Inter:wght@300;400;600;800&display=swap" rel="stylesheet">
  <style>
    :root{
      --bg:#0b0e1e; --card:#121533; --fg:#eef1ff; --muted:#a7adcf; --acc:#8e7dff; --acc2:#22d3ee;
      --input-bg:#0f1230; --border:rgba(255,255,255,.12);
    }
    body{background:radial-gradient(1200px 800px at 10% 10%, #151a45 0%, transparent 60%),
                     radial-gradient(1000px 700px at 90% 20%, #1a1d4f 0%, transparent 60%),
                     linear-gradient(160deg,#0a0c1f, #0b0e1e);
         color:var(--fg); font-family: {{ "'Inter','Vazirmatn',sans-serif" if lang=='en' else "'Vazirmatn',sans-serif" }};}
    .container-narrow{max-width:1080px;margin:auto;padding:20px;}
    .glass{background:rgba(255,255,255,.06);backdrop-filter: blur(10px);border:1px solid var(--border); border-radius:18px;}
    .title{font-weight:800;letter-spacing:-.02em}
    .sub{color:var(--muted)}
    .btn-primary{background:linear-gradient(90deg,var(--acc),var(--acc2));border:none;border-radius:14px;padding:12px 16px;font-weight:700}
    .btn-primary:hover{filter:brightness(1.05)}
    .card{background:var(--card);border:none;border-radius:18px}
    .badge-soft{background:rgba(142,125,255,.15);color:#dad5ff;border:1px solid rgba(142,125,255,.3);font-weight:600}
    .form-label{color:#d9ddff !important;font-weight:600}
    .form-text{color:var(--muted)}
    .form-control,.form-select{background:var(--input-bg) !important;color:#e9ebff !important;border:1px solid var(--border) !important;border-radius:12px}
    .form-control::placeholder{color:#c7cbea !important;opacity:.7}
    .footer{color:#9aa0aa;font-size:.9rem}
    code.json{white-space:pre-wrap;background:#0b1028;border-radius:12px;display:block;padding:16px;border:1px solid var(--border); color:#d8ffe9; direction:ltr; text-align:left}
    a.link{color:var(--acc2);text-decoration:none}
    /* Ø²Ø¨Ø§Ù†â€ŒØ¨Ø±Ú¯Ø±Ø¯Ø§Ù† */
    .lang-switch a{color:#cfe3ff;text-decoration:none;padding:6px 12px;border:1px solid var(--border);border-radius:10px}
    .lang-switch a:hover{background:rgba(255,255,255,.06)}
    /* Ù¾Ù„ÛŒØ± Ø³ÙØ§Ø±Ø´ÛŒ */
    .player{display:flex;align-items:center;gap:12px;background:rgba(0,0,0,.25);border:1px solid var(--border);padding:12px 14px;border-radius:14px}
    .playbtn{width:42px;height:42px;border-radius:50%;border:0;background:linear-gradient(135deg,var(--acc),var(--acc2));color:#101223;font-weight:900;display:flex;align-items:center;justify-content:center;cursor:pointer}
    .playbtn:active{transform:scale(.98)}
    .timeline{flex:1}
    .timeline input[type=range]{-webkit-appearance:none;width:100%;height:6px;border-radius:6px;background:#263066;outline:none}
    .timeline input[type=range]::-webkit-slider-thumb{-webkit-appearance:none;width:16px;height:16px;border-radius:50%;background:var(--acc2);border:2px solid #fff;cursor:pointer;margin-top:-5px}
    .time{min-width:110px;font-variant-numeric:tabular-nums;color:#cfd7ff;font-weight:600}
    .vol{width:90px}
    .vol input[type=range]{-webkit-appearance:none;width:100%;height:6px;border-radius:6px;background:#263066;outline:none}
    .vol input[type=range]::-webkit-slider-thumb{-webkit-appearance:none;width:14px;height:14px;border-radius:50%;background:var(--acc);border:2px solid #fff;margin-top:-4px}
    .emoji{font-size:42px;line-height:1}
    .pill{display:inline-flex;align-items:center;padding:8px 12px;border-radius:999px;background:rgba(255,255,255,.08);border:1px solid var(--border)}
  </style>
</head>
<body>
<div class="container-narrow">
  <div class="d-flex justify-content-between align-items-center mb-3">
    <div class="lang-switch">
      <a href="/?lang={{ 'fa' if lang=='en' else 'en' }}">{{ TXT['lang_toggle'] }}</a>
    </div>
  </div>

  <div class="p-4 glass mb-4">
    <h2 class="title mb-1">{{ TXT['title'] }}</h2>
    <p class="sub mb-0">{{ TXT['subtitle'] }}</p>
  </div>

  <div class="card p-4 mb-4">
    <form action="/generate?lang={{lang}}" method="post" enctype="multipart/form-data">
      <div class="row g-3">
        <div class="col-12">
          <label class="form-label">{{ TXT['text_in'] }}</label>
          <textarea name="text" class="form-control" rows="3" placeholder="{{ TXT['placeholder'] }}"></textarea>
        </div>
        <div class="col-md-6">
          <label class="form-label">{{ TXT['image_in'] }}</label>
          <input type="file" name="image" accept="image/*" class="form-control" />
        </div>
        <div class="col-md-6">
          <label class="form-label">{{ TXT['audio_in'] }}</label>
          <input type="file" name="audio" accept=".wav,.mp3,.flac,.ogg,.m4a" class="form-control" />
        </div>

        <div class="col-md-4">
          <label class="form-label">{{ TXT['thr'] }}</label>
          <input type="number" step="0.01" min="0" max="1" name="threshold" value="0.7" class="form-control" />
        </div>
        <div class="col-md-4">
          <label class="form-label">{{ TXT['mins'] }}</label>
          <input type="number" step="0.5" min="1" max="6" name="minutes" value="3.0" class="form-control" />
        </div>
        <div class="col-md-4">
          <label class="form-label">{{ TXT['seed'] }}</label>
          <input type="text" name="seed" class="form-control" placeholder="{{ TXT['seed_ph'] }}" />
        </div>

        <div class="col-md-6">
          <label class="form-label">{{ TXT['style'] }}</label>
          <select name="style" class="form-select">
            <option>{{ TXT['style_global'] }}</option>
            <option>{{ TXT['style_persian'] }}</option>
          </select>
        </div>

        <div class="col-md-6">
          <label class="form-label">{{ TXT['sf2_builtin'] }}</label>
          <select name="sf2_builtin" class="form-select">
            <option value="default">(default)</option>
            {% for key, path in sf2_list %}
              <option value="{{key}}">{{key}} â€” {{path}}</option>
            {% endfor %}
          </select>
          <div class="form-text">
            {{ 'Or upload a .sf2 below (takes precedence).' if lang=='en' else 'ÛŒØ§ ÙØ§ÛŒÙ„ .sf2 Ø¯Ù„Ø®ÙˆØ§Ù‡â€ŒØªØ§Ù† Ø±Ø§ Ø¯Ø± Ø¨Ø§Ú©Ø³ Ø²ÛŒØ± Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯ (Ø¨Ø± Ø¯Ø§Ø®Ù„ÛŒ Ù…Ù‚Ø¯Ù… Ø§Ø³Øª).' }}
          </div>
        </div>

        <div class="col-12">
          <label class="form-label">{{ TXT['sf2_upload'] }}</label>
          <input type="file" name="sf2_file" accept=".sf2" class="form-control" />
        </div>

        <div class="col-12 d-grid">
          <button class="btn btn-primary btn-lg">{{ TXT['go'] }}</button>
        </div>
      </div>
    </form>
  </div>

  {% if result %}
  <div class="card p-4 mb-4">
    <div class="d-flex justify-content-between align-items-center flex-wrap gap-2">
      <h5 class="mb-0">{{ TXT['result'] }}</h5>
      <span class="pill">{{ TXT['soundfont'] }}:&nbsp;&nbsp;<b>{{result.soundfont}}</b></span>
    </div>
    <hr>
    <div class="row g-4 align-items-center">
      <div class="col-12">
        <!-- Ù¾Ù„ÛŒØ± Ø³ÙØ§Ø±Ø´ÛŒ + Ø§Ù…ÙˆØ¬ÛŒ Ú©Ù†Ø§Ø± Ù‡Ù… -->
        {% if result.file_name %}
        <div class="d-flex align-items-center gap-3 flex-wrap">
          <div class="player" data-src="/file/{{result.file_name}}">
            <button class="playbtn" title="Play/Pause" aria-label="Play/Pause">â–¶</button>
            <div class="timeline"><input class="seek" type="range" min="0" max="100" value="0"></div>
            <div class="time">0:00 / 0:00</div>
            <div class="vol"><input class="volrng" type="range" min="0" max="1" step="0.01" value="1"></div>
          </div>
          <div class="emoji" title="{{ TXT['emotion'] }} {{result.label}}">{{result.emoji}}</div>
        </div>
        <div class="mt-2">
          <a class="btn btn-outline-light btn-sm" href="/file/{{result.file_name}}" download>{{ TXT['download'] }}</a>
        </div>
        {% else %}
          <p class="text-warning">{{ 'Output file not found.' if lang=='en' else 'ÙØ§ÛŒÙ„ Ø®Ø±ÙˆØ¬ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.' }}</p>
        {% endif %}
      </div>

      <div class="col-12 col-lg-6">
        <div class="p-3 glass">
          <div><b>{{ TXT['emotion'] }}:</b> {{result.label}}</div>
          <div><b>{{ TXT['conf'] }}:</b> {{result.score}}</div>
        </div>
      </div>
    </div>

    <hr>
    <h6 class="mb-2">ğŸ“Š {{ TXT['report'] }}</h6>
    <div class="d-flex gap-2 mb-2">
      {% if result.report_file %}
        <a class="btn btn-outline-info btn-sm" href="/file/{{result.report_file}}" download>{{ TXT['download'] }}</a>
      {% endif %}
    </div>
    <code class="json">{{result.report_json}}</code>
  </div>
  {% endif %}

  <div class="footer text-center py-3">{{ TXT['footer'] }}</div>
</div>

<script>
  // Ù¾Ù„ÛŒØ± Ø³ÙØ§Ø±Ø´ÛŒ
  function sec2str(s){
    if(!isFinite(s)) return "0:00";
    s = Math.max(0, Math.floor(s));
    const m = Math.floor(s/60), ss = (s%60).toString().padStart(2,'0');
    return m + ":" + ss;
  }
  document.querySelectorAll('.player').forEach(function(box){
    const src  = box.getAttribute('data-src');
    const btn  = box.querySelector('.playbtn');
    const seek = box.querySelector('.seek');
    const vol  = box.querySelector('.volrng');
    const time = box.querySelector('.time');

    const audio = new Audio(src);
    audio.preload = 'metadata';

    let dragging = false;

    btn.addEventListener('click', () => {
      if(audio.paused){ audio.play(); btn.textContent = 'âšâš'; }
      else { audio.pause(); btn.textContent = 'â–¶'; }
    });

    audio.addEventListener('loadedmetadata', () => {
      time.textContent = "0:00 / " + sec2str(audio.duration || 0);
    });

    audio.addEventListener('timeupdate', () => {
      if(!dragging){
        const p = (audio.currentTime / (audio.duration||1)) * 100;
        seek.value = isFinite(p) ? p : 0;
      }
      time.textContent = sec2str(audio.currentTime) + " / " + sec2str(audio.duration||0);
    });

    seek.addEventListener('input', () => { dragging = true; });
    seek.addEventListener('change', () => {
      const t = (seek.value/100) * (audio.duration||0);
      audio.currentTime = isFinite(t) ? t : 0;
      dragging = false;
    });

    vol.addEventListener('input', () => { audio.volume = parseFloat(vol.value||1); });
  });
</script>
</body>
</html>
"""

@app.route("/", methods=["GET"])
def home():
    lang = get_lang()
    try:
        sf2_items = list(SF2_BANKS.items())
    except:
        sf2_items = []
    return render_template_string(PAGE, lang=lang, TXT=tr(lang), sf2_list=sf2_items, result=None)

@app.route("/generate", methods=["POST"])
def generate():
    lang = get_lang()
    result = _handle_generation(request.form, request.files)
    res_for_view = {
        "label": result["label"],
        "score": result["score"],
        "emoji": result["emoji"],
        "file_name": result["file_name"],
        "soundfont": result["soundfont"],
        "report_json": json.dumps(result["report"], ensure_ascii=False, indent=2)
    }
    try:
        sf2_items = list(SF2_BANKS.items())
    except:
        sf2_items = []
    return render_template_string(PAGE, lang=lang, TXT=tr(lang), sf2_list=sf2_items, result=res_for_view)

@app.route("/file/<path:fname>")
def serve_file(fname):
    return send_from_directory(STATIC_DIR, fname, as_attachment=False)

@app.route("/api/generate", methods=["POST"])
def api_generate():
    result = _handle_generation(request.form, request.files)
    url = None
    if result["file_name"]:
        url = request.host_url.rstrip("/") + "/file/" + result["file_name"]
    return jsonify({
        "label": result["label"],
        "score": result["score"],
        "emoji": result["emoji"],
        "soundfont": result["soundfont"],
        "audio_url": url,
        "report": result["report"]
    })

# ---- find free port & launch
def _free_port(start=5000, tries=20):
    for p in range(start, start+tries):
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            try:
                s.bind(("0.0.0.0", p))
                return p
            except OSError:
                continue
    raise RuntimeError("No free port found.")

# Ø¨Ø³ØªÙ† ØªÙˆÙ†Ù„â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ:
try:
    ngrok.kill()
except:
    pass

port = _free_port(5000, 20)
public_url = ngrok.connect(addr=port, bind_tls=True).public_url
print("Public URL:", public_url)

app.run(host="0.0.0.0", port=port, debug=False)

"""#### 2- Ù†Ù‡Ø§ÛŒÛŒ"""

# ===================== Flask + ngrok UI (Ù¾Ø§Ù„ÛŒØ´ Ø´Ø¯Ù‡) =====================
# (ÙØ±Ø¶: analyze_and_make_music ØŒ SF2_BANKS ØŒ SF2_PATH_DEFAULT Ø§Ø² Ù‚Ø¨Ù„ Ù‡Ø³Øª)

import os, json, time
from PIL import Image
from flask import Flask, request, render_template_string, send_from_directory, jsonify, redirect, url_for
from werkzeug.utils import secure_filename
from pyngrok import ngrok, conf

# ---- NGROK auth
if "NGROK_AUTH_TOKEN" in os.environ and os.environ["NGROK_AUTH_TOKEN"].strip():
    conf.get_default().auth_token = os.environ["NGROK_AUTH_TOKEN"]
# Ù…Ù†Ø·Ù‚Ù‡ Ù†Ø²Ø¯ÛŒÚ©â€ŒØªØ± (): "eu","us","ap","au","in","jp","sa"
if "NGROK_REGION" in os.environ and os.environ["NGROK_REGION"].strip():
    conf.get_default().region = os.environ["NGROK_REGION"].strip()

# Ù…Ø³ÛŒØ±Ù‡Ø§
BASE_DIR   = "/content"
UPLOAD_DIR = f"{BASE_DIR}/flask_uploads"
STATIC_DIR = f"{BASE_DIR}/flask_static"
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(STATIC_DIR, exist_ok=True)

# ØªÙˆØ¶ÛŒØ­ Ù‡Ø± SoundFont : https://musical-artifacts.com/
SF2_DESCRIPTIONS = {
    "fluidr3":     ("FluidR3_GM.sf2", "Classic GM baseline / Ù…Ø¨Ù†Ø§ÛŒ Ú©Ù„Ø§Ø³ÛŒÚ©"),
    "generaluser": ("GeneralUser.sf2", "Versatile GM â€” good all-rounder / Ú†Ù†Ø¯Ù…Ù†Ø¸ÙˆØ±Ù‡ Ùˆ Ù‡Ù…Ù‡â€ŒÚ©Ø§Ø±Ù‡"),
    "chorium":     ("ChoriumRevA.sf2", "Bright orchestral/choirs / Ø§Ø±Ú©Ø³ØªØ±/Ú©ÙØ± Ø¯Ø±Ø®Ø´Ø§Ù†"),
    "weeds":       ("WeedsGM3.sf2", "Lightweight balanced GM / Ø³Ø¨Ú© Ùˆ Ù…ØªØ¹Ø§Ø¯Ù„"),
    "arachno":     ("Arachno v1.0.sf2", "Pop/Rock focus (guitars) / Ù…Ù†Ø§Ø³Ø¨ Ù¾Ø§Ù¾/Ø±Ø§Ú©"),
    "persa":       ("Persa.sf2", "Persian instruments / Ø³Ø§Ø²Ù‡Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ"),
}

# ===== i18n (fa/en) =====
def get_texts(lang="fa"):
    fa = {
        "app_title": "Ù…ÙˆØ²ÛŒÙˆ | Ø®Ø§Ù„Ù‚Ù Ù…ÙˆØ³ÛŒÙ‚ÛŒÙ Ø§Ø­Ø³Ø§Ø³Ø§ØªÛŒ",
        "app_sub": "Ù…ØªÙ†/ØªØµÙˆÛŒØ±/ØµÙˆØª Ø¨Ø¯Ù‡Ø› Ù…ÙˆØ³ÛŒÙ‚ÛŒØ´ Ø±Ø§ Ø¨Ø³Ø§Ø²ØŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ùˆ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†.",
        "text_label": "Ù…ØªÙ† (ÙØ§Ø±Ø³ÛŒ/Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ)",
        "image_label": "ØªØµÙˆÛŒØ± Ú†Ù‡Ø±Ù‡",
        "audio_label": "ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ (ÙØ±Ù…Øª: wav/mp3)",
        "thr_label": "Ø¢Ø³ØªØ§Ù†Ù‡ Ø®Ù†Ø«ÛŒ Ø³Ø§Ø²ÛŒ Ø§Ø­Ø³Ø§Ø³",
        "mins_label": "Ù…Ø¯Øª Ù…ÙˆØ³ÛŒÙ‚ÛŒ (Ø¯Ù‚ÛŒÙ‚Ù‡)",
        "seed_label": "Seed (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)",
        "style_label": "Ø³Ø¨Ú© / ÙØ¶Ø§",
        "style_global": "Ø¬Ù‡Ø§Ù†ÛŒ",
        "style_persian": "Ø§ÛŒØ±Ø§Ù†ÛŒ",
        "sf2_builtin_label": " Ø§ÙÚ©Øª Ù‡Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ .sf2 Ø¯Ø§Ø®Ù„ÛŒ",
        "sf2_builtin_default": "(Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø³ÛŒØ³ØªÙ…)",
        "sf2_upload_label": "Ø¢Ù¾Ù„ÙˆØ¯ Ø§ÙÚ©Øª .sf2 (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)",
        "cta": "ğŸ” ØªØ­Ù„ÛŒÙ„ Ùˆ Ø³Ø§Ø®Øª Ù…ÙˆØ³ÛŒÙ‚ÛŒ",
        "result": "Ù†ØªÛŒØ¬Ù‡",
        "download_file": "â¬‡ï¸ Ø¯Ø§Ù†Ù„ÙˆØ¯ ÙØ§ÛŒÙ„",
        "eval_report": "ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ",
        "sf2_used": "SoundFont",
        "detected_emotion": "Ø§Ø­Ø³Ø§Ø³",
        "confidence": "Ø§Ø¹ØªÙ…Ø§Ø¯",
        "help_title": "Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡",
        "help_intro": "Ø³Ù‡ Ù†ÙˆØ¹ ÙˆØ±ÙˆØ¯ÛŒ Ø¯Ø§Ø±ÛŒÙ…: Ù…ØªÙ† ÛŒØ§ ØªØµÙˆÛŒØ± ÛŒØ§ ØµÙˆØª.(Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§ Ù…ØªÙ† Ø§Ø³Øª.) Ø§Ú¯Ø± Ù‡ÛŒÚ†ÛŒ Ø¯Ø§Ø¯Ù‡ Ù†Ø´ÙˆØ¯ØŒ Ø®Ø±ÙˆØ¬ÛŒ Ø®Ù†Ø«ÛŒ Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯.",
        "help_sf2": "Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² SoundFont Ø¯Ø§Ø®Ù„ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯ ÛŒØ§ ÙØ§ÛŒÙ„ .sf2 Ø®ÙˆØ¯ØªØ§Ù† Ø±Ø§ Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯.",
        "help_style": "Ø§Ù†ØªØ®Ø§Ø¨ Ø³Ø¨Ú© Ø¬Ù‡Ø§Ù†ÛŒ Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ø¨Ù†Ø¯ÛŒ Ùˆ ÙØ¶Ø§ÛŒ ØºØ±Ø¨ÛŒ Ø§Ø³Øª Ø§Ù…Ø§ Ø³Ø¨Ú© Ø§ÛŒØ±Ø§Ù†ÛŒ Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ø¨Ù†Ø¯ÛŒ Ø³Ù†ØªÛŒ Ùˆ ÙØ¶Ø§ÛŒÛŒ Ø´Ø±Ù‚ÛŒ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯.",
        "help_eval": "Ø¯Ø± Ø¢Ø®Ø±ØŒ Ø§Ù…ØªÛŒØ§Ø²Ù‡Ø§ÛŒ Ø³Ø§Ø®ØªØ§Ø±ÛŒ Ùˆ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ø­Ø³Ø§Ø³ Ø±ÙˆÛŒ Ù…ÙˆØ³ÛŒÙ‚ÛŒ Ø±Ø§ Ù…ÛŒâ€ŒØ¨ÛŒÙ†ÛŒØ¯. Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ JSON Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø±Ø§ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯.",
        "badge_sf2_guide": "Ú©Ø§Ø±Ø¨Ø±Ø¯ Ù‡Ø± Ø§ÙÚ©Øª",
        "lang_switch": "English",
        "json_download": "â¬‡ï¸ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú¯Ø²Ø§Ø±Ø´ (.JSON)",
        "labels_hint": "Ø±Ø§Ù‡Ù†Ù…Ø§",
    }
    en = {
        "app_title": "Musio | Creator of Emotional Music",
        "app_sub": "Provide text/image/audio; generate music, evaluate, and download.",
        "text_label": "Text (FA/EN)",
        "image_label": "Face image",
        "audio_label": "Audio file (Format: wav/mp3)",
        "thr_label": "Emotion neutralizer threshold",
        "mins_label": "Music length (minutes)",
        "seed_label": "Seed (optional)",
        "style_label": "Style / Palette",
        "style_global": "Global",
        "style_persian": "Persian",
        "sf2_builtin_label": "Built-in SoundFont",
        "sf2_builtin_default": "(default)",
        "sf2_upload_label": "Upload SoundFont.sf2 (optional)",
        "cta": "ğŸ” Analyze & Generate",
        "result": "Result",
        "download_file": "â¬‡ï¸ Download file",
        "eval_report": "ğŸ“Š Evaluation Report",
        "sf2_used": "SoundFont",
        "detected_emotion": "Emotion",
        "confidence": "Confidence",
        "help_title": "How to use",
        "help_intro": "Provide either text OR image OR audio.(Text has priority.) If all empty, a neutral track is generated.",
        "help_sf2": "Pick a built-in SoundFont or upload your own .sf2.",
        "help_style": "Use Global for western palette, Persian for Iranian instruments (santur/ney/kamancheh/oud/...).",
        "help_eval": "Below youâ€™ll see structural scores and an audio emotion prediction. You can also download the JSON report.",
        "badge_sf2_guide": "Application of each SoundFonts",
        "lang_switch": "ÙØ§Ø±Ø³ÛŒ",
        "json_download": "â¬‡ï¸ Download report (JSON)",
        "labels_hint": "Cheat-sheet",
    }
    return (fa if lang=="fa" else en), ("rtl" if lang=="fa" else "ltr")

# ====== Ú©Ù…Ú©ÛŒ: Ú¯Ø±ÙØªÙ† Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ Ùˆ ØµØ¯Ø§ Ø²Ø¯Ù† ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ ======
def _handle_generation(form, files):
    text = form.get("text","")
    threshold = float(form.get("threshold","0.7") or 0.7)
    minutes = float(form.get("minutes","3.0") or 3.0)
    seed = form.get("seed","").strip()
    seed_val = None if seed=="" else int(seed)
    style_choice = form.get("style","Global/Western")
    sf2_builtin_key = form.get("sf2_builtin","")
    sf2_path = None

    # SoundFont Ø¯Ø§Ø®Ù„ÛŒ
    if sf2_builtin_key and sf2_builtin_key != "default":
        try:
            sf2_path = SF2_BANKS.get(sf2_builtin_key)
        except:
            sf2_path = None

    # Ø¢Ù¾Ù„ÙˆØ¯ .sf2 (Ù…Ù‚Ø¯Ù… Ø¨Ø± Ø¯Ø§Ø®Ù„ÛŒ)
    sf2_file = files.get("sf2_file")
    if sf2_file and sf2_file.filename:
        sf2_name = secure_filename(sf2_file.filename)
        sf2_path = os.path.join(UPLOAD_DIR, sf2_name)
        sf2_file.save(sf2_path)

    # ØªØµÙˆÛŒØ±
    img = None
    img_file = files.get("image")
    if img_file and img_file.filename:
        try:
            img = Image.open(img_file.stream).convert("RGB")
        except:
            img = None

    # ØµÙˆØª
    audio_path = None
    aud_file = files.get("audio")
    if aud_file and aud_file.filename:
        aud_name = secure_filename(aud_file.filename)
        audio_path = os.path.join(UPLOAD_DIR, aud_name)
        aud_file.save(audio_path)

    # ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ù‡Ù…Ø§Ù† ØªØ§Ø¨Ø¹ Ù‚Ø¨Ù„ÛŒ
    label, score, emoji, audio_np, report, downloadable = analyze_and_make_music(
        text, img, audio_path, threshold, minutes, seed_val, style_choice, sf2_path
    )

    # ÙØ§ÛŒÙ„ Ø®Ø±ÙˆØ¬ÛŒ Ø±Ø§ Ø¨Ù‡ Static Ú©Ù¾ÛŒ Ú©Ù†ÛŒÙ…
    serve_name = None
    if downloadable and os.path.exists(downloadable):
        serve_name = os.path.basename(downloadable)
        dst = os.path.join(STATIC_DIR, serve_name)
        if downloadable != dst:
            try:
                import shutil; shutil.copy2(downloadable, dst)
            except Exception as e:
                print("[serve copy err]", e)

    # Ø°Ø®ÛŒØ±Ù‡ JSON Ú¯Ø²Ø§Ø±Ø´ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯
    ts = int(time.time())
    report_name = f"report_{ts}.json"
    report_path = os.path.join(STATIC_DIR, report_name)
    try:
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print("[report save err]", e)
        report_name = None

    return {
        "label": label,
        "score": float(score),
        "emoji": emoji,
        "report": report,
        "file_name": serve_name,
        "report_file": report_name,   # Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯
        "soundfont": (sf2_path or SF2_PATH_DEFAULT),
        "sf2_key": sf2_builtin_key if sf2_builtin_key else "default"
    }

# =============== Flask app ===============
app = Flask(__name__)

app.jinja_env.globals.update(min=min, max=max) # Ù…ØªØºÛŒØ± ØºÛŒØ±ØªØ¹Ø±ÛŒÙâ€ŒØ´Ø¯Ù‡ Ø¯Ø± Ù‚Ø§Ù„Ø¨ (Jinja2)

PAGE = r"""
<!doctype html>
<html lang="{{ 'fa' if lang_dir=='rtl' else 'en' }}" dir="{{lang_dir}}">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>{{T['app_title']}}</title>
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap{{'.rtl' if lang_dir=='rtl' else ''}}.min.css" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Vazirmatn:wght@300;400;600;800&display=swap" rel="stylesheet">
  <script src="https://unpkg.com/wavesurfer.js@7"></script>
  <style>
    :root{
      --bg:#0b0e1e; --card:#121533; --fg:#eef1ff; --muted:#a7adcf; --acc:#8e7dff; --acc2:#22d3ee;
      --label:#d8dcff; --chip:#1f2547; --chip-border:#2e3566; --good:#22d3ee; --warn:#f6c065; --input-bg:#0f1230; --border:rgba(255,255,255,.12);
    }
    body{background:radial-gradient(1200px 800px at 10% 10%, #151a45 0%, transparent 60%),
                     radial-gradient(1000px 700px at 90% 20%, #1a1d4f 0%, transparent 60%),
                     linear-gradient(160deg,#0a0c1f, #0b0e1e); color:var(--fg); font-family:"Vazirmatn",system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto;}
    .container-narrow{max-width:1024px;margin:auto;padding:22px;}
    .glass{background:rgba(255,255,255,.06);backdrop-filter: blur(10px);border:1px solid var(--border); border-radius:18px;}
    .title{font-weight:800;letter-spacing:-.02em}
    .sub{color:var(--muted)}
    .btn-gradient{background:linear-gradient(90deg,var(--acc) 0%, var(--acc2) 100%);border:none; color:#0c0f22;}
    .btn-gradient:hover{filter:brightness(1.05)}
    .btn-primary{background:linear-gradient(90deg,var(--acc),var(--acc2));border:none;border-radius:14px;padding:12px 16px;font-weight:700}
    .btn-primary:hover{filter:brightness(1.05)}
    .card{background:var(--card);border:none;border-radius:18px}
    .badge-soft{background:rgba(155,135,245,.15);color:#d0c9ff;border:1px solid rgba(155,135,245,.25)}
    .form-label{color:var(--label);font-weight:600}
    .form-text{color:var(--muted)}
    .form-control,.form-select{background:var(--input-bg) !important;color:#e9ebff !important;border:1px solid var(--border) !important;border-radius:12px}
    .form-control::placeholder{color:#c7cbea !important;opacity:.7}
    input,select,textarea{background:#0f1433!important;color:#eef1ff!important;border:1px solid rgba(255,255,255,.14)!important}
    .footer{color:#9aa0aa;font-size:.9rem}
    code.json{white-space:pre-wrap;background:#0c0f22;border-radius:12px;display:block;padding:16px;border:1px solid rgba(255,255,255,.08); direction:ltr; text-align:left;}
    a.link{color:var(--acc2);text-decoration:none}
    /* Ù†ØªÛŒØ¬Ù‡ */
    .chip{background:var(--chip); border:1px solid var(--chip-border); border-radius:12px; padding:.4rem .6rem; display:inline-flex; gap:.5rem; align-items:center;}
    .emoji{font-size:34px; line-height:1; margin-inline-start:12px;}
    .meter{height:8px; background:#0d1330; border-radius:999px; overflow:hidden}
    .meter>span{display:block; height:100%; background:linear-gradient(90deg,var(--acc),var(--acc2));}
    /* Ù…ÙˆØ¬â€ŒÙ†Ú¯Ø§Ø± */
    .wave-wrap{display:flex; align-items:center; gap:14px}
    .wave{width:100%; height:78px; border-radius:12px; background:#0d1538; border:1px solid rgba(255,255,255,.08)}
    .ctrl-btn{border:none; background:#0f1536; color:#cfe3ff; width:44px; height:44px; border-radius:12px}
    .ctrl-btn:hover{filter:brightness(1.1)}
    .vol-wrap{display:flex; align-items:center; gap:8px}
    input[type=range].vol{width:120px}
    .pill{border-radius:999px; background:#0e1331; border:1px solid rgba(255,255,255,.12); padding:.25rem .6rem; color:#cfe3ff;}
    /* Ø¢Ú©Ø§Ø±Ø¯Ø¦ÙˆÙ† Ø±Ø§Ù‡Ù†Ù…Ø§ */
    .accordion-button{background:#0f1433; color:var(--fg); border:none}
    .accordion-body{background:#0f1433; color:var(--fg)}
    /* Ø²Ø¨Ø§Ù†â€ŒØ¨Ø±Ú¯Ø±Ø¯Ø§Ù† */
    .lang-switch a{color:#cfe3ff;text-decoration:none;padding:6px 12px;border:1px solid var(--border);border-radius:10px}
    .lang-switch a:hover{background:rgba(255,255,255,.06)}
  </style>
</head>
<body>
<div class="container-narrow">
  <div class="d-flex justify-content-between align-items-center mb-3">
    <div class="lang-switch">
      {% set switch_lang = 'en' if lang_dir=='rtl' else 'fa' %}
      <a class="link" href="{{ url_for('home') }}?lang={{switch_lang}}">{{T['lang_switch']}}</a>
    </div>
  </div>

  <div class="p-4 glass mb-4">
    <h2 class="title">ğŸµ ğŸ­ {{T['app_title']}}</h2>
    <p class="sub mb-0">{{T['app_sub']}}</p>
  </div>

  <!-- Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ -->
  <div class="accordion mb-4" id="helpAcc">
    <div class="accordion-item">
      <h2 class="accordion-header">
        <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#help">
          â„¹ï¸ {{T['help_title']}}
        </button>
      </h2>
      <div id="help" class="accordion-collapse collapse" data-bs-parent="#helpAcc">
        <div class="accordion-body">
          <ul>
            <li>{{T['help_intro']}}</li>
            <li>{{T['help_sf2']}}</li>
            <li>{{T['help_style']}}</li>
            <li>{{T['help_eval']}}</li>
          </ul>
        </div>
      </div>
    </div>
  </div>

  <div class="card p-4 mb-4">
    <form action="{{ url_for('generate') }}?lang={{'fa' if lang_dir=='rtl' else 'en'}}" method="post" enctype="multipart/form-data">
      <div class="row g-3">
        <div class="col-12">
          <label class="form-label">{{T['text_label']}}</label>
          <textarea name="text" class="form-control" rows="3" placeholder="{{ 'Ù…Ø«Ø§Ù„: Ø§Ù…Ø±ÙˆØ² Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ´Ø­Ø§Ù„Ù…!' if lang_dir=='rtl' else 'e.g., I feel super happy today!' }}"></textarea>
        </div>
        <div class="col-md-6">
          <label class="form-label">{{T['image_label']}}</label>
          <input type="file" name="image" accept="image/*" class="form-control" />
        </div>
        <div class="col-md-6">
          <label class="form-label">{{T['audio_label']}}</label>
          <input type="file" name="audio" accept=".wav,.mp3,.flac,.ogg,.m4a" class="form-control" />
        </div>

        <div class="col-md-3">
          <label class="form-label">{{T['thr_label']}}</label>
          <input type="number" step="0.01" min="0" max="1" name="threshold" value="0.7" class="form-control" />
        </div>
        <div class="col-md-3">
          <label class="form-label">{{T['mins_label']}}</label>
          <input type="number" step="0.5" min="1" max="6" name="minutes" value="3.0" class="form-control" />
        </div>
        <div class="col-md-3">
          <label class="form-label">{{T['seed_label']}}</label>
          <input type="text" name="seed" class="form-control" placeholder="{{ 'Ø®Ø§Ù„ÛŒ = ØªØµØ§Ø¯ÙÛŒ' if lang_dir=='rtl' else 'Empty = random' }}" />
        </div>
        <div class="col-md-3">
          <label class="form-label">{{T['style_label']}}</label>
          <select name="style" class="form-select">
            <option>{{T['style_global']}}</option>
            <option>{{T['style_persian']}}</option>
          </select>
        </div>

        <div class="col-8">
          <label class="form-label">{{T['sf2_builtin_label']}}</label>
          <select name="sf2_builtin" class="form-select">
            <option value="default">{{T['sf2_builtin_default']}}</option>
            {% for key, path in sf2_list %}
            <option value="{{key}}">{{key}} â€” {{path}}</option>
            {% endfor %}
          </select>
          <div class="form-text">{{ 'ÛŒØ§ ÙØ§ÛŒÙ„ .sf2 Ø¯Ù„Ø®ÙˆØ§Ù‡â€Œ Ø±Ø§ Ø¯Ø± Ø¨Ø§Ú©Ø³ Ø±ÙˆØ¨Ø±Ùˆ Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯ (Ú©Ù‡ Ø¨Ø± Ø¯Ø§Ø®Ù„ÛŒ Ù…Ù‚Ø¯Ù… Ø§Ø³Øª).' if lang_dir=='rtl' else 'Or upload a .sf2 by yourself (takes precedence).' }}</div>
        </div>

        <div class="col-md-4">
          <label class="form-label">{{T['sf2_upload_label']}}</label>
          <input type="file" name="sf2_file" accept=".sf2" class="form-control" />
        </div>

        <!-- Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ SoundFont -->
        <div class="accordion mb-4" id="helpsf2">
          <div class="accordion-item">
            <h3 class="accordion-header">
              <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#helps">
                â„¹ï¸ {{T['badge_sf2_guide']}}
              </button>
            </h3>
            <div id="helps" class="accordion-collapse collapse" data-bs-parent="#helpsf2">
              <div class="accordion-body">
                <ul>
                  <div class="col-12">
                    <div class="mt-2 d-flex flex-wrap gap-2">
                      {% for k, info in sf2_desc %}
                        <span class="pill">{{info[0]}} || {{info[1]}}</span>
                      {% endfor %}
                    </div>
                  </div>
                </ul>
              </div>
            </div>
          </div>
        </div>

        <div class="col-12 d-grid mt-4">
          <button class="btn btn-primary btn-lg">{{T['cta']}}</button>
        </div>
      </div>
    </form>
  </div>

  {% if result %}
  <div class="card p-4 mb-4">
    <div class="form-label d-flex justify-content-between align-items-center flex-wrap gap-2">
      <h5 class="mb-0">{{T['result']}}</h5>
      <span class="pill">{{T['sf2_used']}}: {{result.sf2_title}}</span>
    </div>
    <hr>
    <div class="row g-3 align-items-center">
      <div class="col-12 col-lg-8">
        <!-- Ù…ÙˆØ¬â€ŒÙ†Ú¯Ø§Ø± + Ú©Ù†ØªØ±Ù„â€ŒÙ‡Ø§ -->
        <div class="wave-wrap">
          <button class="ctrl-btn" id="btnFwd" title="+10s">â©</button>
          <button class="ctrl-btn" id="btnPlay" title="Play/Pause">â–¶ï¸</button>
          <button class="ctrl-btn" id="btnBack" title="âˆ’10s">âª</button>
          <div class="vol-wrap">
            ğŸ”Š<input type="range" min="0" max="1" value="0.9" step="0.01" class="vol" id="vol">
          </div>
          <div id="waveform" class="wave" data-audio-url="{{ '/file/' + result.file_name }}"></div>
        </div>
        <div class="mt-2">
          {% if result.file_name %}
            <a class="btn btn-outline-info btn-sm" href="/file/{{result.file_name}}" download>{{T['download_file']}}</a>
          {% endif %}
        </div>
      </div>
      <div class="col-12 col-lg-4">
        <!-- Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ø¨Ø¯ÙˆÙ† Ø¨Ø§Ú©Ø³ + Ú†ÛŒÙ¾â€ŒÙ‡Ø§ -->
        <div class="d-flex align-items-center">
          <div class="emoji">{{result.emoji}}</div>
          <div class="form-label ms-3">
            <div class="chip">
              <strong>{{T['detected_emotion']}}:</strong>
              <span>{{result.label}}</span>
            </div>
            <div class="form-label mt-2">
              <div class="form-label d-flex justify-content-between"><small class="form-label">{{T['confidence']}}</small><small>{{'%.0f' % (result.score*100)}}%</small></div>
              <div class="meter mt-1"><span style="width: {{ max(4, min(100, result.score*100)) }}%"></span></div>
            </div>
          </div>
        </div>
        <!-- Ø¬Ø§ÛŒ ØªÙˆØ¶ÛŒØ­ Ú©ÙˆØªØ§Ù‡ -->
        <div class="form-label mt-3 small text-muted">
          <span class="badge bg-secondary">{{T['labels_hint']}}</span>
          <div class="form-label mt-1">
            {{ 'Global â†’ Ù¾Ø§Ù¾/Ø±Ø§Ú©/Ø§Ù„Ú©ØªØ±ÙˆÙ†ÛŒÚ©ØŒ Persian â†’ Ø³Ù†ØªÛŒ/Ù…Ø­Ù„ÛŒ. Ø¨Ø±Ø§ÛŒ Ø´Ø§Ø¯: Arachno/WeedsØŒ Ø¨Ø±Ø§ÛŒ Ø§Ø±Ú©Ø³ØªØ±: ChoriumØŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ: Persa.'
               if lang_dir=='rtl' else
               'Global â†’ pop/rock/electronic, Persian â†’ traditional Iranian. For happy: Arachno/Weeds, for orchestral: Chorium, for Iranian: Persa.' }}
          </div>
        </div>
      </div>
    </div>

    <hr class="mt-4">
    <h6 class="form-label mb-2">{{T['eval_report']}}</h6>
    <code class="json">{{result.report_json}}</code>
    <div class="d-flex gap-2 mb-2">
      {% if result.report_file %}
        <a class="btn btn-outline-info btn-sm" href="/file/{{result.report_file}}" download>{{T['json_download']}}</a>
      {% endif %}
    </div>
  </div>
  {% endif %}

  <div class="footer text-center py-3">
    Flask + ngrok Â· Wavesurfer.js Â· Bootstrap || {{ 'Ø¨Ø±Ø§ÛŒ Ù…Ù‡Ù†Ø¯Ø³Ø§Ù†Ù Ù‡Ù†Ø±Ù…Ù†Ø¯ âœ¨' if lang_dir=='rtl' else 'crafted for Artist Engineers âœ¨' }}
  </div>
</div>

<script>
(function(){
  // Wavesurfer
  const el = document.getElementById('waveform');
  if(!el) return;
  const url = el.getAttribute('data-audio-url');
  if(!url){ return; }
  const ws = WaveSurfer.create({
    container: el,
    waveColor: '#5f6ee3',
    progressColor: '#22d3ee',
    cursorColor: '#ffffff',
    height: 78,
    barWidth: 2,
    barRadius: 2,
    barGap: 2,
    normalize: true,
    hideScrollbar: true,
  });
  ws.load(url);
  // Controls
  const btnPlay = document.getElementById('btnPlay');
  const btnBack = document.getElementById('btnBack');
  const btnFwd  = document.getElementById('btnFwd');
  const vol     = document.getElementById('vol');
  if(btnPlay){ btnPlay.onclick = ()=>{ ws.playPause(); btnPlay.textContent = ws.isPlaying() ? 'â¸ï¸' : 'â–¶ï¸'; } }
  if(btnBack){ btnBack.onclick = ()=> ws.skip(-10) }
  if(btnFwd){  btnFwd.onclick  = ()=> ws.skip(10) }
  if(vol){     vol.oninput     = (e)=> ws.setVolume(parseFloat(e.target.value)); }
  ws.on('play', ()=> btnPlay && (btnPlay.textContent='â¸ï¸'));
  ws.on('pause',()=> btnPlay && (btnPlay.textContent='â–¶ï¸'));
})();
</script>

<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>
"""

@app.route("/", methods=["GET"])
def home():
    lang = request.args.get("lang","fa").lower()
    T, lang_dir = get_texts(lang)
    try:
        sf2_items = list(SF2_BANKS.items())
    except:
        sf2_items = []
    sf2_desc = list(SF2_DESCRIPTIONS.items())
    return render_template_string(PAGE, T=T, lang_dir=lang_dir, sf2_list=sf2_items, sf2_desc=sf2_desc, result=None)

@app.route("/generate", methods=["POST"])
def generate():
    lang = request.args.get("lang","fa").lower()
    T, lang_dir = get_texts(lang)
    result = _handle_generation(request.form, request.files)

    # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¹Ù†ÙˆØ§Ù† SF2
    key = result.get("sf2_key","default")
    if key in SF2_DESCRIPTIONS:
        sf2_title = SF2_DESCRIPTIONS[key][0]
    else:
        # Ø§Ú¯Ø± Ø¢Ù¾Ù„ÙˆØ¯ÛŒ Ø¨ÙˆØ¯Ù‡ ÛŒØ§ default
        sf2_title = os.path.basename(result["soundfont"])

    # URL ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ
    file_url = None
    if result["file_name"]:
        file_url = request.host_url.rstrip("/") + "/file/" + result["file_name"]

    res_for_view = {
        "label": result["label"],
        "score": result["score"],
        "emoji": result["emoji"],
        "file_name": result["file_name"],
        "report_file": result["report_file"],
        "soundfont": result["soundfont"],
        "sf2_title": sf2_title,
        "report_json": json.dumps(result["report"], ensure_ascii=False, indent=2)
    }

    try:
        sf2_items = list(SF2_BANKS.items())
    except:
        sf2_items = []
    sf2_desc = list(SF2_DESCRIPTIONS.items())
    return render_template_string(
        PAGE, T=T, lang_dir=lang_dir, sf2_list=sf2_items, sf2_desc=sf2_desc,
        result=res_for_view, file_url=file_url
    )

@app.route("/file/<path:fname>")
def serve_file(fname):
    return send_from_directory(STATIC_DIR, fname, as_attachment=False)

# ===== launch with ngrok (Ø§Ú¯Ø± Ù‚Ø¨Ù„Ø§Ù‹ Ù¾ÙˆØ±Øª Ø§Ø´ØºØ§Ù„ Ø§Ø³ØªØŒ Ù¾ÙˆØ±Øª Ø¬Ø¯ÛŒØ¯) =====
try:
    ngrok.kill()
except:
    pass

port = int(os.environ.get("FLASK_PORT", 5000))
public_url = ngrok.connect(port).public_url
print("Public URL:", public_url)

# Ø§Ú¯Ø± Ø±ÙˆÛŒ 5000 Ø§Ø´ØºØ§Ù„ Ø¨ÙˆØ¯ØŒ Flask Ø±Ø§ Ø±ÙˆÛŒ Ù¾ÙˆØ±Øª Ø¢Ø²Ø§Ø¯ Ø±ÙˆØ´Ù† Ú©Ù†
import socket
def _port_in_use(p):
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        return s.connect_ex(("0.0.0.0", p)) == 0

if _port_in_use(port):
    # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù¾ÙˆØ±Øª Ø¢Ø²Ø§Ø¯
    for candidate in range(5001, 5100):
        if not _port_in_use(candidate):
            port = candidate
            break
    print("Switched to port:", port)

app.run(host="0.0.0.0", port=port, debug=False)
# ===================================================================